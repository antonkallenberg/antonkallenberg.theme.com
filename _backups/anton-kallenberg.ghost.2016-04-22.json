{"db":[{"meta":{"exported_on":1461337634272,"version":"004"},"data":{"posts":[{"id":3,"uuid":"1958a032-d84e-4385-b0df-348a5e93bf04","title":"About me","slug":"about-me","markdown":"","html":"","image":null,"featured":0,"page":1,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":1458552067954,"created_by":1,"updated_at":1460991052660,"updated_by":1,"published_at":1269163260000,"published_by":1},{"id":4,"uuid":"456472da-4086-4354-a694-659a97074b0a","title":"Contact","slug":"contact","markdown":"\n\n\n","html":"","image":null,"featured":0,"page":1,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":1458557389995,"created_by":1,"updated_at":1460991041547,"updated_by":1,"published_at":1269168600000,"published_by":1},{"id":6,"uuid":"47248ed5-766b-4c49-9a67-cb1fa254bcfb","title":"PAGE CONSTRAINTS","slug":"page-constraints","markdown":"I have built a plug-in which I call PageConstraints. The plugin adds support for adding constraints to PageTypeBuilder page types. Constraints can be added with .NETs built-in DataAnnotations or by implementing custom code.\n\nFor example. Following page type is a regular PTB page type with built-in DataAnnotation attributes and a custom Attribute (UrlResolvable).\n\n```csharp\n[PageType(\"73312396-6092-475f-9f3a-e5be1adfcf95\", Filename = \"~/Templates/Pages/Author.aspx\")]\npublic class AuthorPage : TypedPageData\n{\n   [Required(ErrorMessage = \"name cant be empty\")]\n   [PageTypeProperty(Type = typeof(PropertyString))]\n   public virtual string Name { get; set; }\n \n   [UrlResolvable(@\"^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$\", ErrorMessage = \"could not resolve remote url or remote server did not return status code 200\")]\n   [PageTypeProperty(Type = typeof(PropertyString))]\n   public virtual string WebPage { get; set; }\n \n   [Range(18, 130, ErrorMessage = \"author must be between 18 and 130 years old\")]\n   public virtual int? Age { get; set; }\n}\n```\n\n## Custom attributes\n\nIts quite simple to implement a custom DataAnnotation attribute that you can add to your page type. Following Attribute is checking if a entered url is a valid uri, matching a regular expression and if the remote server of the entered url is returning a response with http status 200 by extending build-in RegularExpressionAttribute. You can see how it can be used with WebPage property at AuthorPage.\n\n```csharp\npublic class UrlResolvableAttribute : RegularExpressionAttribute\n{\n    public string Url { get; set; }\n    public UrlResolvableAttribute(string pattern) : base(pattern) { }\n \n    public override bool IsValid(object value)\n    {\n        if (value == null) return true;\n \n        var valid = base.IsValid(value);\n        if (!valid) return false;\n \n        string url;\n        if (!GetUrl(value, out url)) return false;\n \n        Uri uri;\n        if (!GetUri(url, out uri)) return false;\n \n        var request = WebRequest.Create(uri.ToString());\n        try\n        {\n           using (var reponse = (HttpWebResponse)request.GetResponse())\n           {\n               if (reponse == null)\n               {\n                   return false;\n               }\n \n              var statusCode = reponse.StatusCode;\n              return statusCode == HttpStatusCode.OK;\n           }\n        }\n        catch (WebException ex)\n        {\n           if (ex.Status == WebExceptionStatus.ProtocolError)\n           {\n               var response = ex.Response as HttpWebResponse;\n               if (response != null && response.StatusCode != HttpStatusCode.OK)\n               {\n                   return false;\n               }\n           }\n        }\n \n        return false;\n    }\n \n    private bool GetUrl(object value, out string url)\n    {\n       url = value != null ? value.ToString() : string.Empty;\n       if (string.IsNullOrEmpty(url))\n       {\n           return false;\n       }\n       return true;\n    }\n \n    private bool GetUri(string url, out Uri uri)\n    {\n       uri = null;\n       try\n       {\n           uri = new Uri(url);\n       }\n       catch (Exception)\n       {\n           return false;\n       }\n       return true;\n    }\n}\n```\n\n## Custom constraint\n\nIf you need to add custom code that is not supported by DataAnnotations for the constraint with a context of the current page, implement the method IsViolated from interface IConstraintPage.\n\n```csharp\n[PageType(\"cfe42b48-496c-475e-86db-c2030175bc83\", Filename = \"~/Templates/Pages/Book.aspx\")]\npublic class BookPage : TypedPageData, IConstraintPage\n{\n    [Required(ErrorMessage = \"book must have a name\")]\n    [StringLength(30, ErrorMessage = \"book cant have a name longer that 10 chars\")]\n    [PageTypeProperty(Type = typeof(PropertyString))]\n    public virtual string Name { get; set; }\n \n    [Required(ErrorMessage = \"isbn cant be empty\")]\n    [RegularExpression(@\"^(?=[-0-9xX ]{13}$)(?:[0-9]+[- ]){3}[0-9]*[xX0-9]$\", ErrorMessage = \"isbn is not valid\")]\n    [PageTypeProperty(Type = typeof(PropertyString))]\n    public virtual string Isbn { get; set; }\n \n    [Required(ErrorMessage = \"book must have authors\")]\n    [PageTypeProperty(Type = typeof(PropertyLinkCollection))]\n    public virtual LinkItemCollection Authors { get; set; }\n \n    public bool IsViolated(out string reason)\n    {\n      reason = string.Empty;\n      if (!string.IsNullOrEmpty(Name) && Name.Contains(\"twilight\"))\n      {\n          reason = \"this bookstore does not allow the twilight books\";\n          return true;\n      }\n      return false;\n    }\n}\n```\n\nError messages are displayed in the standard yellow popup when trying to save or publish the page in edit mode and a EPiServerCancelException is thrown.\n\nSource, tests, examples and compiled binary is available at [GitHub](https://github.com/antonkallenberg/PageConstraints). Please let me know if you think this is a needed plugin and if you have ideas for improvements.\n\nSource is compiled with EPiServer CMS R2 (6.1.379.0) and PageTypeBuilder 2.0.","html":"<p>I have built a plug-in which I call PageConstraints. The plugin adds support for adding constraints to PageTypeBuilder page types. Constraints can be added with .NETs built-in DataAnnotations or by implementing custom code.</p>\n\n<p>For example. Following page type is a regular PTB page type with built-in DataAnnotation attributes and a custom Attribute (UrlResolvable).</p>\n\n<pre><code class=\"language-csharp\">[PageType(\"73312396-6092-475f-9f3a-e5be1adfcf95\", Filename = \"~/Templates/Pages/Author.aspx\")]\npublic class AuthorPage : TypedPageData  \n{\n   [Required(ErrorMessage = \"name cant be empty\")]\n   [PageTypeProperty(Type = typeof(PropertyString))]\n   public virtual string Name { get; set; }\n\n   [UrlResolvable(@\"^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$\", ErrorMessage = \"could not resolve remote url or remote server did not return status code 200\")]\n   [PageTypeProperty(Type = typeof(PropertyString))]\n   public virtual string WebPage { get; set; }\n\n   [Range(18, 130, ErrorMessage = \"author must be between 18 and 130 years old\")]\n   public virtual int? Age { get; set; }\n}\n</code></pre>\n\n<h2 id=\"customattributes\">Custom attributes</h2>\n\n<p>Its quite simple to implement a custom DataAnnotation attribute that you can add to your page type. Following Attribute is checking if a entered url is a valid uri, matching a regular expression and if the remote server of the entered url is returning a response with http status 200 by extending build-in RegularExpressionAttribute. You can see how it can be used with WebPage property at AuthorPage.</p>\n\n<pre><code class=\"language-csharp\">public class UrlResolvableAttribute : RegularExpressionAttribute  \n{\n    public string Url { get; set; }\n    public UrlResolvableAttribute(string pattern) : base(pattern) { }\n\n    public override bool IsValid(object value)\n    {\n        if (value == null) return true;\n\n        var valid = base.IsValid(value);\n        if (!valid) return false;\n\n        string url;\n        if (!GetUrl(value, out url)) return false;\n\n        Uri uri;\n        if (!GetUri(url, out uri)) return false;\n\n        var request = WebRequest.Create(uri.ToString());\n        try\n        {\n           using (var reponse = (HttpWebResponse)request.GetResponse())\n           {\n               if (reponse == null)\n               {\n                   return false;\n               }\n\n              var statusCode = reponse.StatusCode;\n              return statusCode == HttpStatusCode.OK;\n           }\n        }\n        catch (WebException ex)\n        {\n           if (ex.Status == WebExceptionStatus.ProtocolError)\n           {\n               var response = ex.Response as HttpWebResponse;\n               if (response != null &amp;&amp; response.StatusCode != HttpStatusCode.OK)\n               {\n                   return false;\n               }\n           }\n        }\n\n        return false;\n    }\n\n    private bool GetUrl(object value, out string url)\n    {\n       url = value != null ? value.ToString() : string.Empty;\n       if (string.IsNullOrEmpty(url))\n       {\n           return false;\n       }\n       return true;\n    }\n\n    private bool GetUri(string url, out Uri uri)\n    {\n       uri = null;\n       try\n       {\n           uri = new Uri(url);\n       }\n       catch (Exception)\n       {\n           return false;\n       }\n       return true;\n    }\n}\n</code></pre>\n\n<h2 id=\"customconstraint\">Custom constraint</h2>\n\n<p>If you need to add custom code that is not supported by DataAnnotations for the constraint with a context of the current page, implement the method IsViolated from interface IConstraintPage.</p>\n\n<pre><code class=\"language-csharp\">[PageType(\"cfe42b48-496c-475e-86db-c2030175bc83\", Filename = \"~/Templates/Pages/Book.aspx\")]\npublic class BookPage : TypedPageData, IConstraintPage  \n{\n    [Required(ErrorMessage = \"book must have a name\")]\n    [StringLength(30, ErrorMessage = \"book cant have a name longer that 10 chars\")]\n    [PageTypeProperty(Type = typeof(PropertyString))]\n    public virtual string Name { get; set; }\n\n    [Required(ErrorMessage = \"isbn cant be empty\")]\n    [RegularExpression(@\"^(?=[-0-9xX ]{13}$)(?:[0-9]+[- ]){3}[0-9]*[xX0-9]$\", ErrorMessage = \"isbn is not valid\")]\n    [PageTypeProperty(Type = typeof(PropertyString))]\n    public virtual string Isbn { get; set; }\n\n    [Required(ErrorMessage = \"book must have authors\")]\n    [PageTypeProperty(Type = typeof(PropertyLinkCollection))]\n    public virtual LinkItemCollection Authors { get; set; }\n\n    public bool IsViolated(out string reason)\n    {\n      reason = string.Empty;\n      if (!string.IsNullOrEmpty(Name) &amp;&amp; Name.Contains(\"twilight\"))\n      {\n          reason = \"this bookstore does not allow the twilight books\";\n          return true;\n      }\n      return false;\n    }\n}\n</code></pre>\n\n<p>Error messages are displayed in the standard yellow popup when trying to save or publish the page in edit mode and a EPiServerCancelException is thrown.</p>\n\n<p>Source, tests, examples and compiled binary is available at <a href=\"https://github.com/antonkallenberg/PageConstraints\">GitHub</a>. Please let me know if you think this is a needed plugin and if you have ideas for improvements.</p>\n\n<p>Source is compiled with EPiServer CMS R2 (6.1.379.0) and PageTypeBuilder 2.0.</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":1460990578589,"created_by":1,"updated_at":1461136225293,"updated_by":1,"published_at":1337859420000,"published_by":1},{"id":7,"uuid":"94d795c9-124c-4fe7-b1bd-ed6335044c40","title":"Code to remove","slug":"code-to-remove","markdown":"## Remove:\n* https://github.com/antonkallenberg/SiteMapIndex\n* https://github.com/antonkallenberg/Blog\n* https://github.com/antonkallenberg/Presentations\n\n## Move:\n* ","html":"<h2 id=\"remove\">Remove:</h2>\n\n<ul>\n<li><a href=\"https://github.com/antonkallenberg/SiteMapIndex\">https://github.com/antonkallenberg/SiteMapIndex</a></li>\n<li><a href=\"https://github.com/antonkallenberg/Blog\">https://github.com/antonkallenberg/Blog</a></li>\n<li><a href=\"https://github.com/antonkallenberg/Presentations\">https://github.com/antonkallenberg/Presentations</a></li>\n</ul>\n\n<h2 id=\"move\">Move:</h2>\n\n<ul>\n<li></li>\n</ul>","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":1460991089774,"created_by":1,"updated_at":1461147087953,"updated_by":1,"published_at":1271603460000,"published_by":null},{"id":8,"uuid":"d081a6f4-616a-4de8-a898-a85be000b9a8","title":"CREATING BLOCKS IN EPISERVER 7 PREVIEW","slug":"creating-blocks-in-episerver-7-preview","markdown":"This post is about blocks in EPiServer CMS 7. Blocks are like a built-in composer support to the cms core.  Blocks makes it simpler to reuse functionality for developers and web editors. It’s a lot like building a “composer function” with the great power of using MVC and typed page data in EPiServer 7.\n\nWe are not forced to implement a controller/model. For simpler blocks it could maybe seem overkill to implement a controller/model, more on this topic later.\n\n## The block type\nA block is a lot like a PageData object but is defined as a BlockData object that inherits from ContentData, same base type as PageData inherits from.\n\nImagine that we are building a web application for a book store then we would maybe need a reusable block that shows the latest books.\n\nWe create a block type like creating a typed page data object but the class inherits from BlockData instead of PageData. The block definition contains a heading for the block and a reference to the root of where to get books from.\n\n```csharp\n[ContentType(GUID = \"9E4C0D85-1139-48EA-B905-67C08BEE3C69\",\n            DisplayName = \"Latest books\",\n            Description = \"Simple block that displays latest books\",\n            GroupName = \"Book listing\",\n            Order = 1)]\npublic class LatestBooksBlock : BlockData \n{\n    [CultureSpecific]\n    [Display(\n        GroupName = SystemTabNames.Content,\n        Name = \"Heading\",\n        Description = \"The heading of the block.\",\n        Order = 1)]\n    public virtual string Heading { get; set; }\n\n    [CultureSpecific]\n    [Display(\n        GroupName = SystemTabNames.Content,\n        Name = \"Book root\",\n        Description = \"The root where to get new books from\",\n        Order = 2)]\n    public virtual PageReference BookRoot { get; set; }\n}\n```\n\n## The model\n\nFor this block we create a view model that is representing data for the presentation (the view).  The model contains a list of new books and our complete block type object.\n\n```csharp\npublic class LatestBooksViewModel \n{\n    public LatestBooksBlock LatestBooksBlock { get; private set; }\n    public IEnumerable<PageTypes.BookPage> LatestBooks { get; set; }\n\n    public LatestBooksViewModel(LatestBooksBlock latestBooksBlock, IEnumerable<PageTypes.BookPage> latestBooks) \n    {\n        LatestBooksBlock = latestBooksBlock;\n        LatestBooks = latestBooks;\n    }\n}\n```\n\n## The controller\n\nThe controller will get the data we need for the block, add it to the view model, picking a view and wire it all together. Actually its quit simple. Following code is assuming that some kind of container is wiring up our dependencies. More on this topic [here](http://www.popkram.com/blog/2012/06/episerver-7-preview-container-support/) and [here](https://blogs.msdn.microsoft.com/elee/2010/11/19/asp-net-mvc-3-idependencyresolver-and-structuremap/). Also the [sample mvc templates](http://world.episerver.com/Download/Items/EPiServer-CMS/EPiServer-7-Preview---CMS/EPiServer-7-Preview---CMS11/) are shipped with a structure map dependency resolver you can look at.\n\n\n```csharp\npublic class LatestBooksController : ActionControllerBase, IRenderTemplate<LatestBooksBlock> \n{\n    private readonly IContentRepository contentRepository;\n\n    public LatestBooksController(IContentRepository contentRepository) \n    {\n        this.contentRepository = contentRepository;\n    }\n\n    public ActionResult Index(LatestBooksBlock latestBooksBlock) \n    {\n        var latestBooks = Enumerable.Empty<BookPage>();\n        var bookRoot = latestBooksBlock.BookRoot;\n        \n        if (!PageReference.IsNullOrEmpty(bookRoot)) \n        {\n            latestBooks = contentRepository.GetChildren<BookPage>(bookRoot, LanguageSelector.AutoDetect(true), 0, 5).OrderByDescending(x => x.StartPublish);\n        }\n        \n        return View(new LatestBooksViewModel(latestBooksBlock, latestBooks));\n    }\n}\n```\n\nBy inheriting from ActionControllBase we get some useful stuff and by implement IRenderTemplate<T> with T as our block type (LatestBooksBlock) we bind the controller to our block type.\n\nThe IContentRepository interface is injected to the constructor which is the one of the (many(!)) interfaces for the EPiServer.Datafactory.\n\nWhen the block loads the Index method is executed and the instance of our block type is injected as a parameter. The method then gets the five latest books and adding it to a instance of our view model and passing it to the view. Notice the startIndex and maxRows parameters within the GetChildren<T> call, sweet stuff :)\n\n## The view\n\nThe view is a partial view that is bound to controller by default asp.net mvc folder structure convention. The file is placed in ~/Views/LatestBooks/Index.cshtml.\n\n\n```csharp\n@using System.Linq;\n@using EPiServer.Web.Mvc.Html\n@model EPiBooks.Models.Blocks.LatestBooksViewModel\n \n<h1>@Html.PropertyFor(x => x.LatestBooksBlock.Heading)</h1>\n@if (Model.LatestBooks.Any()) \n{\n    <ul>\n        @foreach (var book in Model.LatestBooks) \n        {\n            <li>@Html.ContentLink(book)</li>\n        }\n    </ul>\n}\n```\n\nWe are defining that we are binding the LastestBooksViewModel to the view by adding the “@model”-line same model that we passed to the view from our controller. The view then uses simple if-statements and loops to render a list of new books with the help of some built-in EPiServer html helpers\n\n## Adding the block to a page\n\nNow we have created our block and we want to use it. A block can be used similar to a PageTypeBuilder property group described [here](http://joelabrahamsson.com/page-type-builder-2-preview-1-released/).\n\n```csharp\n[Display(Name = \"Latest books\",\n     Description = \"A block that lists latest books.\",\n     GroupName = SystemTabNames.Content,\n     Order = 3)]\n public virtual LatestBooksBlock LatestBooks { get; set; }\n```\n\nThe block will then be a part of the page type and can be edited directly on the page in edit mode.\n\n![](/content/images/2016/04/1-1.png)\n\nA block can also be added dynamically to ContentArea.\n\n```csharp\n[Display(Name = \"Content area\",\n    Description = \"A content for adding any shared block\",\n    GroupName = SystemTabNames.Content,\n    Order = 4)]\npublic virtual ContentArea ContentArea { get; set; }\n```\n\nIn this way we can create a block separate from a page and reuse the same block at different pages. Now we can create instances of shared blocks and drag-and-drop them to a ContentArea of any page.\n\n![](/content/images/2016/04/2.png)\n\n\n## Creating a block without controller/model\n\nAs I wrote we can create a block without using a controller/model. We create a simple block type, same way as for the LastestBooksBlock. The block is only containing a text editor.\n\n```csharp\n[ContentType(GUID = \"6fe1bf14-2aeb-4954-9fcb-eb989f92d46b\",\n    DisplayName = \"Text block\",\n    Description = \"Simple block that displays text from a editor\",\n    Order = 2)]\npublic class TextBlock : BlockData \n{\n    [CultureSpecific]\n    [Display(Name = \"Main body\",\n             Description = \"The main body using the XHTML-editor you can insert for example text, images and tables.\",\n             GroupName = SystemTabNames.Content,\n             Order = 1)]\n    public virtual XhtmlString MainBody { get; set; }\n}\n```\n\nWe skip all the controllers and models and just add a view to out shared folder (~/Views/Shared/TextBlock.cshtml)\n\n```csharp\n@model EPiBooks.BlockTypes.TextBlock\n \n@Model.MainBody\n```\n\nNow we can created instances of TextBlock and add them to a ContentArea.\n\n![](/content/images/2016/04/3.png)\n\n## Conclusion\n\nI think blocks are a really powerful and cool feature that I know we can befit from and use a lot in future project. I really like the idea of a built-in composer into the core of the cms and I hope that blocks will be replacing composer after the CMS 7 release.\n\n\n","html":"<p>This post is about blocks in EPiServer CMS 7. Blocks are like a built-in composer support to the cms core.  Blocks makes it simpler to reuse functionality for developers and web editors. It’s a lot like building a “composer function” with the great power of using MVC and typed page data in EPiServer 7.</p>\n\n<p>We are not forced to implement a controller/model. For simpler blocks it could maybe seem overkill to implement a controller/model, more on this topic later.</p>\n\n<h2 id=\"theblocktype\">The block type</h2>\n\n<p>A block is a lot like a PageData object but is defined as a BlockData object that inherits from ContentData, same base type as PageData inherits from.</p>\n\n<p>Imagine that we are building a web application for a book store then we would maybe need a reusable block that shows the latest books.</p>\n\n<p>We create a block type like creating a typed page data object but the class inherits from BlockData instead of PageData. The block definition contains a heading for the block and a reference to the root of where to get books from.</p>\n\n<pre><code class=\"language-csharp\">[ContentType(GUID = \"9E4C0D85-1139-48EA-B905-67C08BEE3C69\",\n            DisplayName = \"Latest books\",\n            Description = \"Simple block that displays latest books\",\n            GroupName = \"Book listing\",\n            Order = 1)]\npublic class LatestBooksBlock : BlockData  \n{\n    [CultureSpecific]\n    [Display(\n        GroupName = SystemTabNames.Content,\n        Name = \"Heading\",\n        Description = \"The heading of the block.\",\n        Order = 1)]\n    public virtual string Heading { get; set; }\n\n    [CultureSpecific]\n    [Display(\n        GroupName = SystemTabNames.Content,\n        Name = \"Book root\",\n        Description = \"The root where to get new books from\",\n        Order = 2)]\n    public virtual PageReference BookRoot { get; set; }\n}\n</code></pre>\n\n<h2 id=\"themodel\">The model</h2>\n\n<p>For this block we create a view model that is representing data for the presentation (the view).  The model contains a list of new books and our complete block type object.</p>\n\n<pre><code class=\"language-csharp\">public class LatestBooksViewModel  \n{\n    public LatestBooksBlock LatestBooksBlock { get; private set; }\n    public IEnumerable&lt;PageTypes.BookPage&gt; LatestBooks { get; set; }\n\n    public LatestBooksViewModel(LatestBooksBlock latestBooksBlock, IEnumerable&lt;PageTypes.BookPage&gt; latestBooks) \n    {\n        LatestBooksBlock = latestBooksBlock;\n        LatestBooks = latestBooks;\n    }\n}\n</code></pre>\n\n<h2 id=\"thecontroller\">The controller</h2>\n\n<p>The controller will get the data we need for the block, add it to the view model, picking a view and wire it all together. Actually its quit simple. Following code is assuming that some kind of container is wiring up our dependencies. More on this topic <a href=\"http://www.popkram.com/blog/2012/06/episerver-7-preview-container-support/\">here</a> and <a href=\"https://blogs.msdn.microsoft.com/elee/2010/11/19/asp-net-mvc-3-idependencyresolver-and-structuremap/\">here</a>. Also the <a href=\"http://world.episerver.com/Download/Items/EPiServer-CMS/EPiServer-7-Preview---CMS/EPiServer-7-Preview---CMS11/\">sample mvc templates</a> are shipped with a structure map dependency resolver you can look at.</p>\n\n<pre><code class=\"language-csharp\">public class LatestBooksController : ActionControllerBase, IRenderTemplate&lt;LatestBooksBlock&gt;  \n{\n    private readonly IContentRepository contentRepository;\n\n    public LatestBooksController(IContentRepository contentRepository) \n    {\n        this.contentRepository = contentRepository;\n    }\n\n    public ActionResult Index(LatestBooksBlock latestBooksBlock) \n    {\n        var latestBooks = Enumerable.Empty&lt;BookPage&gt;();\n        var bookRoot = latestBooksBlock.BookRoot;\n\n        if (!PageReference.IsNullOrEmpty(bookRoot)) \n        {\n            latestBooks = contentRepository.GetChildren&lt;BookPage&gt;(bookRoot, LanguageSelector.AutoDetect(true), 0, 5).OrderByDescending(x =&gt; x.StartPublish);\n        }\n\n        return View(new LatestBooksViewModel(latestBooksBlock, latestBooks));\n    }\n}\n</code></pre>\n\n<p>By inheriting from ActionControllBase we get some useful stuff and by implement IRenderTemplate<T> with T as our block type (LatestBooksBlock) we bind the controller to our block type.</p>\n\n<p>The IContentRepository interface is injected to the constructor which is the one of the (many(!)) interfaces for the EPiServer.Datafactory.</p>\n\n<p>When the block loads the Index method is executed and the instance of our block type is injected as a parameter. The method then gets the five latest books and adding it to a instance of our view model and passing it to the view. Notice the startIndex and maxRows parameters within the GetChildren<T> call, sweet stuff :)</p>\n\n<h2 id=\"theview\">The view</h2>\n\n<p>The view is a partial view that is bound to controller by default asp.net mvc folder structure convention. The file is placed in ~/Views/LatestBooks/Index.cshtml.</p>\n\n<pre><code class=\"language-csharp\">@using System.Linq;\n@using EPiServer.Web.Mvc.Html\n@model EPiBooks.Models.Blocks.LatestBooksViewModel\n\n&lt;h1&gt;@Html.PropertyFor(x =&gt; x.LatestBooksBlock.Heading)&lt;/h1&gt;  \n@if (Model.LatestBooks.Any()) \n{\n    &lt;ul&gt;\n        @foreach (var book in Model.LatestBooks) \n        {\n            &lt;li&gt;@Html.ContentLink(book)&lt;/li&gt;\n        }\n    &lt;/ul&gt;\n}\n</code></pre>\n\n<p>We are defining that we are binding the LastestBooksViewModel to the view by adding the “@model”-line same model that we passed to the view from our controller. The view then uses simple if-statements and loops to render a list of new books with the help of some built-in EPiServer html helpers</p>\n\n<h2 id=\"addingtheblocktoapage\">Adding the block to a page</h2>\n\n<p>Now we have created our block and we want to use it. A block can be used similar to a PageTypeBuilder property group described <a href=\"http://joelabrahamsson.com/page-type-builder-2-preview-1-released/\">here</a>.</p>\n\n<pre><code class=\"language-csharp\">[Display(Name = \"Latest books\",\n     Description = \"A block that lists latest books.\",\n     GroupName = SystemTabNames.Content,\n     Order = 3)]\n public virtual LatestBooksBlock LatestBooks { get; set; }\n</code></pre>\n\n<p>The block will then be a part of the page type and can be edited directly on the page in edit mode.</p>\n\n<p><img src=\"/content/images/2016/04/1-1.png\" alt=\"\" /></p>\n\n<p>A block can also be added dynamically to ContentArea.</p>\n\n<pre><code class=\"language-csharp\">[Display(Name = \"Content area\",\n    Description = \"A content for adding any shared block\",\n    GroupName = SystemTabNames.Content,\n    Order = 4)]\npublic virtual ContentArea ContentArea { get; set; }  \n</code></pre>\n\n<p>In this way we can create a block separate from a page and reuse the same block at different pages. Now we can create instances of shared blocks and drag-and-drop them to a ContentArea of any page.</p>\n\n<p><img src=\"/content/images/2016/04/2.png\" alt=\"\" /></p>\n\n<h2 id=\"creatingablockwithoutcontrollermodel\">Creating a block without controller/model</h2>\n\n<p>As I wrote we can create a block without using a controller/model. We create a simple block type, same way as for the LastestBooksBlock. The block is only containing a text editor.</p>\n\n<pre><code class=\"language-csharp\">[ContentType(GUID = \"6fe1bf14-2aeb-4954-9fcb-eb989f92d46b\",\n    DisplayName = \"Text block\",\n    Description = \"Simple block that displays text from a editor\",\n    Order = 2)]\npublic class TextBlock : BlockData  \n{\n    [CultureSpecific]\n    [Display(Name = \"Main body\",\n             Description = \"The main body using the XHTML-editor you can insert for example text, images and tables.\",\n             GroupName = SystemTabNames.Content,\n             Order = 1)]\n    public virtual XhtmlString MainBody { get; set; }\n}\n</code></pre>\n\n<p>We skip all the controllers and models and just add a view to out shared folder (~/Views/Shared/TextBlock.cshtml)</p>\n\n<pre><code class=\"language-csharp\">@model EPiBooks.BlockTypes.TextBlock\n\n@Model.MainBody\n</code></pre>\n\n<p>Now we can created instances of TextBlock and add them to a ContentArea.</p>\n\n<p><img src=\"/content/images/2016/04/3.png\" alt=\"\" /></p>\n\n<h2 id=\"conclusion\">Conclusion</h2>\n\n<p>I think blocks are a really powerful and cool feature that I know we can befit from and use a lot in future project. I really like the idea of a built-in composer into the core of the cms and I hope that blocks will be replacing composer after the CMS 7 release.</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":1460991314621,"created_by":1,"updated_at":1461136820470,"updated_by":1,"published_at":1343129820000,"published_by":1},{"id":9,"uuid":"16a501f4-0040-4d2d-a58d-33e850e1a8ef","title":"POWERSHELL FTP MODULE IN A PSAKE BUILD PROCESS","slug":"powershell-ftp-module-in-a-psake-build-process","markdown":"In my effort to learn more about PowerShell I wrote a simple ftp module. I wrote the module primary for using it in a automated deployment process with [Psake](https://github.com/psake/psake), [GitHub](https://github.com/), [Jenkins](https://jenkins.io) and a target environment with ftp access.  I looked around to find a similar module and the best match I found was this [script](https://github.com/ruslander/Nutshells/blob/master/ftp-ls.ps1) so the module is inspired a lot by that script.\n\nThe module publish four methods that does what I needed the module to do for my deployment process so its not really a generic ftp module but I works for some scenarios.\n\n* Set-FtpConnection\n* Send-ToFtp\n* Get-FromFtp\n* Remove-FromFtp\n\nThe build process that I needed to support looks like this. **This post focus on the last steps; 6 and 7**. Step 1 – 5 requires one or more individual posts.\n\n1. Setup (clean up last release, create deployment and backup folders)\n2. Compile (.net assemblies, CoffeeSscript to JavaScript and Less to CSS + minifying)\n3. Run unit tests and break deploy if it fails (MSpec)\n4. Copy needed files for running the application at different target environments\n5. Merge/create configurations for different target environments\n6. Backup the current application from the target environment\n * Creating a ftp connection to the target environment\n * Download the complete deployed application from the web root to the \nbackup folders that was created at setup (step 1)\n * Upload the backup to a backup folder at the ftp server\n7. Redeploy the application\n * Creating a ftp connection to the target environment\n * Drop the web root\n * Upload the new release from the deployment folder created at setup (step 1)\n\nThe build script (based on psake) looks like this. I have removed code and properties that is not related to this post. If you want to se the complete script you can find it [here](https://github.com/antonkallenberg/MvcCiTest/blob/v1.0/src/buildScripts/builder.ps1)\n\n```powershell\nimport-module .\\utilities.psm1\nimport-module .\\ftp.psm1\n \nproperties {\n    $TargetEnvironment = 'Debug'\n    $DateLabel = ([DateTime]::Now.ToString(\"yyyy-MM-dd_HH-mm-ss\"))\n    $ApplicationBackupRoot = \"..\\..\\Deploy\\Backup\"\n    $ApplicationBackupRootWithDateLabel = \"..\\..\\Deploy\\Backup\\$DateLabel\"\n    $BuildOutputDestinationRoot = \"..\\..\\Deploy\\Build\"\n \n    $StagingFtpUri = 'ftp://127.0.0.1:55/'\n    $StagingFtpWwwRoot = \"www\"\n    $StagingFtpBackupRoot = \"backup\"\n    $StagingFtpUsername = 'anton'\n    $StagingFtpPassword = 'anton'\n \n    #....\n    #code is removed to not loose focus from this post\n    #....\n}\n \ntask Default -depends CopyFiles\n \ntask Staging -depends DeployWebToStagingFtp\n \ntask DeployWebToStagingFtp -depends BackupWebAtStagingFtp {\n    $fullBuildOutputDestinationRoot = Resolve-Path $BuildOutputDestinationRoot\n    Set-FtpConnection $StagingFtpUri $StagingFtpUsername $StagingFtpPassword\n    Remove-FromFtp $StagingFtpWwwRoot\n    Send-ToFtp $fullBuildOutputDestinationRoot $StagingFtpWwwRoot\n}\n \ntask BackupWebAtStagingFtp -depends MergeConfiguration {\n    $fullSourcePath = Resolve-Path $ApplicationBackupRootWithDateLabel\n    $fullApplicationBackupRootPath = Resolve-Path $ApplicationBackupRoot\n    Set-FtpConnection $StagingFtpUri $StagingFtpUsername $StagingFtpPassword\n    Get-FromFtp $fullSourcePath $StagingFtpWwwRoot\n    Send-ToFtp $fullApplicationBackupRootPath $StagingFtpBackupRoot\n}\n \ntask MergeConfiguration -depends CopyFiles {\n    #....\n    #code is removed to not loose focus from this post\n    #....\n}\n \ntask CopyFiles -depends Test {\n    #....\n    #code is removed to not loose focus from this post\n    #....\n}\n \ntask Test -depends Compile, Setup {\n    #....\n    #code is removed to not loose focus from this post\n    #....\n}\n \ntask Compile -depends Setup {\n    #....\n    #code is removed to not loose focus from this post\n    #....\n}\n \ntask Setup {\n    Add-FolderIfMissing $BuildOutputDestinationRoot\n    if (!($TargetEnvironment -ieq 'debug')) {\n        Add-FolderIfMissing $ApplicationBackupRoot\n        Add-FolderIfMissing $ApplicationBackupRootWithDateLabel\n    }\n}\n```\n\nAs you can see this version of the script only supports deploys to the staging environment but we could quite easily add support to other environments too. The code shows how the ftp module is used in a psake build script. Standalone in a PowerShell session it would something like this.\n\n```powershell\nPS C:\\Projects\\MvcCiTest\\src\\buildScripts> Set-ExecutionPolicy RemoteSigned\n \nExecution Policy Change\nThe execution policy helps protect you from scripts that you do not trust. Changing the execution policy might expose\nyou to the security risks described in the about_Execution_Policies help topic. Do you want to change the execution\npolicy?\n[Y] Yes  [N] No  [S] Suspend  [?] Help (default is \"Y\"):\nPS C:\\Projects\\MvcCiTest\\src\\buildScripts> Import-Module .\\ftp.psm1\nPS C:\\Projects\\MvcCiTest\\src\\buildScripts> Set-FtpConnection \"ftp://127.0.0.1:21/\" \"anton\" \"anton\"\nPS C:\\Projects\\MvcCiTest\\src\\buildScripts> Send-ToFtp \"C:\\Users\\Anton\\Pictures\\\" \"www/pictures\"\nuploading cheetah-picture.jpg\nPS C:\\Projects\\MvcCiTest\\src\\buildScripts> Get-FromFtp \"C:\\temp\" \"www/pictures\"\nDownloading ftp://127.0.0.1:21/www/pictures/heetah-picture.jpg ...\nPS C:\\Projects\\MvcCiTest\\src\\buildScripts> Remove-FromFtp \"www/pictures\"\n deleting ...\nDelete status: 250 File deleted successfully\n \nPS C:\\Projects\\MvcCiTest\\src\\buildScripts> Remove-Module ftp\n```\n\nSo finally… the first version of my ftp module looks like this. Would appreciate any suggestions on how to improve the module the code is available at [GitHub](https://github.com/antonkallenberg/MvcCiTest/blob/master/src/buildScripts/ftp.psm1) feel free to send me a pull request :)\n\n```powershell\n[string]$script:ftpHost\n[string]$script:username\n[string]$script:password\n[System.Net.NetworkCredential]$script:Credentials\n \nfunction Set-FtpConnection {\n    param([string]$host, [string]$username, [string]$password)\n \n    $script:Credentials = New-Object System.Net.NetworkCredential($username, $password)\n    $script:ftpHost  = $host\n    $script:username = $username\n    $script:password = $password\n}\n \nfunction Send-ToFtp {\n    param([string]$sourcePath, [string]$ftpFolder)\n \n    foreach($item in Get-ChildItem -recurse $sourcePath){\n        $itemName = [System.IO.Path]::GetFullPath($item.FullName).SubString([System.IO.Path]::GetFullPath($sourcePath).Length + 1)\n        $fullFtpPath = [System.IO.Path]::Combine($script:ftpHost+\"/$ftpFolder/\", $itemName)\n        if ($item.Attributes -eq \"Directory\"){\n            try{\n                $uri = New-Object System.Uri($fullFtpPath)\n                $fullFtpPathRequest = [System.Net.WebRequest]::Create($uri)\n                $fullFtpPathRequest.Credentials = $script:Credentials\n                $fullFtpPathRequest.Method = [System.Net.WebRequestMethods+Ftp]::MakeDirectory\n                $fullFtpPathRequest.GetResponse()\n            }catch [Net.WebException] {\n                Write-Host \"$item probably exists ...\"\n            }\n            continue;\n        }\n \n        $webClient = New-Object System.Net.WebClient\n        $webClient.Credentials = $script:Credentials\n        $uri = New-Object System.Uri($fullFtpPath)\n        Write-Host \"uploading $item\"\n       $webClient.UploadFile($uri, $item.FullName)\n    }\n}\n \nfunction Get-FromFtp {\n    param([string]$sourceFolder, [string]$ftpFolder)\n \n    $fullFtpPath = [System.IO.Path]::Combine($script:ftpHost, $ftpFolder)\n    $dirs = Get-FtpDirecoryTree $fullFtpPath\n    foreach($dir in $dirs){\n       $path = [io.path]::Combine($sourceFolder, $dir)\n       if ((Test-Path $path) -eq $false) {\n          New-Item -Path $path -ItemType Directory | Out-Null\n       }\n    }\n    $files = Get-FtpFilesTree $fullFtpPath\n    foreach($file in $files) {\n        $ftpPath = $fullFtpPath + \"/\" + $file\n        $localFilePath = [io.path]::Combine($sourceFolder, $file)\n        Write-Host \"Downloading $ftpPath ...\"\n        Get-FtpFile $ftpPath $localFilePath\n    }\n}\n \nfunction Remove-FromFtp($ftpFolder) {\n    $fullFtpPath = [System.IO.Path]::Combine($script:ftpHost, $ftpFolder)\n    $fileTree = Get-FtpFilesTree $fullFtpPath\n    if($fileTree -gt 0){\n        foreach($file in $fileTree) {\n            $ftpFile = [io.path]::Combine($fullFtpPath, $file)\n            Remove-FtpItem $ftpFile \"file\"\n        }\n    }\n    $dirTree = [array](Get-FtpDirecoryTree $fullFtpPath) | sort -Property @{ Expression = {$_.Split('/').Count} } -Desc\n    if($dirTree -gt 0) {\n        foreach($dir in $dirTree) {\n            $ftpDir = [io.path]::Combine($fullFtpPath, $dir)\n            Remove-FtpItem $ftpDir \"directory\"\n        }\n    }\n}\n \nfunction Get-FtpDirecoryTree($fullFtpPath) {\n    if($fullFtpPath.EndsWith(\"/\") -eq $false) {\n        $fullFtpPath = $fullFtpPath += \"/\"\n    }\n \n    $folderTree = New-Object \"System.Collections.Generic.List[string]\"\n    $folders = New-Object \"System.Collections.Generic.Queue[string]\"\n    $folders.Enqueue($fullFtpPath)\n    while($folders.Count -gt 0) {\n        $folder = $folders.Dequeue()\n        $directoryContent = Get-FtpDirectoryContent $folder\n        $dirs = Get-FtpDirectories $folder\n        foreach ($line in $dirs){\n            $dir = @($directoryContent | Where { $line.EndsWith($_) })[0]\n            [void]$directoryContent.Remove($dir)\n            $folders.Enqueue($folder + $dir + \"/\")\n            $folderTree.Add($folder.Replace($fullFtpPath, \"\") + $dir + \"/\")\n        }\n    }\n    return ,$folderTree\n}\n \nfunction Get-FtpFilesTree($fullFtpPath) {\n    if($fullFtpPath.EndsWith(\"/\") -eq $false) {\n        $fullFtpPath = $fullFtpPath += \"/\"\n    }\n \n    $fileTree = New-Object \"System.Collections.Generic.List[string]\"\n    $folders = New-Object \"System.Collections.Generic.Queue[string]\"\n    $folders.Enqueue($fullFtpPath)\n    while($folders.Count -gt 0){\n        $folder = $folders.Dequeue()\n        $directoryContent = Get-FtpDirectoryContent $folder\n        $dirs = Get-FtpDirectories $folder\n        foreach ($line in $dirs){\n            $dir = @($directoryContent | Where { $line.EndsWith($_) })[0]\n            [void]$directoryContent.Remove($dir)\n            $folders.Enqueue($folder + $dir + \"/\")\n        }\n        $directoryContent | ForEach {\n            $fileTree.Add($folder.Replace($fullFtpPath, \"\") + $_)\n        }\n    }\n \n    return ,$fileTree\n}\n \nfunction Get-FtpDirectories($folder) {\n    $dirs = New-Object \"system.collections.generic.list[string]\"\n    $operation = [System.Net.WebRequestMethods+Ftp]::ListDirectoryDetails\n    $reader = Get-Stream $folder $operation\n    while (($line = $reader.ReadLine()) -ne $null) {\n       if ($line.Trim().ToLower().StartsWith(\"d\") -or $line.Contains(\" <DIR> \")) {\n            $dirs.Add($line)\n        }\n    }\n    $reader.Dispose();\n    return ,$dirs\n}\n \nfunction Get-FtpDirectoryContent($folder) {\n    $files = New-Object \"System.Collections.Generic.List[String]\"\n    $operation = [System.Net.WebRequestMethods+Ftp]::ListDirectory\n    $reader = Get-Stream $folder $operation\n    while (($line = $reader.ReadLine()) -ne $null) {\n       $files.Add($line.Trim())\n    }\n    $reader.Dispose();\n    return ,$files\n}\n \nfunction Get-FtpFile($ftpPath, $localFilePath) {\n    $ftpRequest = [System.Net.FtpWebRequest]::create($ftpPath)\n    $ftpRequest.Credentials = $script:Credentials\n    $ftpRequest.Method = [System.Net.WebRequestMethods+Ftp]::DownloadFile\n    $ftpRequest.UseBinary = $true\n    $ftpRequest.KeepAlive = $false\n    $ftpResponse = $ftpRequest.GetResponse()\n    $responseStream = $ftpResponse.GetResponseStream()\n \n    [byte[]]$readBuffer = New-Object byte[] 1024\n    $targetFile = New-Object IO.FileStream ($localFilePath, [IO.FileMode]::Create)\n    while ($readLength -ne 0) {\n        $readLength = $responseStream.Read($readBuffer,0,1024)\n        $targetFile.Write($readBuffer,0,$readLength)\n    }\n \n    $targetFile.close()\n}\n \nfunction Remove-FtpItem ($fullFtpPathToItem, [string]$type = \"file\") {\n    Write-Host \" deleting $item...\"\n    $ftpRequest = [System.Net.FtpWebRequest]::create($fullFtpPathToItem)\n    $ftpRequest.Credentials = $script:Credentials\n    $ftpRequest.UseBinary = $true\n    $ftpRequest.KeepAlive = $false\n \n    if($type -ieq \"file\") {\n        $ftpRequest.Method = [System.Net.WebRequestMethods+Ftp]::DeleteFile\n    } else {\n        $ftpRequest.Method = [System.Net.WebRequestMethods+Ftp]::RemoveDirectory\n    }\n \n    $ftpResponse = $ftpRequest.GetResponse()\n    \"Delete status: {0}\" -f $ftpResponse.StatusDescription\n}\n \nfunction Get-Stream($url, $meth) {\n    $fullFtpPath = [System.Net.WebRequest]::Create($url)\n    $fullFtpPath.Credentials = $script:Credentials\n    $fullFtpPath.Method = $meth\n    $response = $fullFtpPath.GetResponse()\n    return New-Object IO.StreamReader $response.GetResponseStream()\n}\n \nExport-ModuleMember Set-FtpConnection, Send-ToFtp, Get-FromFtp, Remove-FromFtp\n```","html":"<p>In my effort to learn more about PowerShell I wrote a simple ftp module. I wrote the module primary for using it in a automated deployment process with <a href=\"https://github.com/psake/psake\">Psake</a>, <a href=\"https://github.com/\">GitHub</a>, <a href=\"https://jenkins.io\">Jenkins</a> and a target environment with ftp access.  I looked around to find a similar module and the best match I found was this <a href=\"https://github.com/ruslander/Nutshells/blob/master/ftp-ls.ps1\">script</a> so the module is inspired a lot by that script.</p>\n\n<p>The module publish four methods that does what I needed the module to do for my deployment process so its not really a generic ftp module but I works for some scenarios.</p>\n\n<ul>\n<li>Set-FtpConnection</li>\n<li>Send-ToFtp</li>\n<li>Get-FromFtp</li>\n<li>Remove-FromFtp</li>\n</ul>\n\n<p>The build process that I needed to support looks like this. <strong>This post focus on the last steps; 6 and 7</strong>. Step 1 – 5 requires one or more individual posts.</p>\n\n<ol>\n<li>Setup (clean up last release, create deployment and backup folders)  </li>\n<li>Compile (.net assemblies, CoffeeSscript to JavaScript and Less to CSS + minifying)  </li>\n<li>Run unit tests and break deploy if it fails (MSpec)  </li>\n<li>Copy needed files for running the application at different target environments  </li>\n<li>Merge/create configurations for different target environments  </li>\n<li>Backup the current application from the target environment <br />\n<ul><li>Creating a ftp connection to the target environment</li>\n<li>Download the complete deployed application from the web root to the \nbackup folders that was created at setup (step 1)  </li>\n<li>Upload the backup to a backup folder at the ftp server</li></ul></li>\n<li>Redeploy the application <br />\n<ul><li>Creating a ftp connection to the target environment</li>\n<li>Drop the web root</li>\n<li>Upload the new release from the deployment folder created at setup (step 1)</li></ul></li>\n</ol>\n\n<p>The build script (based on psake) looks like this. I have removed code and properties that is not related to this post. If you want to se the complete script you can find it <a href=\"https://github.com/antonkallenberg/MvcCiTest/blob/v1.0/src/buildScripts/builder.ps1\">here</a></p>\n\n<pre><code class=\"language-powershell\">import-module .\\utilities.psm1  \nimport-module .\\ftp.psm1\n\nproperties {  \n    $TargetEnvironment = 'Debug'\n    $DateLabel = ([DateTime]::Now.ToString(\"yyyy-MM-dd_HH-mm-ss\"))\n    $ApplicationBackupRoot = \"..\\..\\Deploy\\Backup\"\n    $ApplicationBackupRootWithDateLabel = \"..\\..\\Deploy\\Backup\\$DateLabel\"\n    $BuildOutputDestinationRoot = \"..\\..\\Deploy\\Build\"\n\n    $StagingFtpUri = 'ftp://127.0.0.1:55/'\n    $StagingFtpWwwRoot = \"www\"\n    $StagingFtpBackupRoot = \"backup\"\n    $StagingFtpUsername = 'anton'\n    $StagingFtpPassword = 'anton'\n\n    #....\n    #code is removed to not loose focus from this post\n    #....\n}\n\ntask Default -depends CopyFiles\n\ntask Staging -depends DeployWebToStagingFtp\n\ntask DeployWebToStagingFtp -depends BackupWebAtStagingFtp {  \n    $fullBuildOutputDestinationRoot = Resolve-Path $BuildOutputDestinationRoot\n    Set-FtpConnection $StagingFtpUri $StagingFtpUsername $StagingFtpPassword\n    Remove-FromFtp $StagingFtpWwwRoot\n    Send-ToFtp $fullBuildOutputDestinationRoot $StagingFtpWwwRoot\n}\n\ntask BackupWebAtStagingFtp -depends MergeConfiguration {  \n    $fullSourcePath = Resolve-Path $ApplicationBackupRootWithDateLabel\n    $fullApplicationBackupRootPath = Resolve-Path $ApplicationBackupRoot\n    Set-FtpConnection $StagingFtpUri $StagingFtpUsername $StagingFtpPassword\n    Get-FromFtp $fullSourcePath $StagingFtpWwwRoot\n    Send-ToFtp $fullApplicationBackupRootPath $StagingFtpBackupRoot\n}\n\ntask MergeConfiguration -depends CopyFiles {  \n    #....\n    #code is removed to not loose focus from this post\n    #....\n}\n\ntask CopyFiles -depends Test {  \n    #....\n    #code is removed to not loose focus from this post\n    #....\n}\n\ntask Test -depends Compile, Setup {  \n    #....\n    #code is removed to not loose focus from this post\n    #....\n}\n\ntask Compile -depends Setup {  \n    #....\n    #code is removed to not loose focus from this post\n    #....\n}\n\ntask Setup {  \n    Add-FolderIfMissing $BuildOutputDestinationRoot\n    if (!($TargetEnvironment -ieq 'debug')) {\n        Add-FolderIfMissing $ApplicationBackupRoot\n        Add-FolderIfMissing $ApplicationBackupRootWithDateLabel\n    }\n}\n</code></pre>\n\n<p>As you can see this version of the script only supports deploys to the staging environment but we could quite easily add support to other environments too. The code shows how the ftp module is used in a psake build script. Standalone in a PowerShell session it would something like this.</p>\n\n<pre><code class=\"language-powershell\">PS C:\\Projects\\MvcCiTest\\src\\buildScripts&gt; Set-ExecutionPolicy RemoteSigned\n\nExecution Policy Change  \nThe execution policy helps protect you from scripts that you do not trust. Changing the execution policy might expose  \nyou to the security risks described in the about_Execution_Policies help topic. Do you want to change the execution  \npolicy?  \n[Y] Yes  [N] No  [S] Suspend  [?] Help (default is \"Y\"):\nPS C:\\Projects\\MvcCiTest\\src\\buildScripts&gt; Import-Module .\\ftp.psm1  \nPS C:\\Projects\\MvcCiTest\\src\\buildScripts&gt; Set-FtpConnection \"ftp://127.0.0.1:21/\" \"anton\" \"anton\"  \nPS C:\\Projects\\MvcCiTest\\src\\buildScripts&gt; Send-ToFtp \"C:\\Users\\Anton\\Pictures\\\" \"www/pictures\"  \nuploading cheetah-picture.jpg  \nPS C:\\Projects\\MvcCiTest\\src\\buildScripts&gt; Get-FromFtp \"C:\\temp\" \"www/pictures\"  \nDownloading ftp://127.0.0.1:21/www/pictures/heetah-picture.jpg ...  \nPS C:\\Projects\\MvcCiTest\\src\\buildScripts&gt; Remove-FromFtp \"www/pictures\"  \n deleting ...\nDelete status: 250 File deleted successfully\n\nPS C:\\Projects\\MvcCiTest\\src\\buildScripts&gt; Remove-Module ftp  \n</code></pre>\n\n<p>So finally… the first version of my ftp module looks like this. Would appreciate any suggestions on how to improve the module the code is available at <a href=\"https://github.com/antonkallenberg/MvcCiTest/blob/master/src/buildScripts/ftp.psm1\">GitHub</a> feel free to send me a pull request :)</p>\n\n<pre><code class=\"language-powershell\">[string]$script:ftpHost\n[string]$script:username\n[string]$script:password\n[System.Net.NetworkCredential]$script:Credentials\n\nfunction Set-FtpConnection {  \n    param([string]$host, [string]$username, [string]$password)\n\n    $script:Credentials = New-Object System.Net.NetworkCredential($username, $password)\n    $script:ftpHost  = $host\n    $script:username = $username\n    $script:password = $password\n}\n\nfunction Send-ToFtp {  \n    param([string]$sourcePath, [string]$ftpFolder)\n\n    foreach($item in Get-ChildItem -recurse $sourcePath){\n        $itemName = [System.IO.Path]::GetFullPath($item.FullName).SubString([System.IO.Path]::GetFullPath($sourcePath).Length + 1)\n        $fullFtpPath = [System.IO.Path]::Combine($script:ftpHost+\"/$ftpFolder/\", $itemName)\n        if ($item.Attributes -eq \"Directory\"){\n            try{\n                $uri = New-Object System.Uri($fullFtpPath)\n                $fullFtpPathRequest = [System.Net.WebRequest]::Create($uri)\n                $fullFtpPathRequest.Credentials = $script:Credentials\n                $fullFtpPathRequest.Method = [System.Net.WebRequestMethods+Ftp]::MakeDirectory\n                $fullFtpPathRequest.GetResponse()\n            }catch [Net.WebException] {\n                Write-Host \"$item probably exists ...\"\n            }\n            continue;\n        }\n\n        $webClient = New-Object System.Net.WebClient\n        $webClient.Credentials = $script:Credentials\n        $uri = New-Object System.Uri($fullFtpPath)\n        Write-Host \"uploading $item\"\n       $webClient.UploadFile($uri, $item.FullName)\n    }\n}\n\nfunction Get-FromFtp {  \n    param([string]$sourceFolder, [string]$ftpFolder)\n\n    $fullFtpPath = [System.IO.Path]::Combine($script:ftpHost, $ftpFolder)\n    $dirs = Get-FtpDirecoryTree $fullFtpPath\n    foreach($dir in $dirs){\n       $path = [io.path]::Combine($sourceFolder, $dir)\n       if ((Test-Path $path) -eq $false) {\n          New-Item -Path $path -ItemType Directory | Out-Null\n       }\n    }\n    $files = Get-FtpFilesTree $fullFtpPath\n    foreach($file in $files) {\n        $ftpPath = $fullFtpPath + \"/\" + $file\n        $localFilePath = [io.path]::Combine($sourceFolder, $file)\n        Write-Host \"Downloading $ftpPath ...\"\n        Get-FtpFile $ftpPath $localFilePath\n    }\n}\n\nfunction Remove-FromFtp($ftpFolder) {  \n    $fullFtpPath = [System.IO.Path]::Combine($script:ftpHost, $ftpFolder)\n    $fileTree = Get-FtpFilesTree $fullFtpPath\n    if($fileTree -gt 0){\n        foreach($file in $fileTree) {\n            $ftpFile = [io.path]::Combine($fullFtpPath, $file)\n            Remove-FtpItem $ftpFile \"file\"\n        }\n    }\n    $dirTree = [array](Get-FtpDirecoryTree $fullFtpPath) | sort -Property @{ Expression = {$_.Split('/').Count} } -Desc\n    if($dirTree -gt 0) {\n        foreach($dir in $dirTree) {\n            $ftpDir = [io.path]::Combine($fullFtpPath, $dir)\n            Remove-FtpItem $ftpDir \"directory\"\n        }\n    }\n}\n\nfunction Get-FtpDirecoryTree($fullFtpPath) {  \n    if($fullFtpPath.EndsWith(\"/\") -eq $false) {\n        $fullFtpPath = $fullFtpPath += \"/\"\n    }\n\n    $folderTree = New-Object \"System.Collections.Generic.List[string]\"\n    $folders = New-Object \"System.Collections.Generic.Queue[string]\"\n    $folders.Enqueue($fullFtpPath)\n    while($folders.Count -gt 0) {\n        $folder = $folders.Dequeue()\n        $directoryContent = Get-FtpDirectoryContent $folder\n        $dirs = Get-FtpDirectories $folder\n        foreach ($line in $dirs){\n            $dir = @($directoryContent | Where { $line.EndsWith($_) })[0]\n            [void]$directoryContent.Remove($dir)\n            $folders.Enqueue($folder + $dir + \"/\")\n            $folderTree.Add($folder.Replace($fullFtpPath, \"\") + $dir + \"/\")\n        }\n    }\n    return ,$folderTree\n}\n\nfunction Get-FtpFilesTree($fullFtpPath) {  \n    if($fullFtpPath.EndsWith(\"/\") -eq $false) {\n        $fullFtpPath = $fullFtpPath += \"/\"\n    }\n\n    $fileTree = New-Object \"System.Collections.Generic.List[string]\"\n    $folders = New-Object \"System.Collections.Generic.Queue[string]\"\n    $folders.Enqueue($fullFtpPath)\n    while($folders.Count -gt 0){\n        $folder = $folders.Dequeue()\n        $directoryContent = Get-FtpDirectoryContent $folder\n        $dirs = Get-FtpDirectories $folder\n        foreach ($line in $dirs){\n            $dir = @($directoryContent | Where { $line.EndsWith($_) })[0]\n            [void]$directoryContent.Remove($dir)\n            $folders.Enqueue($folder + $dir + \"/\")\n        }\n        $directoryContent | ForEach {\n            $fileTree.Add($folder.Replace($fullFtpPath, \"\") + $_)\n        }\n    }\n\n    return ,$fileTree\n}\n\nfunction Get-FtpDirectories($folder) {  \n    $dirs = New-Object \"system.collections.generic.list[string]\"\n    $operation = [System.Net.WebRequestMethods+Ftp]::ListDirectoryDetails\n    $reader = Get-Stream $folder $operation\n    while (($line = $reader.ReadLine()) -ne $null) {\n       if ($line.Trim().ToLower().StartsWith(\"d\") -or $line.Contains(\" &lt;DIR&gt; \")) {\n            $dirs.Add($line)\n        }\n    }\n    $reader.Dispose();\n    return ,$dirs\n}\n\nfunction Get-FtpDirectoryContent($folder) {  \n    $files = New-Object \"System.Collections.Generic.List[String]\"\n    $operation = [System.Net.WebRequestMethods+Ftp]::ListDirectory\n    $reader = Get-Stream $folder $operation\n    while (($line = $reader.ReadLine()) -ne $null) {\n       $files.Add($line.Trim())\n    }\n    $reader.Dispose();\n    return ,$files\n}\n\nfunction Get-FtpFile($ftpPath, $localFilePath) {  \n    $ftpRequest = [System.Net.FtpWebRequest]::create($ftpPath)\n    $ftpRequest.Credentials = $script:Credentials\n    $ftpRequest.Method = [System.Net.WebRequestMethods+Ftp]::DownloadFile\n    $ftpRequest.UseBinary = $true\n    $ftpRequest.KeepAlive = $false\n    $ftpResponse = $ftpRequest.GetResponse()\n    $responseStream = $ftpResponse.GetResponseStream()\n\n    [byte[]]$readBuffer = New-Object byte[] 1024\n    $targetFile = New-Object IO.FileStream ($localFilePath, [IO.FileMode]::Create)\n    while ($readLength -ne 0) {\n        $readLength = $responseStream.Read($readBuffer,0,1024)\n        $targetFile.Write($readBuffer,0,$readLength)\n    }\n\n    $targetFile.close()\n}\n\nfunction Remove-FtpItem ($fullFtpPathToItem, [string]$type = \"file\") {  \n    Write-Host \" deleting $item...\"\n    $ftpRequest = [System.Net.FtpWebRequest]::create($fullFtpPathToItem)\n    $ftpRequest.Credentials = $script:Credentials\n    $ftpRequest.UseBinary = $true\n    $ftpRequest.KeepAlive = $false\n\n    if($type -ieq \"file\") {\n        $ftpRequest.Method = [System.Net.WebRequestMethods+Ftp]::DeleteFile\n    } else {\n        $ftpRequest.Method = [System.Net.WebRequestMethods+Ftp]::RemoveDirectory\n    }\n\n    $ftpResponse = $ftpRequest.GetResponse()\n    \"Delete status: {0}\" -f $ftpResponse.StatusDescription\n}\n\nfunction Get-Stream($url, $meth) {  \n    $fullFtpPath = [System.Net.WebRequest]::Create($url)\n    $fullFtpPath.Credentials = $script:Credentials\n    $fullFtpPath.Method = $meth\n    $response = $fullFtpPath.GetResponse()\n    return New-Object IO.StreamReader $response.GetResponseStream()\n}\n\nExport-ModuleMember Set-FtpConnection, Send-ToFtp, Get-FromFtp, Remove-FromFtp  \n</code></pre>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":1460992414299,"created_by":1,"updated_at":1461136881026,"updated_by":1,"published_at":1343216220000,"published_by":1},{"id":10,"uuid":"06b080e5-a3cf-47be-aaa0-20a2ff26524d","title":"USING SERVICESTACK BUNDLER","slug":"using-servicestack-bundler","markdown":"Bundler by [ServiceStack](https://servicestack.net/) is a solution for bundling, minifying and compiling client script (JavaScript/CoffeScript(!)) and style sheets (CSS, LESS and SASS). There are a lot of this sort of solutions out there but I really like this one, hopefully you will too after reading this. At the [projects GitHub page](https://github.com/ServiceStack/Bundler) you can find some really [good documentation](https://github.com/ServiceStack/Bundler#readme) but I will write this post anyway and hopefully it would be even clearer how to use Bundler in your solution.\n\nBundler is implemented in JavaScript and runs through [NodeJS](http://nodejs.org). Bundler its lightweight, easy to configure/customize and simple to get into your own build process. Bundler will out of the box run in a ASP.NET MVC solution and with some tweaking it can run with web forms too. But that we don’t need to do now when we have MVC support within EPiServer 7 (preview)\n\n## Install\nBundler is available as a Nuget package, you could probably install it manually by downloading the source from github but I will go with Nuget for now:\n\n![](/content/images/2016/04/10.png)\n\nAfter the installation you will have some new folders and a new c# file (Mvc.Bundler.cs) in your solution.\n\n![](/content/images/2016/04/11.png)\n\nThe bundler-folder is where Bundler lives, here we have a full distribution of node.js (node.exe), the source for bundler (bundler.js), all the modules that bundler relies on and a Visual Studio 2010 extension(!).\n\nThe Visual Studio extension is quite useful for local development but I have experienced some problems with it, more on local development later. To install the extension just double click the BundlerRunOnSave.vsix and restart Visual Studio. After the install Bundler will run each time you save a related file.\n\n## Configure it!\nMy file structure for this example looks like this:\n\n![](/content/images/2016/04/12.png)\n\nBundler will by default look in the folders “~/Content” and “~/Scripts” for “bundle configurations”, this can be configured in “~/bundler/bundler.cmd”. You can see that I have defined two bundles “app.css.bundle” and “app.js.bundle” this is the configuration for which files that should belong to which bundle. The configuration looks like this:\n\n**app.css.bundle:**\n```text\nNormalize.css\nSite.css\nStartpage.less\n```\n\n**app.js.bundle::**\n```text\nNormalize.css\nSite.css\nStartpage.less\n```\nThe bundles can contain both JavaScript and CoffeeScript files or CSS, Less or Sass files. When Bundler runs it will compile and generate the bundles to something most browsers can understand, JavaScript and CSS!\n\n## The client code\nFor this post I have created some simple CSS/Less and JavaScript/CoffeeScript just to demonstrate how bundler works. The CSS will first normalize the default appearance to get almost the same default behavior in different browsers, set some background colors, add some colors and font sizes to the start page´s h2 and li-elements.\n\nThe JavaScript will run when the DOM is loaded and overwrite the color that has been set by the CSS for the start page´s h2 elements.\n\n**Normalize.css ([view](http://necolas.github.io/normalize.css/))**\n\n**Site.css**\n```css\nbody {\n    background: #fff;\n}\nmenu#TopMenu .selected {\n    font-weight: bold;\n}\n```\n\n**Startpage.less**\n```less\n@listFontSize: 13px;\n@listFontColor: #f00;\n \nbody#Startpage {\n    ul {\n        list-style-type:none; \n        li {\n            font-size:@listFontSize;\n            a {\n                color:@listFontColor;\n            }\n        }\n    }\n}\n```\n\n**Normalize.css ([view](http://code.jquery.com/jquery-1.7.2.js))**\n\n**Bootstrapper.coffee**\n\n```coffeescript\nclass Bootstrapper\n    setup: ->\n        bodyId = $(\"body\").attr 'id'\n        if bodyId is \"Startpage\"\n            startPage = new Startpage\n            startPage.load()\n            return\n \n($ document).ready ->\n    strapper = new Bootstrapper\n    strapper.setup()\n```\n\n**Startpage.js**\n\n```javascript\nvar Startpage = function () { };\nStartpage.prototype.load = function () {\n    $(\"h2\").css(\"color\", \"green\");\n};\n```\n\n## Adding references to bundles in “_Layout”-files\nAs I wrote earlier; a c#-file was added to the solution after the installation of Bundler, this file (Mvc.Bundler.cs) contains some html helpers for referencing bundles and some other sweet features. By using those extensions you will get support for:\n\n* Versioning of files for client caching\n * A version number is added as a querysting based on the files last write time so the client cache is dropped when new versions of your CSS/Javascript files are created.\n* Choose how to render your bundle. Will show how you can render all full files in debug mode but combined and minified in release mode. You can choose from:\n * Normal\n * Minified\n * Combined\n * MinifiedAndCombined\n* Renders the html for the script and css tags\n\n**The_Layout file:**\n```html\n@using ServiceStack.Mvc\n@using EPiServer.Web.Mvc.Html\n@using EPiBooks.Models\n \n<!DOCTYPE html>\n<html>\n<head>\n    <meta name=\"viewport\" content=\"width=device-width\" />\n    <title>@ViewBag.Title</title>\n    @if (HttpContext.Current.IsDebuggingEnabled) {\n        @Html.RenderCssBundle(\"~/Content/app.css.bundle\", BundleOptions.Normal)\n        @Html.RenderJsBundle(\"~/Scripts/app.js.bundle\", BundleOptions.Normal)\n    } else {\n        @Html.RenderCssBundle(\"~/Content/app.css.bundle\", BundleOptions.MinifiedAndCombined)\n        @Html.RenderJsBundle(\"~/Scripts/app.js.bundle\", BundleOptions.MinifiedAndCombined)\n    }\n</head>\n<body id=\"@ViewBag.BodyId\">\n    @{Html.RenderEPiServerQuickNavigator();}\n    <section id=\"MainSection\">  \n        @Html.Partial(\"TopMenu\", new NavigationContext())\n        @RenderBody()\n    </section>\n</body>\n</html>\n```\n\nThe layout checks if the application is in debug mode (debug is set to true in web.config) and then renders all the css and js files without minifying them otherwise (debug is set to false in web.config) all js and css files and minified and combinde into single files.\n\nA cool thing is that Bundler already has created the files needed as physical files on disk so no code is executed at runtime for rendering the js or css files. Files is just fetched as physical files from the servers disk.\n\n## Building for production and local development\n\nBecause the files are pre-generated and not generated at runtime we need to generate the CSS and JavaScript files each time we do a change when developing or releasing to test/production environments.\n\n### Local development\nFor local development Bundler ships with a Visual Studio extension that generates new files each time a js, css, less, sass, coffee or bundle file is edited. The VS-extension is pretty sweet but I have notice that Visual Studio will crash if you create a major syntax error in your JavaScript file and then hit ctrl + s, probably this will be fixed.\n\nBecause of this we could instead generate the files in compile-time, this requires that the project is rebuilt every time you make a change to a js, css, less, sass, coffee or bundle file. This is not that strange for us backend developers but I can only imagine what the UI developers will say about that.\n\nI created a simple one-liner-PowerShell-script that execute Bundler through Node:\n\n```powershell\n&\"..\\bundler\\node.exe\" \"..\\bundler\\bundler.js\" \"..\\Content\" \"..\\Scripts\"\n```\n\nThen added a post-build event:\n![](/content/images/2016/04/13.png)\n\nNow every client resource is compiled, bundled and minified into JavaScript and CSS when the project is compiled.\n\n### Building for remote environments\nWhen our build server is building to remote environments like test and production we also need to run Bundler for compiling and minifying. Fortunately it is as simple as adding that one-liner-PowerShell-script to your build process. Lets say you where building to your remote server with Psake then it would look something like this:\n\n```powershell\nproperties {\n    $BuildConfiguration = 'Release'\n    $CssFilesRoot = \"Content\"\n    $WebScriptFilesRoot = \"Scripts\"\n    $ApplicationSource = '..\\MvcCiTest'\n    $ApplicationSlnFile = '..\\MvcCiTest.sln'\n}\n \ntask Default -depends CopyFiles\n \ntask Staging -depends DeployWebToStagingFtp \n \ntask DeployWebToStagingFtp -depends BackupWebAtStagingFtp {\n    # deploy to staging\n}\n \ntask BackupWebAtStagingFtp -depends MergeConfiguration {\n    # backup staging before deploy\n}\n \ntask MergeConfiguration -depends CopyFiles { \n    # merge configuration\n}\n \ntask CopyFiles -depends Test {\n    # copy needed files\n}\n \ntask Test -depends Compile, Setup { \n    # run unit tests\n}\n \ntask Compile -depends Setup { \n    Exec {\n        msbuild $ApplicationSlnFile /t:Clean /t:Build /p:Configuration=$BuildConfiguration /v:q /nologo \n    }\n    &\"$ApplicationSource\\bundler\\node.exe\" \"$ApplicationSource\\bundler\\bundler.js\" \"$ApplicationSource\\$CssFilesRoot\" \"$ApplicationSource\\$WebScriptFilesRoot\"\n}\n \ntask Setup { \n    # do stuff needed for the release\n}\n``` \n\n## The results\nAs I said; this is a really simple UI… so it looks like this.\n\n![](/content/images/2016/04/14.png)\n\nBut the cool thing is of course the bundling, compiling and minifying!\n\n## Debug mode\n![](/content/images/2016/04/15.png)\n\n## Production mode\n![](/content/images/2016/04/16.png)\n\n## Complete source\nYou can view the complete source at [GitHub](https://github.com/antonkallenberg/EPiBooks/tree/BundlerV1.0).\n\nThanks for reading!\n","html":"<p>Bundler by <a href=\"https://servicestack.net/\">ServiceStack</a> is a solution for bundling, minifying and compiling client script (JavaScript/CoffeScript(!)) and style sheets (CSS, LESS and SASS). There are a lot of this sort of solutions out there but I really like this one, hopefully you will too after reading this. At the <a href=\"https://github.com/ServiceStack/Bundler\">projects GitHub page</a> you can find some really <a href=\"https://github.com/ServiceStack/Bundler#readme\">good documentation</a> but I will write this post anyway and hopefully it would be even clearer how to use Bundler in your solution.</p>\n\n<p>Bundler is implemented in JavaScript and runs through <a href=\"http://nodejs.org\">NodeJS</a>. Bundler its lightweight, easy to configure/customize and simple to get into your own build process. Bundler will out of the box run in a ASP.NET MVC solution and with some tweaking it can run with web forms too. But that we don’t need to do now when we have MVC support within EPiServer 7 (preview)</p>\n\n<h2 id=\"install\">Install</h2>\n\n<p>Bundler is available as a Nuget package, you could probably install it manually by downloading the source from github but I will go with Nuget for now:</p>\n\n<p><img src=\"/content/images/2016/04/10.png\" alt=\"\" /></p>\n\n<p>After the installation you will have some new folders and a new c# file (Mvc.Bundler.cs) in your solution.</p>\n\n<p><img src=\"/content/images/2016/04/11.png\" alt=\"\" /></p>\n\n<p>The bundler-folder is where Bundler lives, here we have a full distribution of node.js (node.exe), the source for bundler (bundler.js), all the modules that bundler relies on and a Visual Studio 2010 extension(!).</p>\n\n<p>The Visual Studio extension is quite useful for local development but I have experienced some problems with it, more on local development later. To install the extension just double click the BundlerRunOnSave.vsix and restart Visual Studio. After the install Bundler will run each time you save a related file.</p>\n\n<h2 id=\"configureit\">Configure it!</h2>\n\n<p>My file structure for this example looks like this:</p>\n\n<p><img src=\"/content/images/2016/04/12.png\" alt=\"\" /></p>\n\n<p>Bundler will by default look in the folders “~/Content” and “~/Scripts” for “bundle configurations”, this can be configured in “~/bundler/bundler.cmd”. You can see that I have defined two bundles “app.css.bundle” and “app.js.bundle” this is the configuration for which files that should belong to which bundle. The configuration looks like this:</p>\n\n<p><strong>app.css.bundle:</strong></p>\n\n<pre><code class=\"language-text\">Normalize.css  \nSite.css  \nStartpage.less  \n</code></pre>\n\n<p><strong>app.js.bundle::</strong></p>\n\n<pre><code class=\"language-text\">Normalize.css  \nSite.css  \nStartpage.less  \n</code></pre>\n\n<p>The bundles can contain both JavaScript and CoffeeScript files or CSS, Less or Sass files. When Bundler runs it will compile and generate the bundles to something most browsers can understand, JavaScript and CSS!</p>\n\n<h2 id=\"theclientcode\">The client code</h2>\n\n<p>For this post I have created some simple CSS/Less and JavaScript/CoffeeScript just to demonstrate how bundler works. The CSS will first normalize the default appearance to get almost the same default behavior in different browsers, set some background colors, add some colors and font sizes to the start page´s h2 and li-elements.</p>\n\n<p>The JavaScript will run when the DOM is loaded and overwrite the color that has been set by the CSS for the start page´s h2 elements.</p>\n\n<p><strong>Normalize.css (<a href=\"http://necolas.github.io/normalize.css/\">view</a>)</strong></p>\n\n<p><strong>Site.css</strong></p>\n\n<pre><code class=\"language-css\">body {  \n    background: #fff;\n}\nmenu#TopMenu .selected {  \n    font-weight: bold;\n}\n</code></pre>\n\n<p><strong>Startpage.less</strong></p>\n\n<pre><code class=\"language-less\">@listFontSize: 13px;\n@listFontColor: #f00;\n\nbody#Startpage {  \n    ul {\n        list-style-type:none; \n        li {\n            font-size:@listFontSize;\n            a {\n                color:@listFontColor;\n            }\n        }\n    }\n}\n</code></pre>\n\n<p><strong>Normalize.css (<a href=\"http://code.jquery.com/jquery-1.7.2.js\">view</a>)</strong></p>\n\n<p><strong>Bootstrapper.coffee</strong></p>\n\n<pre><code class=\"language-coffeescript\">class Bootstrapper  \n    setup: -&gt;\n        bodyId = $(\"body\").attr 'id'\n        if bodyId is \"Startpage\"\n            startPage = new Startpage\n            startPage.load()\n            return\n\n($ document).ready -&gt;\n    strapper = new Bootstrapper\n    strapper.setup()\n</code></pre>\n\n<p><strong>Startpage.js</strong></p>\n\n<pre><code class=\"language-javascript\">var Startpage = function () { };  \nStartpage.prototype.load = function () {  \n    $(\"h2\").css(\"color\", \"green\");\n};\n</code></pre>\n\n<h2 id=\"addingreferencestobundlesin_layoutfiles\">Adding references to bundles in “_Layout”-files</h2>\n\n<p>As I wrote earlier; a c#-file was added to the solution after the installation of Bundler, this file (Mvc.Bundler.cs) contains some html helpers for referencing bundles and some other sweet features. By using those extensions you will get support for:</p>\n\n<ul>\n<li>Versioning of files for client caching\n<ul><li>A version number is added as a querysting based on the files last write time so the client cache is dropped when new versions of your CSS/Javascript files are created.</li></ul></li>\n<li>Choose how to render your bundle. Will show how you can render all full files in debug mode but combined and minified in release mode. You can choose from:\n<ul><li>Normal</li>\n<li>Minified</li>\n<li>Combined</li>\n<li>MinifiedAndCombined</li></ul></li>\n<li>Renders the html for the script and css tags</li>\n</ul>\n\n<p><strong>The_Layout file:</strong></p>\n\n<pre><code class=\"language-html\">@using ServiceStack.Mvc\n@using EPiServer.Web.Mvc.Html\n@using EPiBooks.Models\n\n&lt;!DOCTYPE html&gt;  \n&lt;html&gt;  \n&lt;head&gt;  \n    &lt;meta name=\"viewport\" content=\"width=device-width\" /&gt;\n    &lt;title&gt;@ViewBag.Title&lt;/title&gt;\n    @if (HttpContext.Current.IsDebuggingEnabled) {\n        @Html.RenderCssBundle(\"~/Content/app.css.bundle\", BundleOptions.Normal)\n        @Html.RenderJsBundle(\"~/Scripts/app.js.bundle\", BundleOptions.Normal)\n    } else {\n        @Html.RenderCssBundle(\"~/Content/app.css.bundle\", BundleOptions.MinifiedAndCombined)\n        @Html.RenderJsBundle(\"~/Scripts/app.js.bundle\", BundleOptions.MinifiedAndCombined)\n    }\n&lt;/head&gt;  \n&lt;body id=\"@ViewBag.BodyId\"&gt;  \n    @{Html.RenderEPiServerQuickNavigator();}\n    &lt;section id=\"MainSection\"&gt;  \n        @Html.Partial(\"TopMenu\", new NavigationContext())\n        @RenderBody()\n    &lt;/section&gt;\n&lt;/body&gt;  \n&lt;/html&gt;  \n</code></pre>\n\n<p>The layout checks if the application is in debug mode (debug is set to true in web.config) and then renders all the css and js files without minifying them otherwise (debug is set to false in web.config) all js and css files and minified and combinde into single files.</p>\n\n<p>A cool thing is that Bundler already has created the files needed as physical files on disk so no code is executed at runtime for rendering the js or css files. Files is just fetched as physical files from the servers disk.</p>\n\n<h2 id=\"buildingforproductionandlocaldevelopment\">Building for production and local development</h2>\n\n<p>Because the files are pre-generated and not generated at runtime we need to generate the CSS and JavaScript files each time we do a change when developing or releasing to test/production environments.</p>\n\n<h3 id=\"localdevelopment\">Local development</h3>\n\n<p>For local development Bundler ships with a Visual Studio extension that generates new files each time a js, css, less, sass, coffee or bundle file is edited. The VS-extension is pretty sweet but I have notice that Visual Studio will crash if you create a major syntax error in your JavaScript file and then hit ctrl + s, probably this will be fixed.</p>\n\n<p>Because of this we could instead generate the files in compile-time, this requires that the project is rebuilt every time you make a change to a js, css, less, sass, coffee or bundle file. This is not that strange for us backend developers but I can only imagine what the UI developers will say about that.</p>\n\n<p>I created a simple one-liner-PowerShell-script that execute Bundler through Node:</p>\n\n<pre><code class=\"language-powershell\">&amp;\"..\\bundler\\node.exe\" \"..\\bundler\\bundler.js\" \"..\\Content\" \"..\\Scripts\"\n</code></pre>\n\n<p>Then added a post-build event: <br />\n<img src=\"/content/images/2016/04/13.png\" alt=\"\" /></p>\n\n<p>Now every client resource is compiled, bundled and minified into JavaScript and CSS when the project is compiled.</p>\n\n<h3 id=\"buildingforremoteenvironments\">Building for remote environments</h3>\n\n<p>When our build server is building to remote environments like test and production we also need to run Bundler for compiling and minifying. Fortunately it is as simple as adding that one-liner-PowerShell-script to your build process. Lets say you where building to your remote server with Psake then it would look something like this:</p>\n\n<pre><code class=\"language-powershell\">properties {  \n    $BuildConfiguration = 'Release'\n    $CssFilesRoot = \"Content\"\n    $WebScriptFilesRoot = \"Scripts\"\n    $ApplicationSource = '..\\MvcCiTest'\n    $ApplicationSlnFile = '..\\MvcCiTest.sln'\n}\n\ntask Default -depends CopyFiles\n\ntask Staging -depends DeployWebToStagingFtp \n\ntask DeployWebToStagingFtp -depends BackupWebAtStagingFtp {  \n    # deploy to staging\n}\n\ntask BackupWebAtStagingFtp -depends MergeConfiguration {  \n    # backup staging before deploy\n}\n\ntask MergeConfiguration -depends CopyFiles {  \n    # merge configuration\n}\n\ntask CopyFiles -depends Test {  \n    # copy needed files\n}\n\ntask Test -depends Compile, Setup {  \n    # run unit tests\n}\n\ntask Compile -depends Setup {  \n    Exec {\n        msbuild $ApplicationSlnFile /t:Clean /t:Build /p:Configuration=$BuildConfiguration /v:q /nologo \n    }\n    &amp;\"$ApplicationSource\\bundler\\node.exe\" \"$ApplicationSource\\bundler\\bundler.js\" \"$ApplicationSource\\$CssFilesRoot\" \"$ApplicationSource\\$WebScriptFilesRoot\"\n}\n\ntask Setup {  \n    # do stuff needed for the release\n}\n</code></pre>\n\n<h2 id=\"theresults\">The results</h2>\n\n<p>As I said; this is a really simple UI… so it looks like this.</p>\n\n<p><img src=\"/content/images/2016/04/14.png\" alt=\"\" /></p>\n\n<p>But the cool thing is of course the bundling, compiling and minifying!</p>\n\n<h2 id=\"debugmode\">Debug mode</h2>\n\n<p><img src=\"/content/images/2016/04/15.png\" alt=\"\" /></p>\n\n<h2 id=\"productionmode\">Production mode</h2>\n\n<p><img src=\"/content/images/2016/04/16.png\" alt=\"\" /></p>\n\n<h2 id=\"completesource\">Complete source</h2>\n\n<p>You can view the complete source at <a href=\"https://github.com/antonkallenberg/EPiBooks/tree/BundlerV1.0\">GitHub</a>.</p>\n\n<p>Thanks for reading!</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":1461134721609,"created_by":1,"updated_at":1461136100962,"updated_by":1,"published_at":1343302620000,"published_by":1},{"id":11,"uuid":"ea92aaba-c4fb-4222-9e33-5051181525e7","title":"Ideas for new posts","slug":"ideas-for-new-posts","markdown":"* Pushing test result to Elastic Search\n* Building CloudMon","html":"<ul>\n<li>Pushing test result to Elastic Search</li>\n<li>Building CloudMon</li>\n</ul>","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":1461137914811,"created_by":1,"updated_at":1461137945136,"updated_by":1,"published_at":null,"published_by":null},{"id":12,"uuid":"99e2e071-8c28-4610-9f23-f5f327180755","title":"HOOKED ON COFFEE","slug":"hooked-on-coffeescript","markdown":"More and more I find myself talking about CoffeeScript and all the things I like about the language so I thought it would be a great idea to summarize all my talking in a post. Before I begin I must say that one of the best things about CoffeeScript is that it has given me a greater understanding for JavaScript.\n\n## Me and JavaScript\nI have always had a love/hate relationship with JavaScript. Actually, JavaScript were one of the first languages I wrote code in and JavaScript gave me the inspiration to keep coding and learning new languages. Since then the JavaScript community has really exploded with all kinds of cool libraries and frameworks and I think we will see more and more around Node.js like [Meteor](https://www.meteor.com/)… but JavaScript is not a smooth language to work with in my opinion. It could be quite nice after adding some libraries to your codebase but I find native JavaScript quite poor and missing a lot of language features. Don’t get me wrong JavaScript has a nice object model and some good parts but I don’t think JavaScript is exposing them as well as CoffeeScript does.\n\n## Why I like CoffeScript\nCoffeeScript is a language that compiles into JavaScript. Everything that is possible in CoffeeScript is possible to achieve with native JavaScript but I promise you that it’s a lot of extra effort. Relax, I will not write about every language feature, you can find that type of reference documentation [here](http://coffeescript.org/). I will show some cool things that I hope will convert you to a CoffeeScript lover!\n\n### Prototypes… classes?\nI have seen a lot of [examples](http://www.phpied.com/3-ways-to-define-a-javascript-class/) of prototyping and [libraries for simulating classes in JavaScript](http://classy.pocoo.org/), some of them are really nice but which one to choose and should you really need to add a library to get support for classes? Some of you might say “Yeah, yeah… JavaScript is very object oriented without classes and we don’t need them”, that is maybe true… but why do we have all of those libraries that simulates classes?… it seems that at least a part of the JavaScript community wants classes.\n\nOkay, [so a JavaScript function is a object](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Function), functions can then be used to simulate classes and we can use prototypes to extend that function-object. Prototyping in JavaScript is quite hard to get right and you need to decide which approach to use. CoffeeScript makes that decision for you and implements it quite well through the keyword class(!)\n\nCoffeScript to the left, JavaScript to the right.\n\n![](/content/images/2016/04/20.png)\n\nPrototyping is even harder if you want one JavaScript object to inherit from another. CoffeeScript does that through the keyword \"extends\".\n\n```coffeescript\nclass Person\n    constructor: (@firstName, @lastName) ->\n \n    getName: ->\n        @firstName + \" \" + @lastName\n \nclass Developer extends Person\n    constructor: (first, last) ->\n        super first, last\n \nanton = new Developer \"Anton\", \"Kallenberg\"\nconsole.log(do anton.getName)\n```\n\nCoffeeScript also has a shortcut for extending the prototype of an existing object, using the \"::\"-operator\n\n![](/content/images/2016/04/21.png)\n\nNote that I compare with != in CoffeScript but it translates to !== in JavaScript. CoffeeScript does not have “loose type comparison” to avoid unexpected results of [type coercion](https://developer.mozilla.org/en-US/docs/Web/JavaScript/A_re-introduction_to_JavaScript?redirectlocale=en-US&redirectslug=A_re-introduction_to_JavaScript#Operators).\n\n## Scoping\nYou may noticed that I prefixed my argument variables with @ in the previous example of the Person class. @ is a shorthand for “this”. Also by adding the @ to the method arguments I tell the CoffeeScript compiler to create backing fields for the arguments and to make them private.\n\nThe [fat arrow](http://blog.paracode.com/2011/08/28/coffeescript-fat-arrows/) (=>) is another way that CoffeeScript handles scoping. The fat arrow binds “this” to the current instance of the class for the function defined.\n\n![](/content/images/2016/04/22.png)\n\n\nThe fat arrow can for example be useful when calling a callback in a different scope of \"this\" and want \"this\" to be the class instance, for example when using jQuerys click event\n\n![](/content/images/2016/04/23.png)\n\n## The existential operator\nIf you are not convinced yet that CoffeeScript rocks the existential operator maybe makes it more interesting. Lets say you want to check the property containing work description of  some employees objects but you are not sure if the employee object is null or the name of the  property containing the  work description. In JavaScript this would look something like this:\n\n```javascript\n// ...Don't even bother...\n```\n\nWell, it would be quite a lot of code, in CoffeeScript that could be written with the existential operator (\"?\").\n\n```coffeescript\ndeveloper = cto = hr = null\ndeveloper = {desc : \"coding 4 food!\"}\n \nemployee = projectManager ? cto ? developer ? hr ? {desc: \"doing nothing\" }\n \nworkDescription = employee?.desc ? employee?.description ? employee?.workDescription ? \"no description found\"\nconsole.log(workDescription)\n```\n\nFirst we define some variables to play with; developer, cto, hr and setting them all to null then we create a object for the developer variable. Next we want to set the employee variable to the one of the variables that are not null, if all of them are null we create a new object: {desc: “doing nothing” }. Next we set workDescription to either desc, description, or workDescription if the employee variable is not null. If employee  is null or no property is set we set workDescription to “no description found”.\n\n## Tools and installation\nMore on installation can be found at [http://coffeescript.org/#installation](http://coffeescript.org/#installation)\nThe Chrome extension, [CoffeeConsole](http://snook.ca/archives/browsers/coffeeconsole) is nice for trying CoffeeScript, it compiles CoffeeScript to JavaScript as you type.\n\nFor all visual studio users I can recommend [Mindscape Web Workbench](https://visualstudiogallery.msdn.microsoft.com/2b96d16a-c986-4501-8f97-8008f9db141a).\n\n[Sublime text](http://www.sublimetext.com/) can be extended with a open source plugin to support syntax highlighting and compiling.\n\nIf you are interested in compiling CoffeeScript in a ASP.NET web project you can read more about that in my previous [post](/2012/07/26/using-servicestack-bundler/).\n\n## Books\nIf you still reading you probably think that CoffeeScript is worth learning and trying out. I can recommend the books, [CoffeeScript: Accelerated JavaScript Development](https://pragprog.com/book/tbcoffee/coffeescript) and [Testing with CoffeeScript](https://efendibooks.com/minibooks/testing-with-coffeescript)\n\n![](/content/images/2016/04/24.jpg) \n\n![](/content/images/2016/04/25.png)\n\nThanks for reading!","html":"<p>More and more I find myself talking about CoffeeScript and all the things I like about the language so I thought it would be a great idea to summarize all my talking in a post. Before I begin I must say that one of the best things about CoffeeScript is that it has given me a greater understanding for JavaScript.</p>\n\n<h2 id=\"meandjavascript\">Me and JavaScript</h2>\n\n<p>I have always had a love/hate relationship with JavaScript. Actually, JavaScript were one of the first languages I wrote code in and JavaScript gave me the inspiration to keep coding and learning new languages. Since then the JavaScript community has really exploded with all kinds of cool libraries and frameworks and I think we will see more and more around Node.js like <a href=\"https://www.meteor.com/\">Meteor</a>… but JavaScript is not a smooth language to work with in my opinion. It could be quite nice after adding some libraries to your codebase but I find native JavaScript quite poor and missing a lot of language features. Don’t get me wrong JavaScript has a nice object model and some good parts but I don’t think JavaScript is exposing them as well as CoffeeScript does.</p>\n\n<h2 id=\"whyilikecoffescript\">Why I like CoffeScript</h2>\n\n<p>CoffeeScript is a language that compiles into JavaScript. Everything that is possible in CoffeeScript is possible to achieve with native JavaScript but I promise you that it’s a lot of extra effort. Relax, I will not write about every language feature, you can find that type of reference documentation <a href=\"http://coffeescript.org/\">here</a>. I will show some cool things that I hope will convert you to a CoffeeScript lover!</p>\n\n<h3 id=\"prototypesclasses\">Prototypes… classes?</h3>\n\n<p>I have seen a lot of <a href=\"http://www.phpied.com/3-ways-to-define-a-javascript-class/\">examples</a> of prototyping and <a href=\"http://classy.pocoo.org/\">libraries for simulating classes in JavaScript</a>, some of them are really nice but which one to choose and should you really need to add a library to get support for classes? Some of you might say “Yeah, yeah… JavaScript is very object oriented without classes and we don’t need them”, that is maybe true… but why do we have all of those libraries that simulates classes?… it seems that at least a part of the JavaScript community wants classes.</p>\n\n<p>Okay, <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Function\">so a JavaScript function is a object</a>, functions can then be used to simulate classes and we can use prototypes to extend that function-object. Prototyping in JavaScript is quite hard to get right and you need to decide which approach to use. CoffeeScript makes that decision for you and implements it quite well through the keyword class(!)</p>\n\n<p>CoffeScript to the left, JavaScript to the right.</p>\n\n<p><img src=\"/content/images/2016/04/20.png\" alt=\"\" /></p>\n\n<p>Prototyping is even harder if you want one JavaScript object to inherit from another. CoffeeScript does that through the keyword \"extends\".</p>\n\n<pre><code class=\"language-coffeescript\">class Person  \n    constructor: (@firstName, @lastName) -&gt;\n\n    getName: -&gt;\n        @firstName + \" \" + @lastName\n\nclass Developer extends Person  \n    constructor: (first, last) -&gt;\n        super first, last\n\nanton = new Developer \"Anton\", \"Kallenberg\"  \nconsole.log(do anton.getName)  \n</code></pre>\n\n<p>CoffeeScript also has a shortcut for extending the prototype of an existing object, using the \"::\"-operator</p>\n\n<p><img src=\"/content/images/2016/04/21.png\" alt=\"\" /></p>\n\n<p>Note that I compare with != in CoffeScript but it translates to !== in JavaScript. CoffeeScript does not have “loose type comparison” to avoid unexpected results of <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/A_re-introduction_to_JavaScript?redirectlocale=en-US&amp;redirectslug=A_re-introduction_to_JavaScript#Operators\">type coercion</a>.</p>\n\n<h2 id=\"scoping\">Scoping</h2>\n\n<p>You may noticed that I prefixed my argument variables with @ in the previous example of the Person class. @ is a shorthand for “this”. Also by adding the @ to the method arguments I tell the CoffeeScript compiler to create backing fields for the arguments and to make them private.</p>\n\n<p>The <a href=\"http://blog.paracode.com/2011/08/28/coffeescript-fat-arrows/\">fat arrow</a> (=>) is another way that CoffeeScript handles scoping. The fat arrow binds “this” to the current instance of the class for the function defined.</p>\n\n<p><img src=\"/content/images/2016/04/22.png\" alt=\"\" /></p>\n\n<p>The fat arrow can for example be useful when calling a callback in a different scope of \"this\" and want \"this\" to be the class instance, for example when using jQuerys click event</p>\n\n<p><img src=\"/content/images/2016/04/23.png\" alt=\"\" /></p>\n\n<h2 id=\"theexistentialoperator\">The existential operator</h2>\n\n<p>If you are not convinced yet that CoffeeScript rocks the existential operator maybe makes it more interesting. Lets say you want to check the property containing work description of  some employees objects but you are not sure if the employee object is null or the name of the  property containing the  work description. In JavaScript this would look something like this:</p>\n\n<pre><code class=\"language-javascript\">// ...Don't even bother...\n</code></pre>\n\n<p>Well, it would be quite a lot of code, in CoffeeScript that could be written with the existential operator (\"?\").</p>\n\n<pre><code class=\"language-coffeescript\">developer = cto = hr = null  \ndeveloper = {desc : \"coding 4 food!\"}\n\nemployee = projectManager ? cto ? developer ? hr ? {desc: \"doing nothing\" }\n\nworkDescription = employee?.desc ? employee?.description ? employee?.workDescription ? \"no description found\"  \nconsole.log(workDescription)  \n</code></pre>\n\n<p>First we define some variables to play with; developer, cto, hr and setting them all to null then we create a object for the developer variable. Next we want to set the employee variable to the one of the variables that are not null, if all of them are null we create a new object: {desc: “doing nothing” }. Next we set workDescription to either desc, description, or workDescription if the employee variable is not null. If employee  is null or no property is set we set workDescription to “no description found”.</p>\n\n<h2 id=\"toolsandinstallation\">Tools and installation</h2>\n\n<p>More on installation can be found at <a href=\"http://coffeescript.org/#installation\">http://coffeescript.org/#installation</a> <br />\nThe Chrome extension, <a href=\"http://snook.ca/archives/browsers/coffeeconsole\">CoffeeConsole</a> is nice for trying CoffeeScript, it compiles CoffeeScript to JavaScript as you type.</p>\n\n<p>For all visual studio users I can recommend <a href=\"https://visualstudiogallery.msdn.microsoft.com/2b96d16a-c986-4501-8f97-8008f9db141a\">Mindscape Web Workbench</a>.</p>\n\n<p><a href=\"http://www.sublimetext.com/\">Sublime text</a> can be extended with a open source plugin to support syntax highlighting and compiling.</p>\n\n<p>If you are interested in compiling CoffeeScript in a ASP.NET web project you can read more about that in my previous <a href=\"/2012/07/26/using-servicestack-bundler/\">post</a>.</p>\n\n<h2 id=\"books\">Books</h2>\n\n<p>If you still reading you probably think that CoffeeScript is worth learning and trying out. I can recommend the books, <a href=\"https://pragprog.com/book/tbcoffee/coffeescript\">CoffeeScript: Accelerated JavaScript Development</a> and <a href=\"https://efendibooks.com/minibooks/testing-with-coffeescript\">Testing with CoffeeScript</a></p>\n\n<p><img src=\"/content/images/2016/04/24.jpg\" alt=\"\" title=\"\" /> </p>\n\n<p><img src=\"/content/images/2016/04/25.png\" alt=\"\" /></p>\n\n<p>Thanks for reading!</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":1461146195187,"created_by":1,"updated_at":1461146996417,"updated_by":1,"published_at":1344253020000,"published_by":1},{"id":13,"uuid":"dd79fe80-24bb-4207-898e-6703cbf220e9","title":"CONTINUOUS INTEGRATION WITH JENKINS AND PSAKE","slug":"continuous-integration-with-jenkins-and-psake","markdown":"Every now and then I get questions about build/deploy/test automation. Often the questions are asked by project lead or the customer and containing something like how much money will it cost? or how much time will it consume? My answer is usually something like…\n\n> It will depend on the complexity of the project… what kind of access does the build server have to the environments? What kind of test automation should we implement, unit test, integration test? How complex is the project to build, do we need a lot of configuration transformation and stuff like that?\n\n> Yes, it will be an initial cost but we will get the money back, it will make our life easier, minimize the risk of faulty deploys to production, making the release process person independent and making the project easier to maintain!\n\nThe last line about a easier life and easier to maintain usually sells the concept.  In this post I will show how we can implement deploy automation by using products that are open source and easy to configure to fit most projects. The post will mainly focus on how to implement it so you have a good example to base your arguments on when talking to project lead or your customer.\n\n## How?\nIf you have read my previous post about continuous integration you know that I have been writing about some specific steps in a deploy process like [compiling](/2012/07/26/using-servicestack-bundler/) and moving files using the [FTP protocol](/powershell-ftp-module-in-a-psake-build-process/). This post will try to put it all together to a complete example. This example requires that the build server has PowerShell, Robocopy (included in Win server 2008 R2), EPiServer 7 installed and FTP access to the target environment.\n\n### Installing the build server\nWe need a build server which can:\n\n* Execute our build automatically or manually whenever we want.\n* Create reports about the builds.\n* Notify the project team about faulty builds.\n\nFor that we will use [Jenkins](https://jenkins.io). Jenkins is open source and installs on most platforms, we will install it on Windows because we will be deploying an EPiServer 7 application. Jenkins is built upon Java so your build server needs to have Java installed. If it doesn’t already, go to [java.com](http://java.com) download and install it. Make sure the path to the Java executable is added to the [system environments variables](http://www.java.com/en/download/help/path.xml). Verify that everything went fine by typing:\n\n```powershell\nPS C:\\> java -version\njava version \"1.7.0_05\"\nJava(TM) SE Runtime Environment (build 1.7.0_05-b06)\nJava HotSpot(TM) Client VM (build 23.1-b03, mixed mode, sharing)\n```\n\nAfter you finished the installation of Java, download the Jenkins installation zip file from [jenkins.io](https://jenkins.io/) Extract the files and go to the extracted folder and execute setup.exe.\n\n```powershell\nPS C:\\jenkins-1.476> .\\setup.exe\n```\n\nAfter the installation wizard finish visit http://localhost:8080/ and you will see something like this:\n\n![](/content/images/2016/04/30.png)\n\nJenkins is now installed as a windows service and running at your server. This is a new even simpler installation process than before. If you have installed previous versions of Jenkins you know that you first had to install it from a war file and after that install it as a windows service through the Jenkins UI.  Although, that seems to be included in the installation program nowadays!\n\n### Getting the source code\nJenkins has a lot of plugins and some of them are for getting source code from different source control system like SVN, CVS, TFS and of course GIT. The EPiServer project for this post is hosted at GitHub and fortunately Jenkins has a [GitHub plugin](https://wiki.jenkins-ci.org/display/JENKINS/Github+Plugin)! Install it through the plugin manager, search for “github plugin”, check the checkbox and hit “install without restart” and then check “Restart Jenkins when installation is complete and no jobs are running”\n\n![](/content/images/2016/04/31.png)\n\n### Configure the GitHub plug-in\nGo to “Manage Jenkins” and add the path to git.cmd.\n\n![](/content/images/2016/04/32.png)\n\nThis project is hosted in a public repository at GitHub so we do not need to setup [deploy keys](https://developer.github.com/guides/managing-deploy-keys/). Which can be kind of painful but [possible](http://fourkitchens.com/blog/article/trigger-jenkins-builds-pushing-github), this post doesn’t cover that topic but there are a lot of great articles about that out on the internet. Click on “new job”, set the job name and choose “Build a free-style software project”.\n\nThe repository settings for my project looks like this:\n![](/content/images/2016/04/33.png)\n\nTry your settings by going back to the start page and click “schedule a build” (the clock icon to the right). If everything is set up correctly you should see something like this (with a nice sunny sun :)) after running the build:\n\n![](/content/images/2016/04/34.png)\n\n### The build system\nNow we have a build server that gets the source code whenever we want. In most cases we probably want to do something with the fetched source code.  There are a lot of solutions that can customize your build process like Phantom, Psake or NAnt. I can’t tell you which one is the best but I like Psake, maybe because I’m a PowerShell fan and doesn’t like to mess around with xml configuration files. For this application we will use Psake to do the following steps\n\n1. Some initial setup, copy and creating needed stuff\n2. Compile C# and client script + minifying it.\n3. Run unit tests and break the build if any test fails\n4. Copy needed files for running the application\n5. Transform configuration files for a production environment\n6. Backup the current application at the production environment (ftp)\n7. Redeploy the application (ftp)\n\nIn the root of the project I have created a powershell script called Build.ps1. Jenkins will run this script after he has pulled the source from Github.\n\n```powershell\nparam(\n    [alias(\"env\")]\n    $Environment = 'debug',\n    [alias(\"ftp\")]\n    $DeployToFtp = $true\n)\n \nfunction Build() {\n    Try {\n        if($Environment -ieq 'debug') {\n            .\\Web\\EPiBooks\\Tools\\psake.ps1 \".\\Web\\EPiBooks\\BuildScripts\\Deploy.ps1\" -properties @{ config='debug'; environment=\"$Environment\" }\n        }\n        if($Environment -ieq 'production') {\n            .\\Web\\EPiBooks\\Tools\\psake.ps1 \".\\Web\\EPiBooks\\BuildScripts\\Deploy.ps1\" -properties @{ config='release'; environment=\"$Environment\"; deployToFtp = $DeployToFtp } \"production\"\n        }\n        Write-Host \"$Environment build done!\"\n    }\n    Catch {\n        throw \"build failed\"\n        exit 1\n    }\n    Finally {\n        if ($psake.build_success -eq $false) {\n            exit 1\n        } else {\n            exit 0\n        }\n    }\n}\n \nBuild\n```\n\nThe script is a wrapper around executing Psake and supplying Psake with our build script. The script takes two parameters, Environment and DeployToFtp. These parameters are set by Jenkins to let Psake and our build script know how to build and deploy the application. As you can see the execution is wrapped in a try-catch-finally statement, this is just to have better control on which exit codes to return to Jenkins to break or succeed the deploy.\n\n### Deploy.ps1\nThe Deploy.ps1 is our build/deploy script that does the actual building/deploying. I have tried to put comments into the script to make it more understandable.\n\n```powershell\n# properties that is used by the script\nproperties {\n    $dateLabel = ([DateTime]::Now.ToString(\"yyyy-MM-dd_HH-mm-ss\"))\n    $baseDir = resolve-path .\\..\\..\\..\\\n    $sourceDir = \"$baseDir\\Web\\\"\n    $toolsDir = \"$sourceDir\\EPiBooks\\Tools\\\"\n    $deployBaseDir = \"$baseDir\\Deploy\\\"\n    $deployPkgDir = \"$deployBaseDir\\Package\\\"\n    $backupDir = \"$deployBaseDir\\Backup\\\"\n    $testBaseDir = \"$baseDir\\EPiBooks.Tests\\\"\n    $config = 'debug'\n    $environment = 'debug'\n    $ftpProductionHost = 'ftp://127.0.0.1:21/'\n    $ftpProductionUsername = 'anton'\n    $ftpProductionPassword = 'anton'\n    $ftpProductionWebRootFolder = \"www\"\n    $ftpProductionBackupFolder = \"backup\"\n    $deployToFtp = $true\n}\n \n# the default task that is executed if no task is defined when calling this script\ntask default -depends local\n# task that is used when building the project at a local development environment, depending on the mergeConfig task\ntask local -depends mergeConfig\n# task that is used when building for production, depending on the deploy task\ntask production -depends deploy\n \n# task that is setting up needed stuff for the build process\ntask setup {\n    # remove the ftp module if it's imported\n    remove-module [f]tp\n    # importing the ftp module from the tools dir\n    import-module \"$toolsDir\\ftp.psm1\"\n \n    # removing and creating folders needed for the build, deploy package dir and a backup dir with a date\n    Remove-ThenAddFolder $deployPkgDir\n    Remove-ThenAddFolder $backupDir\n    Remove-ThenAddFolder \"$backupDir\\$dateLabel\"\n \n    <#\n        checking if any episerver dlls is existing in the Libraries folder. This requires that the build server has episerver 7 installed\n        for this application the episerver dlls is not pushed to the source control if we had done that this would not be necessary\n    #>\n    $a = Get-ChildItem \"$sourceDir\\Libraries\\EPiServer.*\"\n    if (-not $a.Count) {\n \n        # if no episerver dlls are found, copy the episerver cms dlls with robocopy from the episerver installation dir\n        robocopy \"C:\\Program Files (x86)\\EPiServer\\CMS\\7.0.449.1\\bin\" \"$sourceDir\\Libraries\" EPiServer.*\n \n        <#\n            checking the last exit code. robocopy is returning a number greater\n            than 1 if something went wrong. For more info check out => http://ss64.com/nt/robocopy-exit.html\n        #>\n        if($LASTEXITCODE -gt 1) {\n            throw \"robocopy command failed\"\n            exit 1\n        }\n \n        # also we need to copy the episerver framework dlls\n        robocopy \"C:\\Program Files (x86)\\EPiServer\\Framework\\7.0.722.1\\bin\" \"$sourceDir\\Libraries\" EPiServer.*\n        if($LASTEXITCODE -gt 1) {\n            throw \"robocopy command failed\"\n            exit 1\n        }\n    }\n}\n \n# compiling csharp and client script with bundler\ntask compile -depends setup {\n    # executing msbuild for compiling the project\n    exec { msbuild  $sourceDir\\EPiBooks.sln /t:Clean /t:Build /p:Configuration=$config /v:q /nologo }\n \n    <#\n        executing Bundle.ps1, Bundle.ps1 is a wrapper around bundler that is compiling client script\n        the wrapper also is executed as post-build script when compiling in debug mode. For more info check out => https://antonkallenberg.com/2012/07/26/using-servicestack-bundler/\n    #>\n    .\\Bundle.ps1\n    # checking so that last exit code is ok else break the build\n    if($LASTEXITCODE -ne 0) {\n        throw \"Failed to bundle client scripts\"\n        exit 1\n    }\n}\n \n# running unit tests\ntask test -depends compile {\n    # executing mspec and suppling the test assembly\n    &\"$sourceDir\\packages\\Machine.Specifications.0.5.7\\tools\\mspec-clr4.exe\" \"$testBaseDir\\bin\\$config\\EPiBooks.Tests.dll\"\n    # checking so that last exit code is ok else break the build\n    if($LASTEXITCODE -ne 0) {\n        throw \"Failed to run unit tests\"\n        exit 1\n    }\n}\n \n# copying the deployment package\ntask copyPkg -depends test {\n    # robocopy has some issue with a trailing slash in the path (or it's by design, don't know), lets remove that slash\n    $deployPath = Remove-LastChar \"$deployPkgDir\"\n    # copying the required files for the deloy package to the deploy folder created at setup\n    robocopy \"$sourceDir\\EPiBooks\" \"$deployPath\" /MIR /XD obj bundler Configurations Properties /XF *.bundle *.coffee *.less *.pdb *.cs *.csproj *.csproj.user *.sln .gitignore README.txt packages.config\n    # checking so that last exit code is ok else break the build (robocopy returning greater that 1 if fail)\n    if($LASTEXITCODE -gt 1) {\n        throw \"robocopy commande failed\"\n        exit 1\n    }\n}\n \n# merging and doing config transformations\ntask mergeConfig -depends copyPkg {\n    # only for production\n    if($environment -ieq \"production\") {\n        # first lets remove the files that will be transformed\n        Remove-IfExists \"$deployPkgDir\\Web.config\"\n        Remove-IfExists \"$deployPkgDir\\episerver.config\"\n \n        <#\n            doing the transformation for Web.config using Config Transformation Tool\n            check out http://ctt.codeplex.com/ for more info\n        #>\n        &\"$toolsDir\\Config.Transformation.Tool.v1.2\\ctt.exe\" \"s:$sourceDir\\EPiBooks\\Web.config\" \"t:$sourceDir\\EPiBooks\\ConfigTransformations\\Production\\Web.Transform.Config\" \"d:$deployPkgDir\\Web.config\"\n        # checking so that last exit code is ok else break the build\n        if($LASTEXITCODE -ne 0) {\n            throw \"Config transformation commande failed\"\n            exit 1\n        }\n \n        # doing the transformation for episerver.config\n        &\"$toolsDir\\Config.Transformation.Tool.v1.2\\ctt.exe\" \"s:$sourceDir\\EPiBooks\\episerver.config\" \"t:$sourceDir\\EPiBooks\\ConfigTransformations\\Production\\episerver.Transform.Config\" \"d:$deployPkgDir\\episerver.config\"\n        # checking so that last exit code is ok else break the build\n        if($LASTEXITCODE -ne 0) {\n            throw \"Config transformation commande failed\"\n            exit 1\n        }\n    }\n}\n \n# deploying the package\ntask deploy -depends mergeConfig {\n    # only if production and deployToFtp property is set to true\n    if($environment -ieq \"production\" -and $deployToFtp -eq $true) {\n        # Setting the connection to the production ftp\n        Set-FtpConnection $ftpProductionHost $ftpProductionUsername $ftpProductionPassword\n \n        # backing up before deploy => by downloading and uploading the current webapplication at production enviorment\n        $localBackupDir = Remove-LastChar \"$backupDir\"\n        Get-FromFtp \"$backupDir\\$dateLabel\" \"$ftpProductionWebRootFolder\"\n        Send-ToFtp \"$localBackupDir\" \"$ftpProductionBackupFolder\"\n \n        # redeploying the application => by removing the existing application and upload the new one\n        Remove-FromFtp \"$ftpProductionWebRootFolder\"\n        $localDeployPkgDir = Remove-LastChar \"$deployPkgDir\"\n        Send-ToFtp \"$localDeployPkgDir\" \"$ftpProductionWebRootFolder\"\n    }\n}\n \n#helper methods\nfunction Remove-IfExists([string]$name) {\n    if ((Test-Path -path $name)) {\n        dir $name -recurse | where {!@(dir -force $_.fullname)} | rm\n        Remove-Item $name -Recurse\n    }\n}\n \nfunction Remove-ThenAddFolder([string]$name) {\n    Remove-IfExists $name\n    New-Item -Path $name -ItemType \"directory\"\n}\n \nfunction Remove-LastChar([string]$str) {\n    $str.Remove(($str.Length-1),1)\n}\n```\n\nNow we have the complete build/deploy script in place and we need Jenkins to execute it after fetching the source. Of course we have some issues with permissions and execution policies, because of that we need to execute a PowerShell process that allow execution of Psake and our build/deploy script. Go to the configuration page and “add a build step”, choose “Execute Windows batch command” and enter:\n\n```text\nPowershell.exe -noprofile -executionpolicy Bypass -file .\\Build.ps1 -env \"production\"\n```\n\nLike this:\n![](/content/images/2016/04/35.png)\n\n\nNow the configuration is done, go back to the start page and hit “schedule a build” and you should see that sunny sun again!\n\n### Scheduling\nJenkins and the Github plugin has some nice scheduling features. We can for example build to our (imaginary) test environment every time something is pushed to the repository or we can just set up a time scheduled when to build. The syntax is [cron](https://en.wikipedia.org/wiki/Cron)-like, this means every hour.\n\n![](/content/images/2016/04/36.png)\n\nMore information on the scheduling interval syntax can be found by clicking the “?” to the right of the “Schedule”-field.\n\n## Conclusion\nJenkins, Github and Psake plays pretty well together. I had some issues with permissions and execution policies but it wasn’t too hard to figure out. Hopefully it will not be too hard to configure deployments keys with private Github repositories.\n\nI know the example has room for some improvement like creating and emailing build reports (which can be done with the [Email-ext](https://wiki.jenkins-ci.org/display/JENKINS/Email-ext+plugin) plug in). The ftp deployment task is not optimal but it works. To get a more reliable solution we could deploy using [web deploy](http://trycatchfail.com/blog/post/Continuous-Deployment-Using-TeamCity-6-MSDeploy-and-psake). The problem with our ftp deployment is that it can be interrupted in the middle of a deploy leaving the application broken (if the network or ftp server goes down).When using web deploy the target machine has a local service that does the deploy which minimizes the risks of a deploy being interrupted in the middle of the installation.\n\nI think we should consider building and doing deploys using Jenkins and Psake. I like that we have almost full control over the process and that it’s almost xml free!\n\nIf you are still reading I think you found this topic interesting, I know I do. Please let me know if you have any thoughts. Thanks for reading!\n\nFull source can be found [here](https://github.com/antonkallenberg/EPiBooks/tree/PsakeJenkinsV1.0).","html":"<p>Every now and then I get questions about build/deploy/test automation. Often the questions are asked by project lead or the customer and containing something like how much money will it cost? or how much time will it consume? My answer is usually something like…</p>\n\n<blockquote>\n  <p>It will depend on the complexity of the project… what kind of access does the build server have to the environments? What kind of test automation should we implement, unit test, integration test? How complex is the project to build, do we need a lot of configuration transformation and stuff like that?</p>\n  \n  <p>Yes, it will be an initial cost but we will get the money back, it will make our life easier, minimize the risk of faulty deploys to production, making the release process person independent and making the project easier to maintain!</p>\n</blockquote>\n\n<p>The last line about a easier life and easier to maintain usually sells the concept.  In this post I will show how we can implement deploy automation by using products that are open source and easy to configure to fit most projects. The post will mainly focus on how to implement it so you have a good example to base your arguments on when talking to project lead or your customer.</p>\n\n<h2 id=\"how\">How?</h2>\n\n<p>If you have read my previous post about continuous integration you know that I have been writing about some specific steps in a deploy process like <a href=\"/2012/07/26/using-servicestack-bundler/\">compiling</a> and moving files using the <a href=\"/powershell-ftp-module-in-a-psake-build-process/\">FTP protocol</a>. This post will try to put it all together to a complete example. This example requires that the build server has PowerShell, Robocopy (included in Win server 2008 R2), EPiServer 7 installed and FTP access to the target environment.</p>\n\n<h3 id=\"installingthebuildserver\">Installing the build server</h3>\n\n<p>We need a build server which can:</p>\n\n<ul>\n<li>Execute our build automatically or manually whenever we want.</li>\n<li>Create reports about the builds.</li>\n<li>Notify the project team about faulty builds.</li>\n</ul>\n\n<p>For that we will use <a href=\"https://jenkins.io\">Jenkins</a>. Jenkins is open source and installs on most platforms, we will install it on Windows because we will be deploying an EPiServer 7 application. Jenkins is built upon Java so your build server needs to have Java installed. If it doesn’t already, go to <a href=\"http://java.com\">java.com</a> download and install it. Make sure the path to the Java executable is added to the <a href=\"http://www.java.com/en/download/help/path.xml\">system environments variables</a>. Verify that everything went fine by typing:</p>\n\n<pre><code class=\"language-powershell\">PS C:\\&gt; java -version  \njava version \"1.7.0_05\"  \nJava(TM) SE Runtime Environment (build 1.7.0_05-b06)  \nJava HotSpot(TM) Client VM (build 23.1-b03, mixed mode, sharing)  \n</code></pre>\n\n<p>After you finished the installation of Java, download the Jenkins installation zip file from <a href=\"https://jenkins.io/\">jenkins.io</a> Extract the files and go to the extracted folder and execute setup.exe.</p>\n\n<pre><code class=\"language-powershell\">PS C:\\jenkins-1.476&gt; .\\setup.exe  \n</code></pre>\n\n<p>After the installation wizard finish visit <a href=\"http://localhost:8080/\">http://localhost:8080/</a> and you will see something like this:</p>\n\n<p><img src=\"/content/images/2016/04/30.png\" alt=\"\" /></p>\n\n<p>Jenkins is now installed as a windows service and running at your server. This is a new even simpler installation process than before. If you have installed previous versions of Jenkins you know that you first had to install it from a war file and after that install it as a windows service through the Jenkins UI.  Although, that seems to be included in the installation program nowadays!</p>\n\n<h3 id=\"gettingthesourcecode\">Getting the source code</h3>\n\n<p>Jenkins has a lot of plugins and some of them are for getting source code from different source control system like SVN, CVS, TFS and of course GIT. The EPiServer project for this post is hosted at GitHub and fortunately Jenkins has a <a href=\"https://wiki.jenkins-ci.org/display/JENKINS/Github+Plugin\">GitHub plugin</a>! Install it through the plugin manager, search for “github plugin”, check the checkbox and hit “install without restart” and then check “Restart Jenkins when installation is complete and no jobs are running”</p>\n\n<p><img src=\"/content/images/2016/04/31.png\" alt=\"\" /></p>\n\n<h3 id=\"configurethegithubplugin\">Configure the GitHub plug-in</h3>\n\n<p>Go to “Manage Jenkins” and add the path to git.cmd.</p>\n\n<p><img src=\"/content/images/2016/04/32.png\" alt=\"\" /></p>\n\n<p>This project is hosted in a public repository at GitHub so we do not need to setup <a href=\"https://developer.github.com/guides/managing-deploy-keys/\">deploy keys</a>. Which can be kind of painful but <a href=\"http://fourkitchens.com/blog/article/trigger-jenkins-builds-pushing-github\">possible</a>, this post doesn’t cover that topic but there are a lot of great articles about that out on the internet. Click on “new job”, set the job name and choose “Build a free-style software project”.</p>\n\n<p>The repository settings for my project looks like this: <br />\n<img src=\"/content/images/2016/04/33.png\" alt=\"\" /></p>\n\n<p>Try your settings by going back to the start page and click “schedule a build” (the clock icon to the right). If everything is set up correctly you should see something like this (with a nice sunny sun :)) after running the build:</p>\n\n<p><img src=\"/content/images/2016/04/34.png\" alt=\"\" /></p>\n\n<h3 id=\"thebuildsystem\">The build system</h3>\n\n<p>Now we have a build server that gets the source code whenever we want. In most cases we probably want to do something with the fetched source code.  There are a lot of solutions that can customize your build process like Phantom, Psake or NAnt. I can’t tell you which one is the best but I like Psake, maybe because I’m a PowerShell fan and doesn’t like to mess around with xml configuration files. For this application we will use Psake to do the following steps</p>\n\n<ol>\n<li>Some initial setup, copy and creating needed stuff  </li>\n<li>Compile C# and client script + minifying it.  </li>\n<li>Run unit tests and break the build if any test fails  </li>\n<li>Copy needed files for running the application  </li>\n<li>Transform configuration files for a production environment  </li>\n<li>Backup the current application at the production environment (ftp)  </li>\n<li>Redeploy the application (ftp)</li>\n</ol>\n\n<p>In the root of the project I have created a powershell script called Build.ps1. Jenkins will run this script after he has pulled the source from Github.</p>\n\n<pre><code class=\"language-powershell\">param(  \n    [alias(\"env\")]\n    $Environment = 'debug',\n    [alias(\"ftp\")]\n    $DeployToFtp = $true\n)\n\nfunction Build() {  \n    Try {\n        if($Environment -ieq 'debug') {\n            .\\Web\\EPiBooks\\Tools\\psake.ps1 \".\\Web\\EPiBooks\\BuildScripts\\Deploy.ps1\" -properties @{ config='debug'; environment=\"$Environment\" }\n        }\n        if($Environment -ieq 'production') {\n            .\\Web\\EPiBooks\\Tools\\psake.ps1 \".\\Web\\EPiBooks\\BuildScripts\\Deploy.ps1\" -properties @{ config='release'; environment=\"$Environment\"; deployToFtp = $DeployToFtp } \"production\"\n        }\n        Write-Host \"$Environment build done!\"\n    }\n    Catch {\n        throw \"build failed\"\n        exit 1\n    }\n    Finally {\n        if ($psake.build_success -eq $false) {\n            exit 1\n        } else {\n            exit 0\n        }\n    }\n}\n\nBuild  \n</code></pre>\n\n<p>The script is a wrapper around executing Psake and supplying Psake with our build script. The script takes two parameters, Environment and DeployToFtp. These parameters are set by Jenkins to let Psake and our build script know how to build and deploy the application. As you can see the execution is wrapped in a try-catch-finally statement, this is just to have better control on which exit codes to return to Jenkins to break or succeed the deploy.</p>\n\n<h3 id=\"deployps1\">Deploy.ps1</h3>\n\n<p>The Deploy.ps1 is our build/deploy script that does the actual building/deploying. I have tried to put comments into the script to make it more understandable.</p>\n\n<pre><code class=\"language-powershell\"># properties that is used by the script\nproperties {  \n    $dateLabel = ([DateTime]::Now.ToString(\"yyyy-MM-dd_HH-mm-ss\"))\n    $baseDir = resolve-path .\\..\\..\\..\\\n    $sourceDir = \"$baseDir\\Web\\\"\n    $toolsDir = \"$sourceDir\\EPiBooks\\Tools\\\"\n    $deployBaseDir = \"$baseDir\\Deploy\\\"\n    $deployPkgDir = \"$deployBaseDir\\Package\\\"\n    $backupDir = \"$deployBaseDir\\Backup\\\"\n    $testBaseDir = \"$baseDir\\EPiBooks.Tests\\\"\n    $config = 'debug'\n    $environment = 'debug'\n    $ftpProductionHost = 'ftp://127.0.0.1:21/'\n    $ftpProductionUsername = 'anton'\n    $ftpProductionPassword = 'anton'\n    $ftpProductionWebRootFolder = \"www\"\n    $ftpProductionBackupFolder = \"backup\"\n    $deployToFtp = $true\n}\n\n# the default task that is executed if no task is defined when calling this script\ntask default -depends local  \n# task that is used when building the project at a local development environment, depending on the mergeConfig task\ntask local -depends mergeConfig  \n# task that is used when building for production, depending on the deploy task\ntask production -depends deploy\n\n# task that is setting up needed stuff for the build process\ntask setup {  \n    # remove the ftp module if it's imported\n    remove-module [f]tp\n    # importing the ftp module from the tools dir\n    import-module \"$toolsDir\\ftp.psm1\"\n\n    # removing and creating folders needed for the build, deploy package dir and a backup dir with a date\n    Remove-ThenAddFolder $deployPkgDir\n    Remove-ThenAddFolder $backupDir\n    Remove-ThenAddFolder \"$backupDir\\$dateLabel\"\n\n    &lt;#\n        checking if any episerver dlls is existing in the Libraries folder. This requires that the build server has episerver 7 installed\n        for this application the episerver dlls is not pushed to the source control if we had done that this would not be necessary\n    #&gt;\n    $a = Get-ChildItem \"$sourceDir\\Libraries\\EPiServer.*\"\n    if (-not $a.Count) {\n\n        # if no episerver dlls are found, copy the episerver cms dlls with robocopy from the episerver installation dir\n        robocopy \"C:\\Program Files (x86)\\EPiServer\\CMS\\7.0.449.1\\bin\" \"$sourceDir\\Libraries\" EPiServer.*\n\n        &lt;#\n            checking the last exit code. robocopy is returning a number greater\n            than 1 if something went wrong. For more info check out =&gt; http://ss64.com/nt/robocopy-exit.html\n        #&gt;\n        if($LASTEXITCODE -gt 1) {\n            throw \"robocopy command failed\"\n            exit 1\n        }\n\n        # also we need to copy the episerver framework dlls\n        robocopy \"C:\\Program Files (x86)\\EPiServer\\Framework\\7.0.722.1\\bin\" \"$sourceDir\\Libraries\" EPiServer.*\n        if($LASTEXITCODE -gt 1) {\n            throw \"robocopy command failed\"\n            exit 1\n        }\n    }\n}\n\n# compiling csharp and client script with bundler\ntask compile -depends setup {  \n    # executing msbuild for compiling the project\n    exec { msbuild  $sourceDir\\EPiBooks.sln /t:Clean /t:Build /p:Configuration=$config /v:q /nologo }\n\n    &lt;#\n        executing Bundle.ps1, Bundle.ps1 is a wrapper around bundler that is compiling client script\n        the wrapper also is executed as post-build script when compiling in debug mode. For more info check out =&gt; https://antonkallenberg.com/2012/07/26/using-servicestack-bundler/\n    #&gt;\n    .\\Bundle.ps1\n    # checking so that last exit code is ok else break the build\n    if($LASTEXITCODE -ne 0) {\n        throw \"Failed to bundle client scripts\"\n        exit 1\n    }\n}\n\n# running unit tests\ntask test -depends compile {  \n    # executing mspec and suppling the test assembly\n    &amp;\"$sourceDir\\packages\\Machine.Specifications.0.5.7\\tools\\mspec-clr4.exe\" \"$testBaseDir\\bin\\$config\\EPiBooks.Tests.dll\"\n    # checking so that last exit code is ok else break the build\n    if($LASTEXITCODE -ne 0) {\n        throw \"Failed to run unit tests\"\n        exit 1\n    }\n}\n\n# copying the deployment package\ntask copyPkg -depends test {  \n    # robocopy has some issue with a trailing slash in the path (or it's by design, don't know), lets remove that slash\n    $deployPath = Remove-LastChar \"$deployPkgDir\"\n    # copying the required files for the deloy package to the deploy folder created at setup\n    robocopy \"$sourceDir\\EPiBooks\" \"$deployPath\" /MIR /XD obj bundler Configurations Properties /XF *.bundle *.coffee *.less *.pdb *.cs *.csproj *.csproj.user *.sln .gitignore README.txt packages.config\n    # checking so that last exit code is ok else break the build (robocopy returning greater that 1 if fail)\n    if($LASTEXITCODE -gt 1) {\n        throw \"robocopy commande failed\"\n        exit 1\n    }\n}\n\n# merging and doing config transformations\ntask mergeConfig -depends copyPkg {  \n    # only for production\n    if($environment -ieq \"production\") {\n        # first lets remove the files that will be transformed\n        Remove-IfExists \"$deployPkgDir\\Web.config\"\n        Remove-IfExists \"$deployPkgDir\\episerver.config\"\n\n        &lt;#\n            doing the transformation for Web.config using Config Transformation Tool\n            check out http://ctt.codeplex.com/ for more info\n        #&gt;\n        &amp;\"$toolsDir\\Config.Transformation.Tool.v1.2\\ctt.exe\" \"s:$sourceDir\\EPiBooks\\Web.config\" \"t:$sourceDir\\EPiBooks\\ConfigTransformations\\Production\\Web.Transform.Config\" \"d:$deployPkgDir\\Web.config\"\n        # checking so that last exit code is ok else break the build\n        if($LASTEXITCODE -ne 0) {\n            throw \"Config transformation commande failed\"\n            exit 1\n        }\n\n        # doing the transformation for episerver.config\n        &amp;\"$toolsDir\\Config.Transformation.Tool.v1.2\\ctt.exe\" \"s:$sourceDir\\EPiBooks\\episerver.config\" \"t:$sourceDir\\EPiBooks\\ConfigTransformations\\Production\\episerver.Transform.Config\" \"d:$deployPkgDir\\episerver.config\"\n        # checking so that last exit code is ok else break the build\n        if($LASTEXITCODE -ne 0) {\n            throw \"Config transformation commande failed\"\n            exit 1\n        }\n    }\n}\n\n# deploying the package\ntask deploy -depends mergeConfig {  \n    # only if production and deployToFtp property is set to true\n    if($environment -ieq \"production\" -and $deployToFtp -eq $true) {\n        # Setting the connection to the production ftp\n        Set-FtpConnection $ftpProductionHost $ftpProductionUsername $ftpProductionPassword\n\n        # backing up before deploy =&gt; by downloading and uploading the current webapplication at production enviorment\n        $localBackupDir = Remove-LastChar \"$backupDir\"\n        Get-FromFtp \"$backupDir\\$dateLabel\" \"$ftpProductionWebRootFolder\"\n        Send-ToFtp \"$localBackupDir\" \"$ftpProductionBackupFolder\"\n\n        # redeploying the application =&gt; by removing the existing application and upload the new one\n        Remove-FromFtp \"$ftpProductionWebRootFolder\"\n        $localDeployPkgDir = Remove-LastChar \"$deployPkgDir\"\n        Send-ToFtp \"$localDeployPkgDir\" \"$ftpProductionWebRootFolder\"\n    }\n}\n\n#helper methods\nfunction Remove-IfExists([string]$name) {  \n    if ((Test-Path -path $name)) {\n        dir $name -recurse | where {!@(dir -force $_.fullname)} | rm\n        Remove-Item $name -Recurse\n    }\n}\n\nfunction Remove-ThenAddFolder([string]$name) {  \n    Remove-IfExists $name\n    New-Item -Path $name -ItemType \"directory\"\n}\n\nfunction Remove-LastChar([string]$str) {  \n    $str.Remove(($str.Length-1),1)\n}\n</code></pre>\n\n<p>Now we have the complete build/deploy script in place and we need Jenkins to execute it after fetching the source. Of course we have some issues with permissions and execution policies, because of that we need to execute a PowerShell process that allow execution of Psake and our build/deploy script. Go to the configuration page and “add a build step”, choose “Execute Windows batch command” and enter:</p>\n\n<pre><code class=\"language-text\">Powershell.exe -noprofile -executionpolicy Bypass -file .\\Build.ps1 -env \"production\"  \n</code></pre>\n\n<p>Like this: <br />\n<img src=\"/content/images/2016/04/35.png\" alt=\"\" /></p>\n\n<p>Now the configuration is done, go back to the start page and hit “schedule a build” and you should see that sunny sun again!</p>\n\n<h3 id=\"scheduling\">Scheduling</h3>\n\n<p>Jenkins and the Github plugin has some nice scheduling features. We can for example build to our (imaginary) test environment every time something is pushed to the repository or we can just set up a time scheduled when to build. The syntax is <a href=\"https://en.wikipedia.org/wiki/Cron\">cron</a>-like, this means every hour.</p>\n\n<p><img src=\"/content/images/2016/04/36.png\" alt=\"\" /></p>\n\n<p>More information on the scheduling interval syntax can be found by clicking the “?” to the right of the “Schedule”-field.</p>\n\n<h2 id=\"conclusion\">Conclusion</h2>\n\n<p>Jenkins, Github and Psake plays pretty well together. I had some issues with permissions and execution policies but it wasn’t too hard to figure out. Hopefully it will not be too hard to configure deployments keys with private Github repositories.</p>\n\n<p>I know the example has room for some improvement like creating and emailing build reports (which can be done with the <a href=\"https://wiki.jenkins-ci.org/display/JENKINS/Email-ext+plugin\">Email-ext</a> plug in). The ftp deployment task is not optimal but it works. To get a more reliable solution we could deploy using <a href=\"http://trycatchfail.com/blog/post/Continuous-Deployment-Using-TeamCity-6-MSDeploy-and-psake\">web deploy</a>. The problem with our ftp deployment is that it can be interrupted in the middle of a deploy leaving the application broken (if the network or ftp server goes down).When using web deploy the target machine has a local service that does the deploy which minimizes the risks of a deploy being interrupted in the middle of the installation.</p>\n\n<p>I think we should consider building and doing deploys using Jenkins and Psake. I like that we have almost full control over the process and that it’s almost xml free!</p>\n\n<p>If you are still reading I think you found this topic interesting, I know I do. Please let me know if you have any thoughts. Thanks for reading!</p>\n\n<p>Full source can be found <a href=\"https://github.com/antonkallenberg/EPiBooks/tree/PsakeJenkinsV1.0\">here</a>.</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":1461151079332,"created_by":1,"updated_at":1461151998279,"updated_by":1,"published_at":1345030620000,"published_by":1},{"id":14,"uuid":"633ce88e-faac-4557-ab49-8b91e499d31e","title":"THE “FOLDER WITHOUT A NAME” PROBLEM","slug":"the-folder-without-a-name-problem","markdown":"Some days ago I created a new Virtual OS at my laptop using [VirtualBox](https://www.virtualbox.org/) but VirtualBox freaked out and created the folder structure for the Virtual OS within a folder without a name:\n\n![](/content/images/2016/04/40.png)\n\nAfter the installation of the OS, VirtualBox started to complain about not finding settings or virtual hard drives. Surprisingly enough it turns out that Windows does not like a folder without a name. Windows does not let you do anything like changing the name, saving files to it or removing it through the Windows explorer UI. I don’t really understand how VirtualBox was able to create the folder and add 7 GB of data to it without getting errors from Windows?\n\n## Removing the folder!\n\nAs I said the folder contained a lot of data and the OS inside the folder was useless. My precisions SSD disk shouldn’t have a useless 7 GB folder, so I needed to remove it! I thought that my good old friend PowerShell would help me, so I told PS to recursive remove the parent folder:\n\n![](/content/images/2016/04/41.png)\n\nPS started to complain and told me that the path was a junction point and that I needed to force delete the folder, so I tried that:\n\n![](/content/images/2016/04/42.png)\n\nNothing happened it just stood there blinking and my laptop started to get slow. After some quick checks at the Task Manager I saw that PowerShell used 40 – 100% of my CPU and almost all of my 8 GB of memory. I killed the PowerShell process and got my resources back.\n\n![](/content/images/2016/04/43.png)\n\nNow I didn’t really know what to do and I stared to search the web. As it turns out Windows has a short alternative folder name so I needed to get that name. PowerShell made me disappointed the last time so I started CMD.exe and ran the dir command:\n\n![](/content/images/2016/04/44.png)\n\nOh nice, the folder seemed to have some kind of random short name so I used that and removed it the rmdir command:\n\n![](/content/images/2016/04/45.png)\n\nWhohoo! The freaky folder was gone and I got my 7 GB back :). Maybe this helps someone else with the same problem, otherwise I just shared the start of my Saturday morning. I’m a forgiving person and five minutes later I were back using VirtualBox and PowerShell again.  Now I can go back to coding in my new virtual OS!","html":"<p>Some days ago I created a new Virtual OS at my laptop using <a href=\"https://www.virtualbox.org/\">VirtualBox</a> but VirtualBox freaked out and created the folder structure for the Virtual OS within a folder without a name:</p>\n\n<p><img src=\"/content/images/2016/04/40.png\" alt=\"\" /></p>\n\n<p>After the installation of the OS, VirtualBox started to complain about not finding settings or virtual hard drives. Surprisingly enough it turns out that Windows does not like a folder without a name. Windows does not let you do anything like changing the name, saving files to it or removing it through the Windows explorer UI. I don’t really understand how VirtualBox was able to create the folder and add 7 GB of data to it without getting errors from Windows?</p>\n\n<h2 id=\"removingthefolder\">Removing the folder!</h2>\n\n<p>As I said the folder contained a lot of data and the OS inside the folder was useless. My precisions SSD disk shouldn’t have a useless 7 GB folder, so I needed to remove it! I thought that my good old friend PowerShell would help me, so I told PS to recursive remove the parent folder:</p>\n\n<p><img src=\"/content/images/2016/04/41.png\" alt=\"\" /></p>\n\n<p>PS started to complain and told me that the path was a junction point and that I needed to force delete the folder, so I tried that:</p>\n\n<p><img src=\"/content/images/2016/04/42.png\" alt=\"\" /></p>\n\n<p>Nothing happened it just stood there blinking and my laptop started to get slow. After some quick checks at the Task Manager I saw that PowerShell used 40 – 100% of my CPU and almost all of my 8 GB of memory. I killed the PowerShell process and got my resources back.</p>\n\n<p><img src=\"/content/images/2016/04/43.png\" alt=\"\" /></p>\n\n<p>Now I didn’t really know what to do and I stared to search the web. As it turns out Windows has a short alternative folder name so I needed to get that name. PowerShell made me disappointed the last time so I started CMD.exe and ran the dir command:</p>\n\n<p><img src=\"/content/images/2016/04/44.png\" alt=\"\" /></p>\n\n<p>Oh nice, the folder seemed to have some kind of random short name so I used that and removed it the rmdir command:</p>\n\n<p><img src=\"/content/images/2016/04/45.png\" alt=\"\" /></p>\n\n<p>Whohoo! The freaky folder was gone and I got my 7 GB back :). Maybe this helps someone else with the same problem, otherwise I just shared the start of my Saturday morning. I’m a forgiving person and five minutes later I were back using VirtualBox and PowerShell again.  Now I can go back to coding in my new virtual OS!</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":1461306912388,"created_by":1,"updated_at":1461307340262,"updated_by":1,"published_at":1345894620000,"published_by":1},{"id":15,"uuid":"26b635e9-3b3b-43e5-ba09-3230f3f39e59","title":"IMAGEVAULT 3 – UNABLE TO LOAD ONE OR MORE OF THE REQUESTED TYPES","slug":"imagevault-3-unable-to-load-one-or-more-of-the-requested-types","markdown":"Recognize this when fetching a EPiServer project from your version control system that uses ImageVault and your development environment doesn’t have ImageVault installed?\n\n![](/content/images/2016/04/50.png)\n\nIt could be related to the fact that ImageVault (version 3) requires six assemblies being preinstalled to the GAC and a lot of “leadtools”-assemblies in a “leadtools“-folder in your “bin”-folder.\n\nIf you don’t want your EPiServer application to depend on stuff in the GAC you can place them in the bin folder instead. But you probably don’t want to check-in your bin folder and add all of those “leadtools”-assemblies as references to your project, it would be a lot of references.\n\nYou could solve this by copying the required ImageVault assemblies to the bin folder (if they don’t already exist from last build) after a successful build.\n\nI placed the “ImageVault GAC”-assemblies and the leadtools”-assemblies in a “libraries”-folder outside of the project and added them to our version control system.\n\nAdded a post build script that will execute on every successful build:\n\n```text\npowershell -command \"& {Set-ExecutionPolicy Remotesigned}\"\npowershell $(ProjectDir)\\Buildscripts\\SetupImageVaultDependecies.ps1 $(SolutionDir) $(ProjectDir)$(OutDir)\n```\n\nand a PowerShell script (SetupImageVaultDependecies.ps1) that copies the assemblies and sets ACL-permissions for everyone to have full control (you probably want to set this more precise for production and test environments)\n\n```powershell\nParam( [string] $SolutionFolder , [string] $BinFolder)\n$targetLeadtoolsFolder = \"$BinFolder\\leadtools\"\nif ((Test-Path -path $targetLeadtoolsFolder) -eq $false) {\n    New-Item -Path $targetLeadtoolsFolder -ItemType \"directory\"\n    Copy-Item \"$SolutionFolder\\Library\\Leadtools\\*.*\" $targetLeadtoolsFolder\n    &icacls $targetLeadtoolsFolder /grant 'Everyone:(OI)(CI)F'\n    Copy-Item \"$SolutionFolder\\Library\\ImageVaultGac\\*.*\" $BinFolder\n}\n```\n\nI hope that ImageVault 4 doesn’t depend on stuff being preinstalled to the GAC and that the dependency to all of those “leadtools”-assemblies is removed.\n\n## Exception message in text\n\n```text\nUnable to load one or more of the requested types. The following information may be a subset of the Type/LoaderException information present - inspect with debugger for complete view.\nCheck assemblies [ImageVault.EPiServer6, Version=3.5.2.0, Culture=neutral, PublicKeyToken=null] and/or types [ImageVault.EPiServer6.Scheduler.ArchiverNotificationJob,\nImageVault.EPiServer6.ScheduledJobImpl,\nImageStoreNET.Developer.WebControls.PropertyComplexMediaControl,\nImageStoreNET.Developer.WebControls.IVFileFilterEventHandler,\nImageStoreNET.Developer.WebControls.IVAlbumTreeData,\nImageStoreNET.Developer.Core.IVFileSystem,\nImageVault.EPiServer6.Mirroring.IMirroringProperty,\nImageVault.EPiServer6.Mirroring.MirroringPropertyFreeText,\nImageVault.EPiServer6.LinkToolPlugin.LinkToolTinyMcePlugin,\nImageStoreNET.Developer.Core.NamespaceDoc,\nImageStoreNET.Developer.Core.IVAlbumDataCollection,\nImageVault.EPiServer6.Mirroring.MirroringPropertyURL,\nImageStoreNET.Developer.WebControls.PropertyImageVaultFile,\nImageStoreNET.Developer.WebControls.IVAlbumTemplateContainer,\nImageStoreNET.Developer.Security.IVAccessLevel,\nImageStoreNET.Developer.Filters.IVFileSortBy,\nImageStoreNET.Admin.TestRemoteSites,\nImageStoreNET.Admin.TestClient,\nImageVault.EPiServer6.ConfigImpl,\nImageStoreNET.Developer.WebControls.PropertyFileControl]. Information from LoaderExceptions property [Method 'GetEPiLicenseData' in type 'ImageVault.EPiServer6.SecurityImpl' from assembly 'ImageVault.EPiServer6, Version=3.5.2.0, Culture=neutral, PublicKeyToken=null' does not have an implementation.].\n```\n","html":"<p>Recognize this when fetching a EPiServer project from your version control system that uses ImageVault and your development environment doesn’t have ImageVault installed?</p>\n\n<p><img src=\"/content/images/2016/04/50.png\" alt=\"\" /></p>\n\n<p>It could be related to the fact that ImageVault (version 3) requires six assemblies being preinstalled to the GAC and a lot of “leadtools”-assemblies in a “leadtools“-folder in your “bin”-folder.</p>\n\n<p>If you don’t want your EPiServer application to depend on stuff in the GAC you can place them in the bin folder instead. But you probably don’t want to check-in your bin folder and add all of those “leadtools”-assemblies as references to your project, it would be a lot of references.</p>\n\n<p>You could solve this by copying the required ImageVault assemblies to the bin folder (if they don’t already exist from last build) after a successful build.</p>\n\n<p>I placed the “ImageVault GAC”-assemblies and the leadtools”-assemblies in a “libraries”-folder outside of the project and added them to our version control system.</p>\n\n<p>Added a post build script that will execute on every successful build:</p>\n\n<pre><code class=\"language-text\">powershell -command \"&amp; {Set-ExecutionPolicy Remotesigned}\"  \npowershell $(ProjectDir)\\Buildscripts\\SetupImageVaultDependecies.ps1 $(SolutionDir) $(ProjectDir)$(OutDir)  \n</code></pre>\n\n<p>and a PowerShell script (SetupImageVaultDependecies.ps1) that copies the assemblies and sets ACL-permissions for everyone to have full control (you probably want to set this more precise for production and test environments)</p>\n\n<pre><code class=\"language-powershell\">Param( [string] $SolutionFolder , [string] $BinFolder)  \n$targetLeadtoolsFolder = \"$BinFolder\\leadtools\"\nif ((Test-Path -path $targetLeadtoolsFolder) -eq $false) {  \n    New-Item -Path $targetLeadtoolsFolder -ItemType \"directory\"\n    Copy-Item \"$SolutionFolder\\Library\\Leadtools\\*.*\" $targetLeadtoolsFolder\n    &amp;icacls $targetLeadtoolsFolder /grant 'Everyone:(OI)(CI)F'\n    Copy-Item \"$SolutionFolder\\Library\\ImageVaultGac\\*.*\" $BinFolder\n}\n</code></pre>\n\n<p>I hope that ImageVault 4 doesn’t depend on stuff being preinstalled to the GAC and that the dependency to all of those “leadtools”-assemblies is removed.</p>\n\n<h2 id=\"exceptionmessageintext\">Exception message in text</h2>\n\n<pre><code class=\"language-text\">Unable to load one or more of the requested types. The following information may be a subset of the Type/LoaderException information present - inspect with debugger for complete view.  \nCheck assemblies [ImageVault.EPiServer6, Version=3.5.2.0, Culture=neutral, PublicKeyToken=null] and/or types [ImageVault.EPiServer6.Scheduler.ArchiverNotificationJob,  \nImageVault.EPiServer6.ScheduledJobImpl,  \nImageStoreNET.Developer.WebControls.PropertyComplexMediaControl,  \nImageStoreNET.Developer.WebControls.IVFileFilterEventHandler,  \nImageStoreNET.Developer.WebControls.IVAlbumTreeData,  \nImageStoreNET.Developer.Core.IVFileSystem,  \nImageVault.EPiServer6.Mirroring.IMirroringProperty,  \nImageVault.EPiServer6.Mirroring.MirroringPropertyFreeText,  \nImageVault.EPiServer6.LinkToolPlugin.LinkToolTinyMcePlugin,  \nImageStoreNET.Developer.Core.NamespaceDoc,  \nImageStoreNET.Developer.Core.IVAlbumDataCollection,  \nImageVault.EPiServer6.Mirroring.MirroringPropertyURL,  \nImageStoreNET.Developer.WebControls.PropertyImageVaultFile,  \nImageStoreNET.Developer.WebControls.IVAlbumTemplateContainer,  \nImageStoreNET.Developer.Security.IVAccessLevel,  \nImageStoreNET.Developer.Filters.IVFileSortBy,  \nImageStoreNET.Admin.TestRemoteSites,  \nImageStoreNET.Admin.TestClient,  \nImageVault.EPiServer6.ConfigImpl,  \nImageStoreNET.Developer.WebControls.PropertyFileControl]. Information from LoaderExceptions property [Method 'GetEPiLicenseData' in type 'ImageVault.EPiServer6.SecurityImpl' from assembly 'ImageVault.EPiServer6, Version=3.5.2.0, Culture=neutral, PublicKeyToken=null' does not have an implementation.].  \n</code></pre>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":1461307408056,"created_by":1,"updated_at":1461307574543,"updated_by":1,"published_at":1349350620000,"published_by":1},{"id":16,"uuid":"62038f56-87e9-4b00-b525-b6a722d2c3d6","title":"PREVENT POST BACK ON ENTER IN AN ASP.NET INPUT FIELD (ASP:TEXTBOX)","slug":"prevent-post-back-on-enter-in-an-asp-net-input-field-asp-textbox","markdown":"Lets say you have the following ASPX code:\n\n```html\n<div id=\"LoginWraper\">\n    <asp:TextBox runat=\"server\" ID=\"TbUsername\" ClientIDMode=\"Static\" />\n    <asp:TextBox runat=\"server\" ID=\"TbPassword\" TextMode=\"Password\" ClientIDMode=\"Static\" />\n    <asp:Button runat=\"server\" ID=\"BtnLogin\" ClientIDMode=\"Static\" Text=\"Login\"/>\n</div>\n```\n\nIf you want to prevent ASP.NET from doing a post back on a “enter press” in “TbUsername” or “TbPassword” when the user is calling himself “Magneto” you will probably end up with something like this:\n\n```javascript\n$(\"#TbUsername, #TbPassword\").on(\"keypress\", function (e) {\n    var keycode = (e.keyCode ? e.keyCode : e.which);\n    if (keycode == '13') {\n        var username = $(\"#TbUsername\").val();\n        if (username === \"magneto\") {\n            console.log(\"magneto is not allowed to login\");\n            return;\n        }\n        $(\"#BtnLogin\").click();\n    }\n});\n```\n\nThis JavaScript will not prevent ASP.NET from doing a post back. Your code will be ignored because ASP.NET has made his magic way earlier that your code gets the possibility to execute.\n\nOne way to take the control back from ASP.NET is to prevent all further actions when a user hits enter followed by an key press in any input field (except textareas). I ended up with this:\n\n```javascript\n$(document).ready(function () {\n    $($(\"form\")[0]).on(\"keypress\", function (e) {\n        var keycode = (e.keyCode ? e.keyCode : e.which);\n        if (keycode == '13') {\n            if ($(e.target).prop(\"type\") === \"textarea\") {\n                return;\n            }\n            e.preventDefault();\n            e.stopPropagation();\n            return false;\n        }\n    });\n});\n```\n\nThe code is very dependent on jQuery. :)","html":"<p>Lets say you have the following ASPX code:</p>\n\n<pre><code class=\"language-html\">&lt;div id=\"LoginWraper\"&gt;  \n    &lt;asp:TextBox runat=\"server\" ID=\"TbUsername\" ClientIDMode=\"Static\" /&gt;\n    &lt;asp:TextBox runat=\"server\" ID=\"TbPassword\" TextMode=\"Password\" ClientIDMode=\"Static\" /&gt;\n    &lt;asp:Button runat=\"server\" ID=\"BtnLogin\" ClientIDMode=\"Static\" Text=\"Login\"/&gt;\n&lt;/div&gt;  \n</code></pre>\n\n<p>If you want to prevent ASP.NET from doing a post back on a “enter press” in “TbUsername” or “TbPassword” when the user is calling himself “Magneto” you will probably end up with something like this:</p>\n\n<pre><code class=\"language-javascript\">$(\"#TbUsername, #TbPassword\").on(\"keypress\", function (e) {\n    var keycode = (e.keyCode ? e.keyCode : e.which);\n    if (keycode == '13') {\n        var username = $(\"#TbUsername\").val();\n        if (username === \"magneto\") {\n            console.log(\"magneto is not allowed to login\");\n            return;\n        }\n        $(\"#BtnLogin\").click();\n    }\n});\n</code></pre>\n\n<p>This JavaScript will not prevent ASP.NET from doing a post back. Your code will be ignored because ASP.NET has made his magic way earlier that your code gets the possibility to execute.</p>\n\n<p>One way to take the control back from ASP.NET is to prevent all further actions when a user hits enter followed by an key press in any input field (except textareas). I ended up with this:</p>\n\n<pre><code class=\"language-javascript\">$(document).ready(function () {\n    $($(\"form\")[0]).on(\"keypress\", function (e) {\n        var keycode = (e.keyCode ? e.keyCode : e.which);\n        if (keycode == '13') {\n            if ($(e.target).prop(\"type\") === \"textarea\") {\n                return;\n            }\n            e.preventDefault();\n            e.stopPropagation();\n            return false;\n        }\n    });\n});\n</code></pre>\n\n<p>The code is very dependent on jQuery. :)</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":1461307660629,"created_by":1,"updated_at":1461307759166,"updated_by":1,"published_at":1355229420000,"published_by":1},{"id":17,"uuid":"ad81e52e-bba2-460f-8d54-89e221f27681","title":"AUTOMATING PRIVATE BUILDS WITH GITHUB, JENKINS, PSAKE AND MS WEB DEPLOY","slug":"automating-private-builds-with-github-jenkins-psake-and-ms-web-deploy","markdown":"I have written some posts before on similar topics. In the latest [post](/2012/08/15/continuous-integration-with-jenkins-and-psake/) about this topic I wrote: “To get a more reliable solution we could deploy using web deploy”. In this post I will go through how we at Cloud Connected has setup our build server with GitHub, Jenkins, Psake and MS Web Deploy.\n\nThis setup is primary for building and deploying ASP.NET web application like EPiServer CMS but we could use it to build and deploy other projects as well like for example PHP, NodeJs and Java projects.\n\n## Prerequisites\nAt Cloud Connected we host almost all of our source code at GitHub. Our build server is a Windows Server 2008 R2 with \n[Jenkins 1.501](https://jenkins.io/index.html), \n[MS Web deploy 3](http://www.iis.net/downloads/microsoft/web-deploy), \n[GIT 1.8.1.2 for windows](https://git-scm.com/downloads), and .NET 4.5 installed on it. The destination webserver for this post is a Windows Server 2012 with NET 4.5  and MS Web Deploy installed on it.\n\n## Private repositories at GitHub\nMost of our clients does not want us to host their projects as open source projects. So we have created private repositories for them. Private GitHub repositories requires our build server to authenticate with GitHub to be able to pull the source code. We do this with local windows accounts and private SSH keys like this:\n\n* We have a local windows user called “Jenkins” on our build server\n* Our windows service that is hosting Jenkins is running as the “Jenkins” user:\n\n![](/content/images/2016/04/60.png)\n\n## SSH keys\nTo authenticate we use SSH keys and then we pull the source code over SSH. We generated the keys with GIT bash (with a empty passphrase) exactly like GitHub describes it here. We created the keys in the context of the Jenkins user, we were actually logged in as the Jenkins user during the key generation to make sure the keys would relate to the Jenkins user.\n\nCloud Connected is an organization at GitHub and organizations can not have a single ssh key added globally for all repositories (as I understand?). For simplicity we had the build server authenticate through a member of the Cloud Connected organization, we just added the SSH key to my account like this:\n\n![](/content/images/2016/04/61.png)\n\nIf the setup is correct you should have the SSH keys generated at “C:\\Users\\Jenkins\\.ssh” like this: \n\n![](/content/images/2016/04/62.png)\n\nAnd you should be able to authenticate with GitHub through GIT bash using:\n\n![](/content/images/2016/04/63.png)\n\n## Deploying using MS Web Deploy\nAs I said in my previous post deploying using web deploy is much more stable that deploying using something like file shares or ftp and the build script is simpler!\n\nTo pull the source from GitHub we use this [plugin](https://wiki.jenkins-ci.org/display/JENKINS/Github+Plugin) (version 1.5). I can not show you the exact configuration because it contains some secret information but to configure the GitHub account information is no hassle. The set up is almost identical to the configuration from my [previous post](/2012/08/15/continuous-integration-with-jenkins-and-psake/) with the difference of using SSH instead of HTTPS.\n\nWe are not using any deployment hooks. We have just scheduled Jenkins to fetch the source and do the deploy after a specified interval.\n\nAfter Jenkins has pulled the source Jenkins executes a PowerShell script that does the actual deploy (the script is pushed to the GitHub repository so Jenkis can find it) like this:\n\n![](/content/images/2016/04/64.png)\n\nthe build.ps1 looks like this (this script is mostly unique per project we build):\n\n```powershell\nparam(\n    [alias(\"env\")]\n    $Environment = 'debug'\n)\n \nfunction Build() {\n    if($Environment -ieq 'debug') {\n        Write-Host \"sorry, script does not support debug build...\"\n    }\n    if($Environment -ieq 'test') {\n        .\\ProjectName.Build\\psake.ps1 \".\\ProjectName.Build\\Deploy.ps1\" -properties @{ config='test'; environment=\"$Environment\";} \"test\"\n    }\n    Write-Host \"$Environment build done!\"\n    if ($psake.build_success -eq $false) {\n        exit 1\n    } else {\n        exit 0\n    }\n}\n \nBuild\n``` \n\n1. The script takes a single parameter that specifies the environment to build and deploy for.\n2. The script checks which environment to build for and then calls psake.ps1 with correct build script and parameters for the specified environment.\n\nPsake.ps1 is a wrapper around the Psake project to be able to execute Psake without being forced to install Psake as a PowerShell module. [Psake.ps1](https://github.com/psake/psake/blob/master/psake.ps1) can be found in the same [repository](https://github.com/psake/psake) as Psake.\n\nWe have placed Psake.ps1 in same folder as psake.psm1 together with our deploy script (for simplicity) like this:\n\n![](/content/images/2016/04/65.png)\n\nNow the fun part, Deploy.ps1 looks like this (this script is mostly unique per project we build):\n\n```powershell\nproperties {\n    $dateLabel = ([DateTime]::Now.ToString(\"yyyy-MM-dd_HH-mm-ss\"))\n    $baseDir = resolve-path .\\..\\\n    $testProjectDir = \"$baseDir\\ProjectName.Tests\"\n    $testRunnerExe = \"$baseDir\\packages\\NUnit.Runners.2.6.2\\tools\\nunit-console.exe\"\n    $webAppProjectDir = \"$baseDir\\ProjectName.Web\"\n    $deployDir = \"$baseDir\\ProjectName.Deploy\"\n    $deployPackagePathAndFileName = \"$deployDir\\ProjectName-$config-$dateLabel.zip\"\n    $webDeployExePath = \"C:\\Program Files\\IIS\\Microsoft Web Deploy V3\\\"\n    $config = 'debug'\n    $environment = 'debug'\n}\n \ntask test -depends deploy\n \ntask compileUnitTests {\n    $csprojFile = \"$testProjectDir\\ProjectName.Tests.csproj\"\n    & msbuild $csprojFile /t:Clean /t:Build /p:Configuration=\"debug\" /v:q\n    if($LASTEXITCODE -ne 0) {\n        throw \"Failed to compile unit test assembly\"\n        exit 1\n    }\n}\n \ntask runUnitTests -depends compileUnitTests {\n    & $testRunnerExe \"$testProjectDir\\bin\\debug\\ProjectName.Tests.dll\"\n    if($LASTEXITCODE -ne 0) {\n        throw \"Failed to run unit tests\"\n        exit 1\n    }\n}\n \ntask createDeployPackage -depends runUnitTests {\n    Remove-ThenAddFolder($deployDir)\n    $csprojFile = \"$webAppProjectDir\\ProjectName.Web.csproj\"\n    & msbuild $csprojFile /t:Package /p:Platform=AnyCPU /p:Configuration=$config /p:PackageLocation=$deployPackagePathAndFileName /v:q\n    if($LASTEXITCODE -ne 0) {\n        throw \"Failed to create deploy package\"\n        exit 1\n    }\n}\n \ntask deploy -depends createDeployPackage {\n    $msdeploy = \"$webDeployExePath\\msdeploy.exe\"\n    $arg1 = \"-verb:sync\"\n    $arg2 = \"-source:package=$deployPackagePathAndFileName\"\n    $arg3 = \"-dest:auto,ComputerName='http://10.100.100.101/MsDeployAgentService',username='WEBSERVERNAME\\WebDeploy',password='password'\"\n    $arg4 = \"-retryAttempts=2\"\n    & $msdeploy $arg1 $arg2 $arg3 $arg4\n    if($LASTEXITCODE -ne 0) {\n        throw \"Failed to deploy to $config\"\n        exit 1\n    }\n}\n \n#helper methods\nfunction Remove-ThenAddFolder([string]$name) {\n    Remove-IfExists $name\n    New-Item -Path $name -ItemType \"directory\"\n}\n \nfunction Remove-IfExists([string]$name) {\n    if ((Test-Path -path $name)) {\n        dir $name -recurse | where {!@(dir -force $_.fullname)} | rm\n        Remove-Item $name -Recurse\n    }\n}\n```\n\nThe script contains four tasks that is executed in this order:\n\n- compileUnitTests\n - Compiles the projects unit test assembly, breaks the build if the assembly can not compile\n- runUnitTests\n - Runs the unit test in the assembly using nunit’s console runner. Breaks the build of not all tests passes.\n- createDeployPackage\n - Creates a empty folder then uses msbuild to build the web application and create a deployment package in the empty folder. Breaks the build if the package could not be created.\n- deploy\n - Uses MS Web Deploy to deploy the package to our test server. Breaks the build if the package can not be deployed to the webserver.\n\n## Nuget package restore\nWe use the awesome [package restore feature](http://docs.nuget.org/consume/package-restore/msbuild-integrated) of Nuget. We just have our package config files pushed to github and lets our build server fetch required packages before building. The only issues we had was the fact that we were required to add the environment variable EnableNuGetPackageRestore with the value true.\n\n## MS Web Deploy 3.0 issues\n### Authentication\nThis were actually the hardest thing to configure in this setup which is pretty stupid because you could think that this should be supported out of the box. Maybe I am missing something (almost hope so) otherwise this needs to be simpler than this:\n\nThe test webserver has a local windows user account named “webdeploy” when executing ms webdeploy we set the credentials for the webdeploy user. The webdeploy user must be an local administrator (annoys me) else webdeploy will shout out that the user must be have “administrative privileges” when trying to deploy.\n\nThis is were stuff is getting funky… If you do as web deploy sais and uses a user that has administrative privileges you will get the error “(401) Unauthorized” back from the destination server, say whaaat?\n\nIt turns out that remote login is not enabled by default. To enable it we must add a registry entry :O.\n\nAfter adding the entry **LocalAccountTokenFilterPolicy** with **DWORD Vaule :  1** under **HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Policies\\System** at the test webserver the webdeploy user can login through msdeploy.\n\n### Paths\nWe were struggling for some time to get msbuild to accept a path with spaces for the -source:package parameter. We never succeeded with this I think it is a bug in msdeploy (or I missed something, please let me know). However we changed Jenkins default workspace folder to a path that did not include any spaces, problem solved.\n\n### Security\nAll deploys to test servers are done on a internal network over VPN’s. No deploys are done on the “public internet” and all endpoint for the ms web deploy services are blocked through firewalls.\n\n### Conclusion\nThe setup is up and running and deploying to our test environment every night. MSDeploy is really stable and deploys the packages without any troubles It was not to hard to set up besides that registry and file path crap. Took me about 8 hours with coffee breaks :). But I have experience with Jenkins, Psake, PowerShell, Git and MsDeploy since before.\n\nI hope this is a setup you would consider for your deployment servers as well, please let me know if you have ideas for improvements our tips from your ci setups.","html":"<p>I have written some posts before on similar topics. In the latest <a href=\"/2012/08/15/continuous-integration-with-jenkins-and-psake/\">post</a> about this topic I wrote: “To get a more reliable solution we could deploy using web deploy”. In this post I will go through how we at Cloud Connected has setup our build server with GitHub, Jenkins, Psake and MS Web Deploy.</p>\n\n<p>This setup is primary for building and deploying ASP.NET web application like EPiServer CMS but we could use it to build and deploy other projects as well like for example PHP, NodeJs and Java projects.</p>\n\n<h2 id=\"prerequisites\">Prerequisites</h2>\n\n<p>At Cloud Connected we host almost all of our source code at GitHub. Our build server is a Windows Server 2008 R2 with <br />\n<a href=\"https://jenkins.io/index.html\">Jenkins 1.501</a>, \n<a href=\"http://www.iis.net/downloads/microsoft/web-deploy\">MS Web deploy 3</a>, \n<a href=\"https://git-scm.com/downloads\">GIT 1.8.1.2 for windows</a>, and .NET 4.5 installed on it. The destination webserver for this post is a Windows Server 2012 with NET 4.5  and MS Web Deploy installed on it.</p>\n\n<h2 id=\"privaterepositoriesatgithub\">Private repositories at GitHub</h2>\n\n<p>Most of our clients does not want us to host their projects as open source projects. So we have created private repositories for them. Private GitHub repositories requires our build server to authenticate with GitHub to be able to pull the source code. We do this with local windows accounts and private SSH keys like this:</p>\n\n<ul>\n<li>We have a local windows user called “Jenkins” on our build server</li>\n<li>Our windows service that is hosting Jenkins is running as the “Jenkins” user:</li>\n</ul>\n\n<p><img src=\"/content/images/2016/04/60.png\" alt=\"\" /></p>\n\n<h2 id=\"sshkeys\">SSH keys</h2>\n\n<p>To authenticate we use SSH keys and then we pull the source code over SSH. We generated the keys with GIT bash (with a empty passphrase) exactly like GitHub describes it here. We created the keys in the context of the Jenkins user, we were actually logged in as the Jenkins user during the key generation to make sure the keys would relate to the Jenkins user.</p>\n\n<p>Cloud Connected is an organization at GitHub and organizations can not have a single ssh key added globally for all repositories (as I understand?). For simplicity we had the build server authenticate through a member of the Cloud Connected organization, we just added the SSH key to my account like this:</p>\n\n<p><img src=\"/content/images/2016/04/61.png\" alt=\"\" /></p>\n\n<p>If the setup is correct you should have the SSH keys generated at “C:\\Users\\Jenkins.ssh” like this: </p>\n\n<p><img src=\"/content/images/2016/04/62.png\" alt=\"\" /></p>\n\n<p>And you should be able to authenticate with GitHub through GIT bash using:</p>\n\n<p><img src=\"/content/images/2016/04/63.png\" alt=\"\" /></p>\n\n<h2 id=\"deployingusingmswebdeploy\">Deploying using MS Web Deploy</h2>\n\n<p>As I said in my previous post deploying using web deploy is much more stable that deploying using something like file shares or ftp and the build script is simpler!</p>\n\n<p>To pull the source from GitHub we use this <a href=\"https://wiki.jenkins-ci.org/display/JENKINS/Github+Plugin\">plugin</a> (version 1.5). I can not show you the exact configuration because it contains some secret information but to configure the GitHub account information is no hassle. The set up is almost identical to the configuration from my <a href=\"/2012/08/15/continuous-integration-with-jenkins-and-psake/\">previous post</a> with the difference of using SSH instead of HTTPS.</p>\n\n<p>We are not using any deployment hooks. We have just scheduled Jenkins to fetch the source and do the deploy after a specified interval.</p>\n\n<p>After Jenkins has pulled the source Jenkins executes a PowerShell script that does the actual deploy (the script is pushed to the GitHub repository so Jenkis can find it) like this:</p>\n\n<p><img src=\"/content/images/2016/04/64.png\" alt=\"\" /></p>\n\n<p>the build.ps1 looks like this (this script is mostly unique per project we build):</p>\n\n<pre><code class=\"language-powershell\">param(  \n    [alias(\"env\")]\n    $Environment = 'debug'\n)\n\nfunction Build() {  \n    if($Environment -ieq 'debug') {\n        Write-Host \"sorry, script does not support debug build...\"\n    }\n    if($Environment -ieq 'test') {\n        .\\ProjectName.Build\\psake.ps1 \".\\ProjectName.Build\\Deploy.ps1\" -properties @{ config='test'; environment=\"$Environment\";} \"test\"\n    }\n    Write-Host \"$Environment build done!\"\n    if ($psake.build_success -eq $false) {\n        exit 1\n    } else {\n        exit 0\n    }\n}\n\nBuild  \n</code></pre>\n\n<ol>\n<li>The script takes a single parameter that specifies the environment to build and deploy for.  </li>\n<li>The script checks which environment to build for and then calls psake.ps1 with correct build script and parameters for the specified environment.</li>\n</ol>\n\n<p>Psake.ps1 is a wrapper around the Psake project to be able to execute Psake without being forced to install Psake as a PowerShell module. <a href=\"https://github.com/psake/psake/blob/master/psake.ps1\">Psake.ps1</a> can be found in the same <a href=\"https://github.com/psake/psake\">repository</a> as Psake.</p>\n\n<p>We have placed Psake.ps1 in same folder as psake.psm1 together with our deploy script (for simplicity) like this:</p>\n\n<p><img src=\"/content/images/2016/04/65.png\" alt=\"\" /></p>\n\n<p>Now the fun part, Deploy.ps1 looks like this (this script is mostly unique per project we build):</p>\n\n<pre><code class=\"language-powershell\">properties {  \n    $dateLabel = ([DateTime]::Now.ToString(\"yyyy-MM-dd_HH-mm-ss\"))\n    $baseDir = resolve-path .\\..\\\n    $testProjectDir = \"$baseDir\\ProjectName.Tests\"\n    $testRunnerExe = \"$baseDir\\packages\\NUnit.Runners.2.6.2\\tools\\nunit-console.exe\"\n    $webAppProjectDir = \"$baseDir\\ProjectName.Web\"\n    $deployDir = \"$baseDir\\ProjectName.Deploy\"\n    $deployPackagePathAndFileName = \"$deployDir\\ProjectName-$config-$dateLabel.zip\"\n    $webDeployExePath = \"C:\\Program Files\\IIS\\Microsoft Web Deploy V3\\\"\n    $config = 'debug'\n    $environment = 'debug'\n}\n\ntask test -depends deploy\n\ntask compileUnitTests {  \n    $csprojFile = \"$testProjectDir\\ProjectName.Tests.csproj\"\n    &amp; msbuild $csprojFile /t:Clean /t:Build /p:Configuration=\"debug\" /v:q\n    if($LASTEXITCODE -ne 0) {\n        throw \"Failed to compile unit test assembly\"\n        exit 1\n    }\n}\n\ntask runUnitTests -depends compileUnitTests {  \n    &amp; $testRunnerExe \"$testProjectDir\\bin\\debug\\ProjectName.Tests.dll\"\n    if($LASTEXITCODE -ne 0) {\n        throw \"Failed to run unit tests\"\n        exit 1\n    }\n}\n\ntask createDeployPackage -depends runUnitTests {  \n    Remove-ThenAddFolder($deployDir)\n    $csprojFile = \"$webAppProjectDir\\ProjectName.Web.csproj\"\n    &amp; msbuild $csprojFile /t:Package /p:Platform=AnyCPU /p:Configuration=$config /p:PackageLocation=$deployPackagePathAndFileName /v:q\n    if($LASTEXITCODE -ne 0) {\n        throw \"Failed to create deploy package\"\n        exit 1\n    }\n}\n\ntask deploy -depends createDeployPackage {  \n    $msdeploy = \"$webDeployExePath\\msdeploy.exe\"\n    $arg1 = \"-verb:sync\"\n    $arg2 = \"-source:package=$deployPackagePathAndFileName\"\n    $arg3 = \"-dest:auto,ComputerName='http://10.100.100.101/MsDeployAgentService',username='WEBSERVERNAME\\WebDeploy',password='password'\"\n    $arg4 = \"-retryAttempts=2\"\n    &amp; $msdeploy $arg1 $arg2 $arg3 $arg4\n    if($LASTEXITCODE -ne 0) {\n        throw \"Failed to deploy to $config\"\n        exit 1\n    }\n}\n\n#helper methods\nfunction Remove-ThenAddFolder([string]$name) {  \n    Remove-IfExists $name\n    New-Item -Path $name -ItemType \"directory\"\n}\n\nfunction Remove-IfExists([string]$name) {  \n    if ((Test-Path -path $name)) {\n        dir $name -recurse | where {!@(dir -force $_.fullname)} | rm\n        Remove-Item $name -Recurse\n    }\n}\n</code></pre>\n\n<p>The script contains four tasks that is executed in this order:</p>\n\n<ul>\n<li>compileUnitTests\n<ul><li>Compiles the projects unit test assembly, breaks the build if the assembly can not compile</li></ul></li>\n<li>runUnitTests\n<ul><li>Runs the unit test in the assembly using nunit’s console runner. Breaks the build of not all tests passes.</li></ul></li>\n<li>createDeployPackage\n<ul><li>Creates a empty folder then uses msbuild to build the web application and create a deployment package in the empty folder. Breaks the build if the package could not be created.</li></ul></li>\n<li>deploy\n<ul><li>Uses MS Web Deploy to deploy the package to our test server. Breaks the build if the package can not be deployed to the webserver.</li></ul></li>\n</ul>\n\n<h2 id=\"nugetpackagerestore\">Nuget package restore</h2>\n\n<p>We use the awesome <a href=\"http://docs.nuget.org/consume/package-restore/msbuild-integrated\">package restore feature</a> of Nuget. We just have our package config files pushed to github and lets our build server fetch required packages before building. The only issues we had was the fact that we were required to add the environment variable EnableNuGetPackageRestore with the value true.</p>\n\n<h2 id=\"mswebdeploy30issues\">MS Web Deploy 3.0 issues</h2>\n\n<h3 id=\"authentication\">Authentication</h3>\n\n<p>This were actually the hardest thing to configure in this setup which is pretty stupid because you could think that this should be supported out of the box. Maybe I am missing something (almost hope so) otherwise this needs to be simpler than this:</p>\n\n<p>The test webserver has a local windows user account named “webdeploy” when executing ms webdeploy we set the credentials for the webdeploy user. The webdeploy user must be an local administrator (annoys me) else webdeploy will shout out that the user must be have “administrative privileges” when trying to deploy.</p>\n\n<p>This is were stuff is getting funky… If you do as web deploy sais and uses a user that has administrative privileges you will get the error “(401) Unauthorized” back from the destination server, say whaaat?</p>\n\n<p>It turns out that remote login is not enabled by default. To enable it we must add a registry entry :O.</p>\n\n<p>After adding the entry <strong>LocalAccountTokenFilterPolicy</strong> with <strong>DWORD Vaule :  1</strong> under <strong>HKEY<em>LOCAL</em>MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Policies\\System</strong> at the test webserver the webdeploy user can login through msdeploy.</p>\n\n<h3 id=\"paths\">Paths</h3>\n\n<p>We were struggling for some time to get msbuild to accept a path with spaces for the -source:package parameter. We never succeeded with this I think it is a bug in msdeploy (or I missed something, please let me know). However we changed Jenkins default workspace folder to a path that did not include any spaces, problem solved.</p>\n\n<h3 id=\"security\">Security</h3>\n\n<p>All deploys to test servers are done on a internal network over VPN’s. No deploys are done on the “public internet” and all endpoint for the ms web deploy services are blocked through firewalls.</p>\n\n<h3 id=\"conclusion\">Conclusion</h3>\n\n<p>The setup is up and running and deploying to our test environment every night. MSDeploy is really stable and deploys the packages without any troubles It was not to hard to set up besides that registry and file path crap. Took me about 8 hours with coffee breaks :). But I have experience with Jenkins, Psake, PowerShell, Git and MsDeploy since before.</p>\n\n<p>I hope this is a setup you would consider for your deployment servers as well, please let me know if you have ideas for improvements our tips from your ci setups.</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":1461307804219,"created_by":1,"updated_at":1461308509921,"updated_by":1,"published_at":1361104620000,"published_by":1},{"id":18,"uuid":"e1bb2a94-3eb7-4654-986e-dd9ad99445f8","title":"POWERSHELL + UNC PATH + CREDENTIALS","slug":"powershell-unc-path-credentials","markdown":"Today I was faced with the fact that one of our backup processes needed to copy compressed database backups to a remote server over an UNC path every night. Of course were the UNC path protected with credentials other than the credentials my script were running under. The backup process is in fact a PowerShell script that is scheduled by the built in Windows scheduler. This may not seem as such a hard task and the solution is pretty simple but it took me a while to figure it out. If you’re thinking about mapping a network drive against the UNC path in advanced (I did too)… well then you can’t run your script with Windows built in scheduler because the context the Windows scheduler is running in can’t somehow find network drives that has been mapped in advanced… Also if you’re thinking about remapping the network drive in the script every time the script is executed something like:\n\n```powershell\n$net = New-Object -comobject Wscript.Network\n$net.MapNetworkDrive(\"Q:\",\"\\\\path\\to\\share\",0,\"domain\\user\",\"password\")\n```\n\nwell, then I think you’ll get a headache :) So after a while I started to think about how I would copy my files manually to the UNC path. If so; then I would just try to access the UNC path, Windows would ask me about credentials, I would enter them and then windows would cache the credentials I just entered while I do my copying! Ahaa! Maybe my script can do the same thing like this:\n\n```powershell\n$uncServer = \"\\\\10.11.12.124\"\n$uncFullPath = \"$uncServer\\my\\backup\\folder\"\n$username = \"anton\"\n$password = \"p@ssw0rd\"\n \nnet use $uncServer $password /USER:$username\ntry\n{\n#copy the backup\nCopy-Item $zipFileName $uncFullPath\n#remove all zips older than 1 month from the unc path\nGet-ChildItem \"$uncFullPath\\*.zip\" |? {$_.lastwritetime -le (Get-Date).AddMonths(-1)} |% {Remove-Item $_ -force }\n}\ncatch [System.Exception] {\nWriteToLog -msg \"could not copy backup to remote server... $_.Exception.Message\" -type Error\n}\nfinally {\nnet use $uncServer /delete\n}\n```\n\nAnd as it seems Windows does cache the credentials in this case too! :) Now backups are being copied to the remote server!","html":"<p>Today I was faced with the fact that one of our backup processes needed to copy compressed database backups to a remote server over an UNC path every night. Of course were the UNC path protected with credentials other than the credentials my script were running under. The backup process is in fact a PowerShell script that is scheduled by the built in Windows scheduler. This may not seem as such a hard task and the solution is pretty simple but it took me a while to figure it out. If you’re thinking about mapping a network drive against the UNC path in advanced (I did too)… well then you can’t run your script with Windows built in scheduler because the context the Windows scheduler is running in can’t somehow find network drives that has been mapped in advanced… Also if you’re thinking about remapping the network drive in the script every time the script is executed something like:</p>\n\n<pre><code class=\"language-powershell\">$net = New-Object -comobject Wscript.Network\n$net.MapNetworkDrive(\"Q:\",\"\\\\path\\to\\share\",0,\"domain\\user\",\"password\")\n</code></pre>\n\n<p>well, then I think you’ll get a headache :) So after a while I started to think about how I would copy my files manually to the UNC path. If so; then I would just try to access the UNC path, Windows would ask me about credentials, I would enter them and then windows would cache the credentials I just entered while I do my copying! Ahaa! Maybe my script can do the same thing like this:</p>\n\n<pre><code class=\"language-powershell\">$uncServer = \"\\\\10.11.12.124\"\n$uncFullPath = \"$uncServer\\my\\backup\\folder\"\n$username = \"anton\"\n$password = \"p@ssw0rd\"\n\nnet use $uncServer $password /USER:$username  \ntry  \n{\n#copy the backup\nCopy-Item $zipFileName $uncFullPath  \n#remove all zips older than 1 month from the unc path\nGet-ChildItem \"$uncFullPath\\*.zip\" |? {$_.lastwritetime -le (Get-Date).AddMonths(-1)} |% {Remove-Item $_ -force }  \n}\ncatch [System.Exception] {  \nWriteToLog -msg \"could not copy backup to remote server... $_.Exception.Message\" -type Error  \n}\nfinally {  \nnet use $uncServer /delete  \n}\n</code></pre>\n\n<p>And as it seems Windows does cache the credentials in this case too! :) Now backups are being copied to the remote server!</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":1461308532768,"created_by":1,"updated_at":1461308613867,"updated_by":1,"published_at":1366457820000,"published_by":1},{"id":19,"uuid":"b7be099b-5684-44fc-ab44-fece76fbde79","title":"USING POWERSHELL TO BACKUP SQLSERVER DATABASES","slug":"using-powershell-to-backup-sqlserver-databases","markdown":"A backup process can be really simple or very complex depending on your requirements. In this post I’m going to show how you can backup your sql server databases by using PowerShell. If you don’t want to use this approach you can of course use [sql server agent and t-sql](https://blogs.msdn.microsoft.com/sqlagent/2010/10/12/create-a-database-backup-job-using-sql-server-management-studio/) or something like that.\n\nThe backup script is executed in PowerShell like this:\n\n```text\n\\Backup.ps1 -serverName \"WIN-1GA63L4PFK6\\SQLEXPRESS\" -backupDirectory \"C:\\Backup\\Backups\" -daysToStoreBackups 7 \n```\n\nIf you need to schedule your backups you can use windows built in scheduler. For simplicity you could wrap the execution of the backup script within a bat-script. Just create a new scheduled task and set the action to execute the bat script:\n\n![](/content/images/2016/04/70.png)\n\nPlease note that you need to choose “run whether user is logged on or not” under the “general”-tab to allow the task to execute when you are not logged on at the server. Also, make sure that the user that is executing the script has permissions to read/write to your specified backup folders.\n\n**Backup.bat**\n\n```bat\npowershell C:\\Backup\\Tools\\Backup.ps1 -serverName \"WIN-1GA63L4PFK6\\SQLEXPRESS\" -backupDirectory \"C:\\Backup\\Backups\" -daysToStoreBackups 7 >> C:\\Backup\\Logs\\%date%.log\n```\n\nThe last thing ( >> C:\\Backup\\Logs\\%date%.log ) just pipes the script output to a file. You can ignore this part if you don’t care about logging.\n\nThe backup script is backing up all databases except the system database and writes the backup files to the local disk. After all backups are completed the script removes all backups older than the number of days you specified for the “daysToStoreBackups”-parameter.\n\nIn a production environment you would probably want to move your backups to a safe offsite repository too. This can either be as simple as copying the backups to a file share or much more complex depending on your infrastructure. More on this topic in a later post.\n\n**The backup script (Backup.ps1)**\n\n```powershell\nparam(\n    $serverName,\n    $backupDirectory,\n    $daysToStoreBackups\n)\n \n[System.Reflection.Assembly]::LoadWithPartialName(\"Microsoft.SqlServer.SMO\") | Out-Null\n[System.Reflection.Assembly]::LoadWithPartialName(\"Microsoft.SqlServer.SmoExtended\") | Out-Null\n[System.Reflection.Assembly]::LoadWithPartialName(\"Microsoft.SqlServer.ConnectionInfo\") | Out-Null\n[System.Reflection.Assembly]::LoadWithPartialName(\"Microsoft.SqlServer.SmoEnum\") | Out-Null\n \n$server = New-Object (\"Microsoft.SqlServer.Management.Smo.Server\") $serverName\n$dbs = $server.Databases\nforeach ($database in $dbs | where { $_.IsSystemObject -eq $False })\n{\n    $dbName = $database.Name\n \n    $timestamp = Get-Date -format yyyy-MM-dd-HHmmss\n    $targetPath = $backupDirectory + \"\\\" + $dbName + \"_\" + $timestamp + \".bak\"\n \n    $smoBackup = New-Object (\"Microsoft.SqlServer.Management.Smo.Backup\")\n    $smoBackup.Action = \"Database\"\n    $smoBackup.BackupSetDescription = \"Full Backup of \" + $dbName\n    $smoBackup.BackupSetName = $dbName + \" Backup\"\n    $smoBackup.Database = $dbName\n    $smoBackup.MediaDescription = \"Disk\"\n    $smoBackup.Devices.AddDevice($targetPath, \"File\")\n    $smoBackup.SqlBackup($server)\n \n    \"backed up $dbName ($serverName) to $targetPath\"\n}\n \nGet-ChildItem \"$backupDirectory\\*.bak\" |? { $_.lastwritetime -le (Get-Date).AddDays(-$daysToStoreBackups)} |% {Remove-Item $_ -force }\n\"removed all previous backups older than $daysToStoreBackups days\"\n```\n\nAnd yes, you must allow PowerShell to execute remote scripts by:\n\n```powershell\nPS C:\\> Set-ExecutionPolicy RemoteSigned\n \nExecution Policy Change\nThe execution policy helps protect you from scripts that you do not trust. Changing the execution policy might expose\nyou to the security risks described in the about_Execution_Policies help topic at\nhttp://go.microsoft.com/fwlink/?LinkID=135170. Do you want to change the execution policy?\n[Y] Yes  [N] No  [S] Suspend  [?] Help (default is \"Y\"): y\n```\n","html":"<p>A backup process can be really simple or very complex depending on your requirements. In this post I’m going to show how you can backup your sql server databases by using PowerShell. If you don’t want to use this approach you can of course use <a href=\"https://blogs.msdn.microsoft.com/sqlagent/2010/10/12/create-a-database-backup-job-using-sql-server-management-studio/\">sql server agent and t-sql</a> or something like that.</p>\n\n<p>The backup script is executed in PowerShell like this:</p>\n\n<pre><code class=\"language-text\">\\Backup.ps1 -serverName \"WIN-1GA63L4PFK6\\SQLEXPRESS\" -backupDirectory \"C:\\Backup\\Backups\" -daysToStoreBackups 7 \n</code></pre>\n\n<p>If you need to schedule your backups you can use windows built in scheduler. For simplicity you could wrap the execution of the backup script within a bat-script. Just create a new scheduled task and set the action to execute the bat script:</p>\n\n<p><img src=\"/content/images/2016/04/70.png\" alt=\"\" /></p>\n\n<p>Please note that you need to choose “run whether user is logged on or not” under the “general”-tab to allow the task to execute when you are not logged on at the server. Also, make sure that the user that is executing the script has permissions to read/write to your specified backup folders.</p>\n\n<p><strong>Backup.bat</strong></p>\n\n<pre><code class=\"language-bat\">powershell C:\\Backup\\Tools\\Backup.ps1 -serverName \"WIN-1GA63L4PFK6\\SQLEXPRESS\" -backupDirectory \"C:\\Backup\\Backups\" -daysToStoreBackups 7 &gt;&gt; C:\\Backup\\Logs\\%date%.log  \n</code></pre>\n\n<p>The last thing ( >> C:\\Backup\\Logs\\%date%.log ) just pipes the script output to a file. You can ignore this part if you don’t care about logging.</p>\n\n<p>The backup script is backing up all databases except the system database and writes the backup files to the local disk. After all backups are completed the script removes all backups older than the number of days you specified for the “daysToStoreBackups”-parameter.</p>\n\n<p>In a production environment you would probably want to move your backups to a safe offsite repository too. This can either be as simple as copying the backups to a file share or much more complex depending on your infrastructure. More on this topic in a later post.</p>\n\n<p><strong>The backup script (Backup.ps1)</strong></p>\n\n<pre><code class=\"language-powershell\">param(  \n    $serverName,\n    $backupDirectory,\n    $daysToStoreBackups\n)\n\n[System.Reflection.Assembly]::LoadWithPartialName(\"Microsoft.SqlServer.SMO\") | Out-Null\n[System.Reflection.Assembly]::LoadWithPartialName(\"Microsoft.SqlServer.SmoExtended\") | Out-Null\n[System.Reflection.Assembly]::LoadWithPartialName(\"Microsoft.SqlServer.ConnectionInfo\") | Out-Null\n[System.Reflection.Assembly]::LoadWithPartialName(\"Microsoft.SqlServer.SmoEnum\") | Out-Null\n\n$server = New-Object (\"Microsoft.SqlServer.Management.Smo.Server\") $serverName\n$dbs = $server.Databases\nforeach ($database in $dbs | where { $_.IsSystemObject -eq $False })  \n{\n    $dbName = $database.Name\n\n    $timestamp = Get-Date -format yyyy-MM-dd-HHmmss\n    $targetPath = $backupDirectory + \"\\\" + $dbName + \"_\" + $timestamp + \".bak\"\n\n    $smoBackup = New-Object (\"Microsoft.SqlServer.Management.Smo.Backup\")\n    $smoBackup.Action = \"Database\"\n    $smoBackup.BackupSetDescription = \"Full Backup of \" + $dbName\n    $smoBackup.BackupSetName = $dbName + \" Backup\"\n    $smoBackup.Database = $dbName\n    $smoBackup.MediaDescription = \"Disk\"\n    $smoBackup.Devices.AddDevice($targetPath, \"File\")\n    $smoBackup.SqlBackup($server)\n\n    \"backed up $dbName ($serverName) to $targetPath\"\n}\n\nGet-ChildItem \"$backupDirectory\\*.bak\" |? { $_.lastwritetime -le (Get-Date).AddDays(-$daysToStoreBackups)} |% {Remove-Item $_ -force }  \n\"removed all previous backups older than $daysToStoreBackups days\"\n</code></pre>\n\n<p>And yes, you must allow PowerShell to execute remote scripts by:</p>\n\n<pre><code class=\"language-powershell\">PS C:\\&gt; Set-ExecutionPolicy RemoteSigned\n\nExecution Policy Change  \nThe execution policy helps protect you from scripts that you do not trust. Changing the execution policy might expose  \nyou to the security risks described in the about_Execution_Policies help topic at  \nhttp://go.microsoft.com/fwlink/?LinkID=135170. Do you want to change the execution policy?  \n[Y] Yes  [N] No  [S] Suspend  [?] Help (default is \"Y\"): y\n</code></pre>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":1461308738840,"created_by":1,"updated_at":1461308940648,"updated_by":1,"published_at":1385037420000,"published_by":1},{"id":20,"uuid":"0f632aca-6e84-4711-9367-6653ecd4a771","title":"JSON.NET – CASE INSENSITIVE DICTIONARY","slug":"json-net-case-insensitive-dictionary","markdown":"[JSON.NET](http://www.newtonsoft.com/json) can of course deserialize a dictionary out of the box but yesterday I needed to change the way the dictionary is created during deserialization.\n\nI needed the dictionary to be case insensitive when comparing string-keys, which means that the dictionary needs to be created with the **StringComparer.OrdinalIgnoreCase** option.\n\n```csharp\nnew Dictionary<string, T>(StringComparer.OrdinalIgnoreCase);\n```\n\nI my case I needed to do this within a [RavenDB](http://ravendb.net/) context so I [asked](https://groups.google.com/forum/#!msg/ravendb/LtAhCKnBbac/91-Uvq0Fz-QJ) the great people at the [RavenDB mailing list](https://groups.google.com/forum/#!forum/ravendb) for any built in support.\n\nI was pointed in few different directions and finally I decided that a I should implement a [JsonConverter](http://www.newtonsoft.com/json/help/html/CustomJsonConverter.htm).\n\nThis is my implementation and I hope this could help anyone else that needs some of their dictionaries to be case insensitive.\n\n```csharp\nusing System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing log4net;\nusing Raven.Imports.Newtonsoft.Json;\nusing Raven.Imports.Newtonsoft.Json.Linq;\n \nnamespace MyNamespace.Raven.JsonConverters\n{\n    public class CaseInsensitiveDictionaryConverter<T> : JsonConverter\n    {\n        private readonly ILog logger = LogManager.GetLogger(typeof (CaseInsensitiveDictionaryConverter<T>));\n \n        public override bool CanWrite\n        {\n            get { return false; }\n        }\n \n        public override bool CanRead\n        {\n            get { return true; }\n        }\n \n        public override void WriteJson(JsonWriter writer, object value, JsonSerializer serializer)\n        {\n            throw new NotSupportedException();\n        }\n \n        public override object ReadJson(JsonReader reader, Type objectType, object existingValue, JsonSerializer serializer)\n        {\n            logger.DebugFormat(\"Reading JSON for: {0}\", objectType.FullName);\n            if (reader.TokenType == JsonToken.Null)\n            {\n                logger.Debug(\"Reader's token type is null\");\n                return null;\n            }\n \n            var jsonObject = JObject.Load(reader);\n            var jsonString = jsonObject.ToString();\n            logger.DebugFormat(\"JSON: {0}\", jsonString);\n \n            logger.Debug(\"Copying original Dictionary<> to new Dictionary<>(StringComparer.OrdinalIgnoreCase)\");\n \n            var originalDictionary = JsonConvert.DeserializeObject<Dictionary<string, T>>(jsonString);\n            return originalDictionary == null ? null : new Dictionary<string, T>(originalDictionary, StringComparer.OrdinalIgnoreCase);\n        }\n \n        public override bool CanConvert(Type objectType)\n        {\n            return objectType.GetInterfaces().Count(i => HasGenericTypeDefinition(i, typeof(IDictionary<,>))) > 0;\n        }\n \n        private static bool HasGenericTypeDefinition(Type objectType, Type typeDefinition)\n        {\n            return objectType.IsGenericType && objectType.GetGenericTypeDefinition() == typeDefinition;\n        }\n    }\n}\n```\n\nThe converter is used like this:\n\n```csharp\nnamespace MyNamespace.Models.Data\n{\n    public class MyClass\n    {\n        [JsonConverter(typeof(CaseInsensitiveDictionaryConverter<MyObject>))]\n        public IDictionary<string, MyObject> MyDictionary { get; set; }\n    }\n}\n```\n\n**Note:** If you use this converter within a RavenDB context you **must** import JSON.NET from the **Raven.Imports.Newtonsoft** namespace. To use it “standalone” just use the standard JSON.NET namespaces.","html":"<p><a href=\"http://www.newtonsoft.com/json\">JSON.NET</a> can of course deserialize a dictionary out of the box but yesterday I needed to change the way the dictionary is created during deserialization.</p>\n\n<p>I needed the dictionary to be case insensitive when comparing string-keys, which means that the dictionary needs to be created with the <strong>StringComparer.OrdinalIgnoreCase</strong> option.</p>\n\n<pre><code class=\"language-csharp\">new Dictionary&lt;string, T&gt;(StringComparer.OrdinalIgnoreCase);  \n</code></pre>\n\n<p>I my case I needed to do this within a <a href=\"http://ravendb.net/\">RavenDB</a> context so I <a href=\"https://groups.google.com/forum/#!msg/ravendb/LtAhCKnBbac/91-Uvq0Fz-QJ\">asked</a> the great people at the <a href=\"https://groups.google.com/forum/#!forum/ravendb\">RavenDB mailing list</a> for any built in support.</p>\n\n<p>I was pointed in few different directions and finally I decided that a I should implement a <a href=\"http://www.newtonsoft.com/json/help/html/CustomJsonConverter.htm\">JsonConverter</a>.</p>\n\n<p>This is my implementation and I hope this could help anyone else that needs some of their dictionaries to be case insensitive.</p>\n\n<pre><code class=\"language-csharp\">using System;  \nusing System.Collections.Generic;  \nusing System.Linq;  \nusing log4net;  \nusing Raven.Imports.Newtonsoft.Json;  \nusing Raven.Imports.Newtonsoft.Json.Linq;\n\nnamespace MyNamespace.Raven.JsonConverters  \n{\n    public class CaseInsensitiveDictionaryConverter&lt;T&gt; : JsonConverter\n    {\n        private readonly ILog logger = LogManager.GetLogger(typeof (CaseInsensitiveDictionaryConverter&lt;T&gt;));\n\n        public override bool CanWrite\n        {\n            get { return false; }\n        }\n\n        public override bool CanRead\n        {\n            get { return true; }\n        }\n\n        public override void WriteJson(JsonWriter writer, object value, JsonSerializer serializer)\n        {\n            throw new NotSupportedException();\n        }\n\n        public override object ReadJson(JsonReader reader, Type objectType, object existingValue, JsonSerializer serializer)\n        {\n            logger.DebugFormat(\"Reading JSON for: {0}\", objectType.FullName);\n            if (reader.TokenType == JsonToken.Null)\n            {\n                logger.Debug(\"Reader's token type is null\");\n                return null;\n            }\n\n            var jsonObject = JObject.Load(reader);\n            var jsonString = jsonObject.ToString();\n            logger.DebugFormat(\"JSON: {0}\", jsonString);\n\n            logger.Debug(\"Copying original Dictionary&lt;&gt; to new Dictionary&lt;&gt;(StringComparer.OrdinalIgnoreCase)\");\n\n            var originalDictionary = JsonConvert.DeserializeObject&lt;Dictionary&lt;string, T&gt;&gt;(jsonString);\n            return originalDictionary == null ? null : new Dictionary&lt;string, T&gt;(originalDictionary, StringComparer.OrdinalIgnoreCase);\n        }\n\n        public override bool CanConvert(Type objectType)\n        {\n            return objectType.GetInterfaces().Count(i =&gt; HasGenericTypeDefinition(i, typeof(IDictionary&lt;,&gt;))) &gt; 0;\n        }\n\n        private static bool HasGenericTypeDefinition(Type objectType, Type typeDefinition)\n        {\n            return objectType.IsGenericType &amp;&amp; objectType.GetGenericTypeDefinition() == typeDefinition;\n        }\n    }\n}\n</code></pre>\n\n<p>The converter is used like this:</p>\n\n<pre><code class=\"language-csharp\">namespace MyNamespace.Models.Data  \n{\n    public class MyClass\n    {\n        [JsonConverter(typeof(CaseInsensitiveDictionaryConverter&lt;MyObject&gt;))]\n        public IDictionary&lt;string, MyObject&gt; MyDictionary { get; set; }\n    }\n}\n</code></pre>\n\n<p><strong>Note:</strong> If you use this converter within a RavenDB context you <strong>must</strong> import JSON.NET from the <strong>Raven.Imports.Newtonsoft</strong> namespace. To use it “standalone” just use the standard JSON.NET namespaces.</p>","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":1461308958691,"created_by":1,"updated_at":1461309320579,"updated_by":1,"published_at":1426250220000,"published_by":1}],"users":[{"id":1,"uuid":"7c439275-e05b-4c61-9d94-f8d7a75dd8b6","name":"Anton Kallenberg","slug":"anton","password":"$2a$10$mlsagAnROzCrFxq/onm2W.RX3QWJJmuzJXeafh/qZfnAfISuRAzVO","email":"anton.kallenberg@cloudconnected.se","image":null,"cover":null,"bio":null,"website":null,"location":null,"accessibility":null,"status":"active","language":"en_US","meta_title":null,"meta_description":null,"tour":null,"last_login":1461133672050,"created_at":1458549409270,"created_by":1,"updated_at":1461133672050,"updated_by":1}],"roles":[{"id":1,"uuid":"ff72b6f7-b716-4bf3-a446-493d845ceb93","name":"Administrator","description":"Administrators","created_at":1458549407719,"created_by":1,"updated_at":1458549407719,"updated_by":1},{"id":2,"uuid":"fa789eb0-829e-48a7-8b21-ddcf5d1e9e09","name":"Editor","description":"Editors","created_at":1458549407726,"created_by":1,"updated_at":1458549407726,"updated_by":1},{"id":3,"uuid":"00ffd0ce-f46a-4500-82b6-83073139f5ef","name":"Author","description":"Authors","created_at":1458549407734,"created_by":1,"updated_at":1458549407734,"updated_by":1},{"id":4,"uuid":"d6cedc8c-636f-4ec4-957d-6cbd8f1640c2","name":"Owner","description":"Blog Owner","created_at":1458549407742,"created_by":1,"updated_at":1458549407742,"updated_by":1}],"roles_users":[{"id":1,"role_id":4,"user_id":1}],"permissions":[{"id":1,"uuid":"8451d34b-4561-487c-8cfe-56eeb0e98474","name":"Export database","object_type":"db","action_type":"exportContent","object_id":null,"created_at":1458549407750,"created_by":1,"updated_at":1458549407750,"updated_by":1},{"id":2,"uuid":"02f5c58a-7f33-4f63-8743-01a37a5d921f","name":"Import database","object_type":"db","action_type":"importContent","object_id":null,"created_at":1458549407759,"created_by":1,"updated_at":1458549407759,"updated_by":1},{"id":3,"uuid":"806a2ff2-de2a-45aa-9afc-a9a95a786817","name":"Delete all content","object_type":"db","action_type":"deleteAllContent","object_id":null,"created_at":1458549407766,"created_by":1,"updated_at":1458549407766,"updated_by":1},{"id":4,"uuid":"a7f8d788-a943-4e7a-9986-3152baa04ef2","name":"Send mail","object_type":"mail","action_type":"send","object_id":null,"created_at":1458549407775,"created_by":1,"updated_at":1458549407775,"updated_by":1},{"id":5,"uuid":"2b4c8a97-bdfe-4b34-90a2-a97e42716823","name":"Browse notifications","object_type":"notification","action_type":"browse","object_id":null,"created_at":1458549407782,"created_by":1,"updated_at":1458549407782,"updated_by":1},{"id":6,"uuid":"85222191-9b69-44fa-8ab5-1f9e0e1d0d20","name":"Add notifications","object_type":"notification","action_type":"add","object_id":null,"created_at":1458549407790,"created_by":1,"updated_at":1458549407790,"updated_by":1},{"id":7,"uuid":"607f3881-47df-411d-835e-761a7b9eb28d","name":"Delete notifications","object_type":"notification","action_type":"destroy","object_id":null,"created_at":1458549407796,"created_by":1,"updated_at":1458549407796,"updated_by":1},{"id":8,"uuid":"f6d2f08a-36b0-471d-a2e7-12e6457cec90","name":"Browse posts","object_type":"post","action_type":"browse","object_id":null,"created_at":1458549407802,"created_by":1,"updated_at":1458549407802,"updated_by":1},{"id":9,"uuid":"dacbd749-8566-44ff-ab0f-4492a4c18f5e","name":"Read posts","object_type":"post","action_type":"read","object_id":null,"created_at":1458549407810,"created_by":1,"updated_at":1458549407810,"updated_by":1},{"id":10,"uuid":"b1fac431-8473-4518-8dad-756863f2181e","name":"Edit posts","object_type":"post","action_type":"edit","object_id":null,"created_at":1458549407819,"created_by":1,"updated_at":1458549407819,"updated_by":1},{"id":11,"uuid":"5b465872-adaf-46fb-a188-b059f15bee63","name":"Add posts","object_type":"post","action_type":"add","object_id":null,"created_at":1458549407825,"created_by":1,"updated_at":1458549407825,"updated_by":1},{"id":12,"uuid":"22f20772-5735-46c2-b837-43f4778e4219","name":"Delete posts","object_type":"post","action_type":"destroy","object_id":null,"created_at":1458549407834,"created_by":1,"updated_at":1458549407834,"updated_by":1},{"id":13,"uuid":"61e307df-4384-4484-9579-4be9a8c05e6b","name":"Browse settings","object_type":"setting","action_type":"browse","object_id":null,"created_at":1458549407841,"created_by":1,"updated_at":1458549407841,"updated_by":1},{"id":14,"uuid":"b091602b-3ee1-41e0-a3ee-2c997374875c","name":"Read settings","object_type":"setting","action_type":"read","object_id":null,"created_at":1458549407847,"created_by":1,"updated_at":1458549407847,"updated_by":1},{"id":15,"uuid":"4c0cbaec-a80d-4002-a170-625a226a9737","name":"Edit settings","object_type":"setting","action_type":"edit","object_id":null,"created_at":1458549407852,"created_by":1,"updated_at":1458549407852,"updated_by":1},{"id":16,"uuid":"5398846d-1fe7-4efb-8c9c-067e4e6d0da9","name":"Generate slugs","object_type":"slug","action_type":"generate","object_id":null,"created_at":1458549407858,"created_by":1,"updated_at":1458549407858,"updated_by":1},{"id":17,"uuid":"e2f19ad1-2201-4ee7-81ab-468b4a43bb71","name":"Browse tags","object_type":"tag","action_type":"browse","object_id":null,"created_at":1458549407867,"created_by":1,"updated_at":1458549407867,"updated_by":1},{"id":18,"uuid":"b85674d3-3e74-4e95-a75f-2039f698331f","name":"Read tags","object_type":"tag","action_type":"read","object_id":null,"created_at":1458549407873,"created_by":1,"updated_at":1458549407873,"updated_by":1},{"id":19,"uuid":"e65ae88c-8def-477c-a4f2-8bad443418a8","name":"Edit tags","object_type":"tag","action_type":"edit","object_id":null,"created_at":1458549407880,"created_by":1,"updated_at":1458549407880,"updated_by":1},{"id":20,"uuid":"7b1bf282-77fc-4015-a57a-2b0e77de3282","name":"Add tags","object_type":"tag","action_type":"add","object_id":null,"created_at":1458549407888,"created_by":1,"updated_at":1458549407888,"updated_by":1},{"id":21,"uuid":"133546e8-7b06-4d51-98dd-d2ae6628fc85","name":"Delete tags","object_type":"tag","action_type":"destroy","object_id":null,"created_at":1458549407895,"created_by":1,"updated_at":1458549407895,"updated_by":1},{"id":22,"uuid":"3707e2de-e374-452e-9c76-bc8c52faa3db","name":"Browse themes","object_type":"theme","action_type":"browse","object_id":null,"created_at":1458549407902,"created_by":1,"updated_at":1458549407902,"updated_by":1},{"id":23,"uuid":"5619afda-4d52-4081-8052-5444a065cb16","name":"Edit themes","object_type":"theme","action_type":"edit","object_id":null,"created_at":1458549407908,"created_by":1,"updated_at":1458549407908,"updated_by":1},{"id":24,"uuid":"2d269b38-2932-4f16-a634-ae927f066ce3","name":"Browse users","object_type":"user","action_type":"browse","object_id":null,"created_at":1458549407913,"created_by":1,"updated_at":1458549407913,"updated_by":1},{"id":25,"uuid":"e7fe055c-9182-4adb-bd8f-cc8c4604b009","name":"Read users","object_type":"user","action_type":"read","object_id":null,"created_at":1458549407924,"created_by":1,"updated_at":1458549407924,"updated_by":1},{"id":26,"uuid":"08622a83-e79e-4ef1-a6c2-686c7af98b78","name":"Edit users","object_type":"user","action_type":"edit","object_id":null,"created_at":1458549407934,"created_by":1,"updated_at":1458549407934,"updated_by":1},{"id":27,"uuid":"7c68b1be-f441-4c8f-bbb7-39b04abbd1a8","name":"Add users","object_type":"user","action_type":"add","object_id":null,"created_at":1458549407942,"created_by":1,"updated_at":1458549407942,"updated_by":1},{"id":28,"uuid":"a1e02acc-3803-4862-93af-8b4c497d292c","name":"Delete users","object_type":"user","action_type":"destroy","object_id":null,"created_at":1458549407948,"created_by":1,"updated_at":1458549407948,"updated_by":1},{"id":29,"uuid":"4851ac0b-61e2-47c5-b49f-4c6d84e09a5e","name":"Assign a role","object_type":"role","action_type":"assign","object_id":null,"created_at":1458549407955,"created_by":1,"updated_at":1458549407955,"updated_by":1},{"id":30,"uuid":"8ab8b62c-6e94-444b-9124-5f5aef716dc3","name":"Browse roles","object_type":"role","action_type":"browse","object_id":null,"created_at":1458549407960,"created_by":1,"updated_at":1458549407960,"updated_by":1}],"permissions_users":[],"permissions_roles":[{"id":1,"role_id":1,"permission_id":1},{"id":2,"role_id":1,"permission_id":2},{"id":3,"role_id":1,"permission_id":3},{"id":4,"role_id":1,"permission_id":4},{"id":5,"role_id":1,"permission_id":5},{"id":6,"role_id":1,"permission_id":6},{"id":7,"role_id":1,"permission_id":7},{"id":8,"role_id":1,"permission_id":8},{"id":9,"role_id":1,"permission_id":9},{"id":10,"role_id":1,"permission_id":10},{"id":11,"role_id":1,"permission_id":11},{"id":12,"role_id":1,"permission_id":12},{"id":13,"role_id":1,"permission_id":13},{"id":14,"role_id":1,"permission_id":14},{"id":15,"role_id":1,"permission_id":15},{"id":16,"role_id":1,"permission_id":16},{"id":17,"role_id":1,"permission_id":17},{"id":18,"role_id":1,"permission_id":18},{"id":19,"role_id":1,"permission_id":19},{"id":20,"role_id":1,"permission_id":20},{"id":21,"role_id":1,"permission_id":21},{"id":22,"role_id":1,"permission_id":22},{"id":23,"role_id":1,"permission_id":23},{"id":24,"role_id":1,"permission_id":24},{"id":25,"role_id":1,"permission_id":25},{"id":26,"role_id":1,"permission_id":26},{"id":27,"role_id":1,"permission_id":27},{"id":28,"role_id":1,"permission_id":28},{"id":29,"role_id":1,"permission_id":29},{"id":30,"role_id":1,"permission_id":30},{"id":31,"role_id":2,"permission_id":8},{"id":32,"role_id":2,"permission_id":9},{"id":33,"role_id":2,"permission_id":10},{"id":34,"role_id":2,"permission_id":11},{"id":35,"role_id":2,"permission_id":12},{"id":36,"role_id":2,"permission_id":13},{"id":37,"role_id":2,"permission_id":14},{"id":38,"role_id":2,"permission_id":16},{"id":39,"role_id":2,"permission_id":17},{"id":40,"role_id":2,"permission_id":18},{"id":41,"role_id":2,"permission_id":19},{"id":42,"role_id":2,"permission_id":20},{"id":43,"role_id":2,"permission_id":21},{"id":44,"role_id":2,"permission_id":24},{"id":45,"role_id":2,"permission_id":25},{"id":46,"role_id":2,"permission_id":26},{"id":47,"role_id":2,"permission_id":27},{"id":48,"role_id":2,"permission_id":28},{"id":49,"role_id":2,"permission_id":29},{"id":50,"role_id":2,"permission_id":30},{"id":51,"role_id":3,"permission_id":8},{"id":52,"role_id":3,"permission_id":9},{"id":53,"role_id":3,"permission_id":11},{"id":54,"role_id":3,"permission_id":13},{"id":55,"role_id":3,"permission_id":14},{"id":56,"role_id":3,"permission_id":16},{"id":57,"role_id":3,"permission_id":17},{"id":58,"role_id":3,"permission_id":18},{"id":59,"role_id":3,"permission_id":20},{"id":60,"role_id":3,"permission_id":24},{"id":61,"role_id":3,"permission_id":25},{"id":62,"role_id":3,"permission_id":30}],"permissions_apps":[],"settings":[{"id":1,"uuid":"d797d3a6-423f-4f60-afee-98399fdac385","key":"databaseVersion","value":"004","type":"core","created_at":1458549409325,"created_by":1,"updated_at":1458549409325,"updated_by":1},{"id":2,"uuid":"869678a0-fd8a-4ea1-a687-435a21f47f47","key":"dbHash","value":"2d171d55-9236-46c4-bf5f-d55e1e967ff6","type":"core","created_at":1458549409325,"created_by":1,"updated_at":1458549409505,"updated_by":1},{"id":3,"uuid":"b3e02bab-ebeb-4200-ae5c-e775fa2ca0ea","key":"nextUpdateCheck","value":"1461423830","type":"core","created_at":1458549409326,"created_by":1,"updated_at":1461337430483,"updated_by":1},{"id":4,"uuid":"42ed0e6a-c2f9-4305-a7f2-0be14ceb1f06","key":"displayUpdateNotification","value":"0.5.0","type":"core","created_at":1458549409326,"created_by":1,"updated_at":1461337430484,"updated_by":1},{"id":5,"uuid":"11ac75b2-fb38-47e3-9218-987c67201d06","key":"title","value":"Anton Kallenberg","type":"blog","created_at":1458549409326,"created_by":1,"updated_at":1461309669977,"updated_by":1},{"id":6,"uuid":"b10cce14-232c-4d65-9e93-b1f764ec9710","key":"description","value":"Tech, code and ideas.","type":"blog","created_at":1458549409326,"created_by":1,"updated_at":1461309669979,"updated_by":1},{"id":7,"uuid":"7c32d5fa-69a5-4542-a5ed-bead00465764","key":"logo","value":"/content/images/2016/04/11037457_10155159643745012_2225317383839110690_n_2-1.jpg","type":"blog","created_at":1458549409327,"created_by":1,"updated_at":1461309669981,"updated_by":1},{"id":8,"uuid":"0d6a4b64-dc9a-444b-b7ac-730c719c13c2","key":"cover","value":"","type":"blog","created_at":1458549409327,"created_by":1,"updated_at":1461309669983,"updated_by":1},{"id":9,"uuid":"9829f92d-298e-436c-acca-f22ea4514ac7","key":"defaultLang","value":"en_US","type":"blog","created_at":1458549409327,"created_by":1,"updated_at":1461309669984,"updated_by":1},{"id":10,"uuid":"7582c07a-9fd0-45c2-9e63-1eb1c1f13b2d","key":"postsPerPage","value":"10","type":"blog","created_at":1458549409328,"created_by":1,"updated_at":1461309669986,"updated_by":1},{"id":11,"uuid":"d9c563f0-470c-4811-9903-d37a97fdf98c","key":"forceI18n","value":"true","type":"blog","created_at":1458549409328,"created_by":1,"updated_at":1461309669987,"updated_by":1},{"id":12,"uuid":"1957770a-e278-460b-a846-e3e393e3bc39","key":"permalinks","value":"/:year/:month/:day/:slug/","type":"blog","created_at":1458549409328,"created_by":1,"updated_at":1461309669989,"updated_by":1},{"id":13,"uuid":"6d6c00c6-6fd3-4ec5-930f-8f92d3d5d6c5","key":"ghost_head","value":"  <script language=\"javascript\" type=\"text/javascript\">\n    /*\n        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n\n    ga('create', 'UA-76770817-1', 'auto');\n    ga('send', 'pageview');\n      */\n  </script>\n","type":"blog","created_at":1458549409329,"created_by":1,"updated_at":1461309669993,"updated_by":1},{"id":14,"uuid":"67824402-e953-4585-9bb4-6f43d9e627b0","key":"ghost_foot","value":"","type":"blog","created_at":1458549409329,"created_by":1,"updated_at":1461309669996,"updated_by":1},{"id":15,"uuid":"ce4b255a-d00d-4be3-b4e5-c08067cdbb94","key":"labs","value":"{}","type":"blog","created_at":1458549409329,"created_by":1,"updated_at":1461309669997,"updated_by":1},{"id":16,"uuid":"24315d24-6c60-4d36-84ad-0eadedbedf22","key":"navigation","value":"[]","type":"blog","created_at":1458549409329,"created_by":1,"updated_at":1461309669998,"updated_by":1},{"id":17,"uuid":"0efedb18-a2da-478d-a923-6bbf497a0eea","key":"activeApps","value":"[]","type":"app","created_at":1458549409330,"created_by":1,"updated_at":1458549409330,"updated_by":1},{"id":18,"uuid":"3b47d240-cdce-47e3-941e-312d3305c58a","key":"installedApps","value":"[]","type":"app","created_at":1458549409330,"created_by":1,"updated_at":1461337421731,"updated_by":1},{"id":19,"uuid":"ae939125-06ee-4230-90d2-c41a69be1d37","key":"isPrivate","value":"false","type":"private","created_at":1458549409330,"created_by":1,"updated_at":1461309670000,"updated_by":1},{"id":20,"uuid":"c75ce578-3214-4e4c-9fff-2bd0c2f44c4b","key":"password","value":"","type":"private","created_at":1458549409331,"created_by":1,"updated_at":1461309670002,"updated_by":1},{"id":21,"uuid":"c32dbce6-073c-43d5-b2a0-68310782303b","key":"activeTheme","value":"antonkallenberg.com","type":"theme","created_at":1458549409330,"created_by":1,"updated_at":1461309669991,"updated_by":1}],"tags":[{"id":1,"uuid":"3d301aa3-0e30-4663-96f5-420376536ede","name":"Getting Started","slug":"getting-started","description":null,"image":null,"hidden":0,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":1458549407686,"created_by":1,"updated_at":1458549407686,"updated_by":1},{"id":2,"uuid":"f1c8bef2-2a60-4697-afd4-921fbf742ef4","name":"EPiServer","slug":"episerver","description":null,"image":null,"hidden":0,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":1460990855898,"created_by":1,"updated_at":1460990855898,"updated_by":1},{"id":3,"uuid":"eb5a03d3-c821-4000-a8bd-44075f095781","name":".NET","slug":"net","description":null,"image":null,"hidden":0,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":1460990855914,"created_by":1,"updated_at":1460990855914,"updated_by":1},{"id":4,"uuid":"58d201e0-53f1-443d-981b-fdcf028de367","name":"ServiceStack","slug":"servicestack","description":null,"image":null,"hidden":0,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":1461134807909,"created_by":1,"updated_at":1461134807909,"updated_by":1},{"id":5,"uuid":"3deb73b6-5f03-4457-aa45-01452ea44a68","name":"PowerShell","slug":"powershell","description":null,"image":null,"hidden":0,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":1461134807921,"created_by":1,"updated_at":1461134807921,"updated_by":1},{"id":6,"uuid":"3367fdc7-28cb-4b24-9f90-35736551776f","name":"CoffeeScript","slug":"coffeescript","description":null,"image":null,"hidden":0,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":1461134807933,"created_by":1,"updated_at":1461134807933,"updated_by":1},{"id":7,"uuid":"afa35744-f66b-4ec1-97f5-41115d6dd68b","name":"JavaScript","slug":"javascript","description":null,"image":null,"hidden":0,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":1461134807946,"created_by":1,"updated_at":1461134807946,"updated_by":1},{"id":8,"uuid":"86de341e-3c4e-4983-b137-e77b1b59b8cb","name":"Less","slug":"less","description":null,"image":null,"hidden":0,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":1461134807958,"created_by":1,"updated_at":1461134807958,"updated_by":1},{"id":9,"uuid":"3e06a1dd-c08d-4229-b41e-1f272c598a8c","name":"NodeJS","slug":"nodejs","description":null,"image":null,"hidden":0,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":1461134941204,"created_by":1,"updated_at":1461134941204,"updated_by":1},{"id":10,"uuid":"51b7a594-0d17-48a4-83dc-11c6ad288563","name":"Continuous Integration","slug":"continuous-integration","description":null,"image":null,"hidden":0,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":1461136881053,"created_by":1,"updated_at":1461136881053,"updated_by":1},{"id":11,"uuid":"e3d189a1-d1a4-4ff7-879c-956da480e426","name":"Jenkins","slug":"jenkins","description":null,"image":null,"hidden":0,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":1461151242534,"created_by":1,"updated_at":1461151242534,"updated_by":1},{"id":12,"uuid":"b0955c93-4c6b-4582-a51d-77efb799c72e","name":"SQLServer","slug":"sqlserver","description":null,"image":null,"hidden":0,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":1461308938987,"created_by":1,"updated_at":1461308938987,"updated_by":1},{"id":13,"uuid":"bd604327-9078-42f4-94f0-0a74a926fa36","name":"RavenDB","slug":"ravendb","description":null,"image":null,"hidden":0,"parent_id":null,"meta_title":null,"meta_description":null,"created_at":1461309320596,"created_by":1,"updated_at":1461309320596,"updated_by":1}],"posts_tags":[{"id":2,"post_id":6,"tag_id":2,"sort_order":0},{"id":3,"post_id":6,"tag_id":3,"sort_order":1},{"id":4,"post_id":10,"tag_id":3,"sort_order":0},{"id":5,"post_id":10,"tag_id":4,"sort_order":2},{"id":6,"post_id":10,"tag_id":2,"sort_order":1},{"id":7,"post_id":10,"tag_id":5,"sort_order":3},{"id":8,"post_id":10,"tag_id":6,"sort_order":4},{"id":9,"post_id":10,"tag_id":7,"sort_order":5},{"id":10,"post_id":10,"tag_id":8,"sort_order":6},{"id":11,"post_id":10,"tag_id":9,"sort_order":7},{"id":12,"post_id":8,"tag_id":2,"sort_order":0},{"id":13,"post_id":8,"tag_id":3,"sort_order":1},{"id":14,"post_id":9,"tag_id":3,"sort_order":0},{"id":15,"post_id":9,"tag_id":2,"sort_order":1},{"id":16,"post_id":9,"tag_id":10,"sort_order":2},{"id":17,"post_id":12,"tag_id":7,"sort_order":0},{"id":18,"post_id":12,"tag_id":6,"sort_order":1},{"id":19,"post_id":13,"tag_id":10,"sort_order":0},{"id":20,"post_id":13,"tag_id":3,"sort_order":1},{"id":21,"post_id":13,"tag_id":2,"sort_order":2},{"id":22,"post_id":13,"tag_id":11,"sort_order":3},{"id":23,"post_id":14,"tag_id":5,"sort_order":0},{"id":24,"post_id":15,"tag_id":2,"sort_order":0},{"id":25,"post_id":15,"tag_id":5,"sort_order":1},{"id":26,"post_id":16,"tag_id":7,"sort_order":0},{"id":27,"post_id":16,"tag_id":3,"sort_order":1},{"id":28,"post_id":17,"tag_id":10,"sort_order":0},{"id":29,"post_id":17,"tag_id":5,"sort_order":1},{"id":30,"post_id":17,"tag_id":3,"sort_order":2},{"id":31,"post_id":17,"tag_id":2,"sort_order":3},{"id":32,"post_id":18,"tag_id":5,"sort_order":0},{"id":33,"post_id":19,"tag_id":5,"sort_order":0},{"id":34,"post_id":19,"tag_id":12,"sort_order":1},{"id":35,"post_id":20,"tag_id":3,"sort_order":0},{"id":36,"post_id":20,"tag_id":13,"sort_order":1}],"apps":[],"app_settings":[],"app_fields":[]}}]}